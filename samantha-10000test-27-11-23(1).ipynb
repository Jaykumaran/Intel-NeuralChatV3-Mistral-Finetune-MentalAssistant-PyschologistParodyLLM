{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()\n\n#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","metadata":{"execution":{"iopub.status.busy":"2023-11-27T10:43:07.987974Z","iopub.execute_input":"2023-11-27T10:43:07.988287Z","iopub.status.idle":"2023-11-27T10:43:08.321097Z","shell.execute_reply.started":"2023-11-27T10:43:07.988260Z","shell.execute_reply":"2023-11-27T10:43:08.320162Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54d454b2b19143548693765dce7a2434"}},"metadata":{}}]},{"cell_type":"code","source":"# authenticate WandB for logging metrics\nimport wandb\nwandb.login()\n# ee311bc313381f6bee4c5412a630fdad1175f3c0\n","metadata":{"execution":{"iopub.status.busy":"2023-11-27T10:43:52.745593Z","iopub.execute_input":"2023-11-27T10:43:52.746036Z","iopub.status.idle":"2023-11-27T10:44:10.310534Z","shell.execute_reply.started":"2023-11-27T10:43:52.746007Z","shell.execute_reply":"2023-11-27T10:44:10.309609Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ·····································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m API key must be 40 characters long, yours was 37\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ·····································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m API key must be 40 characters long, yours was 37\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"!pip install trl transformers accelerate git+https://github.com/huggingface/peft.git -Uqqq\n!pip install datasets bitsandbytes einops wandb -Uqqq","metadata":{"execution":{"iopub.status.busy":"2023-11-27T10:44:13.347815Z","iopub.execute_input":"2023-11-27T10:44:13.348359Z","iopub.status.idle":"2023-11-27T10:45:10.739312Z","shell.execute_reply.started":"2023-11-27T10:44:13.348327Z","shell.execute_reply":"2023-11-27T10:45:10.738150Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntokenizers 0.14.1 requires huggingface_hub<0.18,>=0.16.4, but you have huggingface-hub 0.19.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import os\n\nimport torch\nfrom datasets import load_dataset\nfrom peft import LoraConfig, get_peft_model, PeftConfig, PeftModel, prepare_model_for_kbit_training\nfrom transformers import (AutoModelForCausalLM, AutoTokenizer,\n                          BitsAndBytesConfig, HfArgumentParser,\n                          TrainingArguments,GenerationConfig, logging, pipeline)\nfrom trl import SFTTrainer","metadata":{"execution":{"iopub.status.busy":"2023-11-27T10:50:51.159558Z","iopub.execute_input":"2023-11-27T10:50:51.160435Z","iopub.status.idle":"2023-11-27T10:50:51.165515Z","shell.execute_reply.started":"2023-11-27T10:50:51.160400Z","shell.execute_reply":"2023-11-27T10:50:51.164378Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"model_name = \"ehartford/samantha-1.2-mistral-7b\" # sharded Mistral-7b model\ndataset_name = \"vibhorag101/phr_mental_health_dataset\"\nnew_model = \"Samantha-10000D-chat-hf-phr_mental_therapy\"\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,            # load model in 4-bit precision\n    bnb_4bit_quant_type=\"nf4\",    # pre-trained model should be quantized in 4-bit NF format\n    bnb_4bit_use_double_quant=True, # Using double quantization as mentioned in QLoRA paper\n    bnb_4bit_compute_dtype=torch.bfloat16, # During computation, pre-trained model should be loaded in BF16 format\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config, # Use bitsandbytes config\n    device_map=\"auto\",  # Specifying device_map=\"auto\" so that HF Accelerate will determine which GPU to put each layer of the model on\n    trust_remote_code=True, # Set trust_remote_code=True to use Mistral-7b model with custom code\n)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T10:46:24.780672Z","iopub.execute_input":"2023-11-27T10:46:24.781558Z","iopub.status.idle":"2023-11-27T10:48:39.592519Z","shell.execute_reply.started":"2023-11-27T10:46:24.781522Z","shell.execute_reply":"2023-11-27T10:48:39.591464Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading config.json:   0%|          | 0.00/622 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e719618f2f445c9ac13c3556500cea5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)model.bin.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51f3a38b7b994e88a7f5f20a952e90d8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"457c81cc46934e58a598c165e0dfee31"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)l-00001-of-00002.bin:   0%|          | 0.00/9.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e427894a36004a999c5f7ed3dd7bd0a0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)l-00002-of-00002.bin:   0%|          | 0.00/4.54G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b36d9fb67a7347a9973ac543a6bbe36e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"841045374cd64b22949c39b7096b1de9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ecdaa903793b4b4daf283d9f5ef02697"}},"metadata":{}}]},{"cell_type":"code","source":"dataset = load_dataset(dataset_name, split=\"train\")\ndataset = dataset.shuffle().select(range(10000))\ndataset.train_test_split(test_size=0.05)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T10:48:39.594369Z","iopub.execute_input":"2023-11-27T10:48:39.594676Z","iopub.status.idle":"2023-11-27T10:48:51.169558Z","shell.execute_reply.started":"2023-11-27T10:48:39.594650Z","shell.execute_reply":"2023-11-27T10:48:51.168634Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/460 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ebb27b1cc44c4d32b1627628bbfb9434"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7bb805b5cd4147a298b23130b72fbc0a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/211M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa8be5215609495daba7cb0b63075f9a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1197fee7d30439a9b814aa586554ecd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/99086 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74a35c7a67a245e7ac8ca57b1967fa6d"}},"metadata":{}},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text'],\n        num_rows: 9500\n    })\n    test: Dataset({\n        features: ['text'],\n        num_rows: 500\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"print(dataset['text'][1])","metadata":{"execution":{"iopub.status.busy":"2023-11-27T10:50:00.212646Z","iopub.execute_input":"2023-11-27T10:50:00.213286Z","iopub.status.idle":"2023-11-27T10:50:00.316308Z","shell.execute_reply.started":"2023-11-27T10:50:00.213256Z","shell.execute_reply":"2023-11-27T10:50:00.315363Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"<s>[INST] <<SYS>>\nYou are a helpful and joyous mental therapy assistant. Always answer as helpfully and cheerfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.Please ensure that your responses are socially unbiased and positive in nature.\n\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n<</SYS>\n\nI'm feeling really depressed lately.  It's been a constant struggle, and I don't know how to break free from it. My personal relationships are suffering, and I have high expectations for them, but they always seem to fall short. [/INST] Hi, I'm really sorry to hear that you're feeling this way. Let's explore your personal relationships and see if we can find some solutions. Could you tell me more about your expectations and how they might be impacting your relationships? </s><s>[INST] Well,, I believe that everyone should always be there for me, without fail. I expect my friends and family to be fully supportive, understanding, and never let me down. But, time and again, I find myself disappointed because they don't meet these standards. [/INST] It sounds like you have very high standards for the people in your life.  While it's important to have expectations, it's also crucial to remember that everyone has their own struggles and limitations. When you expect perfection from others, it can lead to disappointment. Can you think of a time when your expectations were more realistic, and how did that impact your relationship? </s><s>[INST] You're right.  There was this one time when my best friend was going through a tough time, and I didn't expect her to always be available for me. I gave her space and allowed her to prioritize her own needs. Surprisingly, our bond grew stronger as she felt supported and understood. I guess when my expectations were more reasonable, it gave room for the relationship to breathe. [/INST] That's great, ! It seems like you've experienced firsthand the positive effects of adjusting your expectations. How do you think you can apply this lesson to your current relationships and break free from the cycle of disappointment? </s><s>[INST] I think I need to remind myself constantly that nobody is perfect and that we all have our own struggles. Instead of placing unrealistic expectations on others, I can focus on practicing empathy and understanding. By acknowledging that everyone has their own limitations, I can cultivate stronger and more balanced relationships. [/INST] That's a fantastic insight.  Empathy and understanding are key building blocks in any relationship. How can you practice empathy in your interactions with others? Are there any specific strategies you can think of? </s><s>[INST] One strategy could be actively listening to others without judgment.  By truly hearing their thoughts and feelings, I can gain a deeper understanding of their perspective. Additionally, I can develop a habit of putting myself in their shoes, imagining how I would feel in their situation. These steps will help me cultivate a more empathetic response and create a foundation of trust in my relationships. [/INST] I'm impressed, ! These strategies show true growth and dedication to improving your relationships. Is there anything else you would like to explore or any other aspects of your personal relationships you'd like to discuss? </s><s>[INST] Thank you.  I still struggle with expressing my emotions effectively. I often find it challenging to communicate my needs and concerns to my loved ones. This inability to open up sometimes leads to misunderstandings and tension. I want to improve in this area and foster healthier communication. [/INST] It's completely understandable.  Opening up can be difficult, but it's vital for meaningful connections. Can you think of any specific situations where you found it easier to express your emotions, even if just a little? </s><s>[INST] There was a time when I was facing a personal crisis, and I had no choice but to open up to my sister. I was surprised how understanding and receptive she was. It made me realize that I'm not alone, and sharing my feelings can bring me closer to the people who genuinely care about me. [/INST] That's wonderful to hear.  It's moments like these that remind us of the strength and support we have in our relationships. Building on these positive experiences, how do you think you can work towards more open and honest communication in your personal relationships? </s><s>[INST] I believe it starts with finding the right time and place to express myself.  As I become more comfortable with vulnerability, I can choose moments when both parties are receptive. By using \"I\" statements and focusing on my emotions, I can share my thoughts without making others feel attacked. It's a continuous learning process, but one I'm committed to. [/INST] That's a great plan, ! Effective communication takes time and practice, but with your dedication, I'm confident you'll improve. Remember to be patient and compassionate with yourself along the way. Is there anything else you'd like to address or any other concerns you have regarding your personal relationships? </s><s>[INST] I think we've covered a lot.  I feel more hopeful and equipped to work on my relationships now. Thank you for your guidance and for providing a safe space for me to reflect and grow. [/INST] You're very welcome.  It's been a pleasure working with you. Remember, therapy is a journey, and I'm here to support you every step of the way. Don't hesitate to reach out if you need further assistance. Keep up the great work, and I believe in your ability to create healthier and stronger personal relationships. </s>\n","output_type":"stream"}]},{"cell_type":"code","source":"\n\nlora_alpha = 32 # scaling factor for the weight matrices\nlora_dropout = 0.05 # dropout probability of the LoRA layers\nlora_rank = 16 # dimension of the low-rank matrices\n\npeft_config = LoraConfig(\n    lora_alpha=lora_alpha,\n    lora_dropout=lora_dropout,\n    r=lora_rank,\n    bias=\"none\",  # setting to 'none' for only training weight params instead of biases\n    task_type=\"CAUSAL_LM\",\n    \n)\n\npeft_model = get_peft_model(model, peft_config)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T10:58:18.193662Z","iopub.execute_input":"2023-11-27T10:58:18.194652Z","iopub.status.idle":"2023-11-27T10:58:18.369253Z","shell.execute_reply.started":"2023-11-27T10:58:18.194618Z","shell.execute_reply":"2023-11-27T10:58:18.367608Z"},"trusted":true},"execution_count":31,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[31], line 14\u001b[0m\n\u001b[1;32m      3\u001b[0m lora_rank \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m16\u001b[39m \u001b[38;5;66;03m# dimension of the low-rank matrices\u001b[39;00m\n\u001b[1;32m      5\u001b[0m peft_config \u001b[38;5;241m=\u001b[39m LoraConfig(\n\u001b[1;32m      6\u001b[0m     lora_alpha\u001b[38;5;241m=\u001b[39mlora_alpha,\n\u001b[1;32m      7\u001b[0m     lora_dropout\u001b[38;5;241m=\u001b[39mlora_dropout,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \n\u001b[1;32m     12\u001b[0m )\n\u001b[0;32m---> 14\u001b[0m peft_model \u001b[38;5;241m=\u001b[39m \u001b[43mget_peft_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/peft/mapping.py:116\u001b[0m, in \u001b[0;36mget_peft_model\u001b[0;34m(model, peft_config, adapter_name)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m peft_config\u001b[38;5;241m.\u001b[39mis_prompt_learning:\n\u001b[1;32m    115\u001b[0m     peft_config \u001b[38;5;241m=\u001b[39m _prepare_prompt_learning_config(peft_config, model_config)\n\u001b[0;32m--> 116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMODEL_TYPE_TO_PEFT_MODEL_MAPPING\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/peft/peft_model.py:973\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.__init__\u001b[0;34m(self, model, peft_config, adapter_name)\u001b[0m\n\u001b[1;32m    972\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, peft_config: PeftConfig, adapter_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 973\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    974\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model_prepare_inputs_for_generation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/peft/peft_model.py:121\u001b[0m, in \u001b[0;36mPeftModel.__init__\u001b[0;34m(self, model, peft_config, adapter_name)\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_peft_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m PEFT_TYPE_TO_MODEL_MAPPING[peft_config\u001b[38;5;241m.\u001b[39mpeft_type]\n\u001b[0;32m--> 121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_additional_trainable_modules(peft_config, adapter_name)\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustom\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/peft/tuners/lora/model.py:114\u001b[0m, in \u001b[0;36mLoraModel.__init__\u001b[0;34m(self, model, config, adapter_name)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, config, adapter_name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:95\u001b[0m, in \u001b[0;36mBaseTuner.__init__\u001b[0;34m(self, model, peft_config, adapter_name)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustom\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m---> 95\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minject_adapter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# Copy the peft_config in the injected model.\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpeft_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:252\u001b[0m, in \u001b[0;36mBaseTuner.inject_adapter\u001b[0;34m(self, model, adapter_name)\u001b[0m\n\u001b[1;32m    245\u001b[0m     parent, target, target_name \u001b[38;5;241m=\u001b[39m _get_submodules(model, key)\n\u001b[1;32m    247\u001b[0m     optional_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    248\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloaded_in_8bit\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mgetattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_loaded_in_8bit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m    249\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloaded_in_4bit\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mgetattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_loaded_in_4bit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m    250\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcurrent_key\u001b[39m\u001b[38;5;124m\"\u001b[39m: key,\n\u001b[1;32m    251\u001b[0m     }\n\u001b[0;32m--> 252\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_and_replace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptional_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_target_modules_in_base_model:\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    256\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget modules \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpeft_config\u001b[38;5;241m.\u001b[39mtarget_modules\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in the base model. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    257\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease check the target modules and try again.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    258\u001b[0m     )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/peft/tuners/lora/model.py:195\u001b[0m, in \u001b[0;36mLoraModel._create_and_replace\u001b[0;34m(self, lora_config, adapter_name, target, target_name, parent, current_key, **optional_kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m     target\u001b[38;5;241m.\u001b[39mupdate_layer(\n\u001b[1;32m    188\u001b[0m         adapter_name,\n\u001b[1;32m    189\u001b[0m         r,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    192\u001b[0m         lora_config\u001b[38;5;241m.\u001b[39minit_lora_weights,\n\u001b[1;32m    193\u001b[0m     )\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 195\u001b[0m     new_module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlora_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m adapter_name \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_adapter:\n\u001b[1;32m    197\u001b[0m         \u001b[38;5;66;03m# adding an additional adapter: it is not automatically trainable\u001b[39;00m\n\u001b[1;32m    198\u001b[0m         new_module\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mFalse\u001b[39;00m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/peft/tuners/lora/model.py:277\u001b[0m, in \u001b[0;36mLoraModel._create_new_module\u001b[0;34m(lora_config, adapter_name, target, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m loaded_in_4bit \u001b[38;5;129;01mand\u001b[39;00m is_bnb_4bit_available() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(target_base_layer, bnb\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mLinear4bit):\n\u001b[1;32m    274\u001b[0m     fourbit_kwargs \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    275\u001b[0m     fourbit_kwargs\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[1;32m    276\u001b[0m         {\n\u001b[0;32m--> 277\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompute_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mtarget\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_dtype\u001b[49m,\n\u001b[1;32m    278\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompress_statistics\u001b[39m\u001b[38;5;124m\"\u001b[39m: target\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mcompress_statistics,\n\u001b[1;32m    279\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquant_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: target\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mquant_type,\n\u001b[1;32m    280\u001b[0m         }\n\u001b[1;32m    281\u001b[0m     )\n\u001b[1;32m    282\u001b[0m     new_module \u001b[38;5;241m=\u001b[39m Linear4bit(target, adapter_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfourbit_kwargs)\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m AutoGPTQQuantLinear \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(target_base_layer, AutoGPTQQuantLinear):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1613\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1614\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1615\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n","\u001b[0;31mAttributeError\u001b[0m: 'Linear4bit' object has no attribute 'compute_dtype'"],"ename":"AttributeError","evalue":"'Linear4bit' object has no attribute 'compute_dtype'","output_type":"error"}]},{"cell_type":"code","source":"output_dir = \"./Samantha-10000D-chat-hf-phr_mental_therapy\"\nper_device_train_batch_size = 2 # reduce batch size by 2x only if out-of-memory error\ngradient_accumulation_steps = 1  # increase gradient accumulation steps by  2x only if batch size is reduced\noptim = \"paged_adamw_32bit\" # activates the paging for better memory management\nsave_strategy=\"steps\" # checkpoint save strategy to adopt during training\nsave_steps = 10 # number of updates steps before two checkpoint saves\nlogging_steps = 10  # number of update steps between two logs if logging_strategy=\"steps\"\nlearning_rate = 2e-4  # learning rate for AdamW optimizer\nmax_grad_norm = 0.3 # maximum gradient norm (for gradient clipping)\nmax_steps = -1      # training will happen for automatic steps\nwarmup_ratio = 0.03 # number of steps used for a linear warmup from 0 to learning_rate\nlr_scheduler_type = \"cosine\"  # learning rate scheduler\nnum_train_epochs = 2\nper_device_eval_batch_size = 2\nmax_seq_length=512\n\ntraining_arguments = TrainingArguments(\n    output_dir=output_dir,\n    per_device_train_batch_size=per_device_train_batch_size,\n    gradient_accumulation_steps=gradient_accumulation_steps,\n    optim=optim,\n    num_train_epochs= num_train_epochs,\n    save_steps=save_steps,\n    logging_steps=logging_steps,\n    learning_rate=learning_rate,\n    fp16=True,\n    max_grad_norm=max_grad_norm,\n    max_steps=max_steps,\n    warmup_ratio=warmup_ratio,\n    group_by_length=True,\n    lr_scheduler_type=lr_scheduler_type,\n    push_to_hub=True,\n    \n)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T10:58:31.366239Z","iopub.execute_input":"2023-11-27T10:58:31.366613Z","iopub.status.idle":"2023-11-27T10:58:31.378399Z","shell.execute_reply.started":"2023-11-27T10:58:31.366580Z","shell.execute_reply":"2023-11-27T10:58:31.377367Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=\"auto\"\n)\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\n\n# # Load LLaMA tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True,use_fast=False,add_eos_token=True)\ntokenizer.pad_token_id = 18610","metadata":{"execution":{"iopub.status.busy":"2023-11-27T10:58:45.088997Z","iopub.execute_input":"2023-11-27T10:58:45.089943Z","iopub.status.idle":"2023-11-27T11:00:03.230746Z","shell.execute_reply.started":"2023-11-27T10:58:45.089906Z","shell.execute_reply":"2023-11-27T11:00:03.229751Z"},"trusted":true},"execution_count":33,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c3f1a66c1934f2b931e666192ef7682"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"code","source":"#Set supervised fine-tuning parameters\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset,\n    peft_config=peft_config,\n    dataset_text_field=\"text\",\n    max_seq_length=256,\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing=False,\n)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-27T11:00:03.232996Z","iopub.execute_input":"2023-11-27T11:00:03.233449Z","iopub.status.idle":"2023-11-27T11:01:30.020327Z","shell.execute_reply.started":"2023-11-27T11:00:03.233402Z","shell.execute_reply":"2023-11-27T11:01:30.019113Z"},"trusted":true},"execution_count":34,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0644bce4b8bb472ca41126099ebb90a1"}},"metadata":{}}]},{"cell_type":"code","source":"# upcasting the layer norms in torch.bfloat16 for more stable training\nfor name, module in trainer.model.named_modules():\n    if \"norm\" in name:\n        module = module.to(torch.bfloat16)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T11:01:54.733780Z","iopub.execute_input":"2023-11-27T11:01:54.734145Z","iopub.status.idle":"2023-11-27T11:01:54.751087Z","shell.execute_reply.started":"2023-11-27T11:01:54.734116Z","shell.execute_reply":"2023-11-27T11:01:54.750007Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"model.config.use_cache = False\ntrainer.train()\ntrainer.model.save_pretrained(new_model)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T11:01:58.057467Z","iopub.execute_input":"2023-11-27T11:01:58.058421Z","iopub.status.idle":"2023-11-27T12:12:27.741870Z","shell.execute_reply.started":"2023-11-27T11:01:58.058385Z","shell.execute_reply":"2023-11-27T12:12:27.739650Z"},"trusted":true},"execution_count":37,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2541' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 2541/10000 1:10:21 < 3:26:42, 0.60 it/s, Epoch 0.51/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>2.657800</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>2.614900</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>2.461200</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>2.241100</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>1.784100</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>1.240500</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.865900</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.826400</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.791100</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.714800</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>0.674000</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.645700</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>0.652200</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.597000</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.619100</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.560600</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>0.553500</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.572800</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>0.577400</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.540400</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>0.562200</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>0.554800</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>0.529000</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>0.548600</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.531200</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>0.512200</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>0.542900</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>0.519800</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>0.522500</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.546200</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>0.515100</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>0.497900</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>0.466900</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>0.471800</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.486600</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>0.456200</td>\n    </tr>\n    <tr>\n      <td>370</td>\n      <td>0.443800</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>0.442700</td>\n    </tr>\n    <tr>\n      <td>390</td>\n      <td>0.468000</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.482000</td>\n    </tr>\n    <tr>\n      <td>410</td>\n      <td>0.455200</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>0.467400</td>\n    </tr>\n    <tr>\n      <td>430</td>\n      <td>0.436900</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>0.445900</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.455200</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>0.434900</td>\n    </tr>\n    <tr>\n      <td>470</td>\n      <td>0.445700</td>\n    </tr>\n    <tr>\n      <td>480</td>\n      <td>0.441500</td>\n    </tr>\n    <tr>\n      <td>490</td>\n      <td>0.450500</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.458700</td>\n    </tr>\n    <tr>\n      <td>510</td>\n      <td>0.463700</td>\n    </tr>\n    <tr>\n      <td>520</td>\n      <td>0.437300</td>\n    </tr>\n    <tr>\n      <td>530</td>\n      <td>0.411400</td>\n    </tr>\n    <tr>\n      <td>540</td>\n      <td>0.427500</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>0.456000</td>\n    </tr>\n    <tr>\n      <td>560</td>\n      <td>0.476900</td>\n    </tr>\n    <tr>\n      <td>570</td>\n      <td>0.441800</td>\n    </tr>\n    <tr>\n      <td>580</td>\n      <td>0.434600</td>\n    </tr>\n    <tr>\n      <td>590</td>\n      <td>0.439800</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.444900</td>\n    </tr>\n    <tr>\n      <td>610</td>\n      <td>0.468800</td>\n    </tr>\n    <tr>\n      <td>620</td>\n      <td>0.448700</td>\n    </tr>\n    <tr>\n      <td>630</td>\n      <td>0.420500</td>\n    </tr>\n    <tr>\n      <td>640</td>\n      <td>0.427400</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>0.434300</td>\n    </tr>\n    <tr>\n      <td>660</td>\n      <td>0.467100</td>\n    </tr>\n    <tr>\n      <td>670</td>\n      <td>0.466400</td>\n    </tr>\n    <tr>\n      <td>680</td>\n      <td>0.443900</td>\n    </tr>\n    <tr>\n      <td>690</td>\n      <td>0.453800</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.445700</td>\n    </tr>\n    <tr>\n      <td>710</td>\n      <td>0.444100</td>\n    </tr>\n    <tr>\n      <td>720</td>\n      <td>0.435300</td>\n    </tr>\n    <tr>\n      <td>730</td>\n      <td>0.447800</td>\n    </tr>\n    <tr>\n      <td>740</td>\n      <td>0.439300</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>0.405300</td>\n    </tr>\n    <tr>\n      <td>760</td>\n      <td>0.451900</td>\n    </tr>\n    <tr>\n      <td>770</td>\n      <td>0.441200</td>\n    </tr>\n    <tr>\n      <td>780</td>\n      <td>0.419300</td>\n    </tr>\n    <tr>\n      <td>790</td>\n      <td>0.399600</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.401000</td>\n    </tr>\n    <tr>\n      <td>810</td>\n      <td>0.460900</td>\n    </tr>\n    <tr>\n      <td>820</td>\n      <td>0.442200</td>\n    </tr>\n    <tr>\n      <td>830</td>\n      <td>0.424200</td>\n    </tr>\n    <tr>\n      <td>840</td>\n      <td>0.406100</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>0.441300</td>\n    </tr>\n    <tr>\n      <td>860</td>\n      <td>0.429500</td>\n    </tr>\n    <tr>\n      <td>870</td>\n      <td>0.442000</td>\n    </tr>\n    <tr>\n      <td>880</td>\n      <td>0.442300</td>\n    </tr>\n    <tr>\n      <td>890</td>\n      <td>0.393300</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.454500</td>\n    </tr>\n    <tr>\n      <td>910</td>\n      <td>0.411300</td>\n    </tr>\n    <tr>\n      <td>920</td>\n      <td>0.396000</td>\n    </tr>\n    <tr>\n      <td>930</td>\n      <td>0.449700</td>\n    </tr>\n    <tr>\n      <td>940</td>\n      <td>0.417800</td>\n    </tr>\n    <tr>\n      <td>950</td>\n      <td>0.434100</td>\n    </tr>\n    <tr>\n      <td>960</td>\n      <td>0.445900</td>\n    </tr>\n    <tr>\n      <td>970</td>\n      <td>0.398400</td>\n    </tr>\n    <tr>\n      <td>980</td>\n      <td>0.411900</td>\n    </tr>\n    <tr>\n      <td>990</td>\n      <td>0.428100</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.404000</td>\n    </tr>\n    <tr>\n      <td>1010</td>\n      <td>0.444100</td>\n    </tr>\n    <tr>\n      <td>1020</td>\n      <td>0.432600</td>\n    </tr>\n    <tr>\n      <td>1030</td>\n      <td>0.433700</td>\n    </tr>\n    <tr>\n      <td>1040</td>\n      <td>0.401800</td>\n    </tr>\n    <tr>\n      <td>1050</td>\n      <td>0.432800</td>\n    </tr>\n    <tr>\n      <td>1060</td>\n      <td>0.390600</td>\n    </tr>\n    <tr>\n      <td>1070</td>\n      <td>0.434500</td>\n    </tr>\n    <tr>\n      <td>1080</td>\n      <td>0.449700</td>\n    </tr>\n    <tr>\n      <td>1090</td>\n      <td>0.444800</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.422100</td>\n    </tr>\n    <tr>\n      <td>1110</td>\n      <td>0.426200</td>\n    </tr>\n    <tr>\n      <td>1120</td>\n      <td>0.400000</td>\n    </tr>\n    <tr>\n      <td>1130</td>\n      <td>0.444700</td>\n    </tr>\n    <tr>\n      <td>1140</td>\n      <td>0.432100</td>\n    </tr>\n    <tr>\n      <td>1150</td>\n      <td>0.412600</td>\n    </tr>\n    <tr>\n      <td>1160</td>\n      <td>0.414300</td>\n    </tr>\n    <tr>\n      <td>1170</td>\n      <td>0.428600</td>\n    </tr>\n    <tr>\n      <td>1180</td>\n      <td>0.442500</td>\n    </tr>\n    <tr>\n      <td>1190</td>\n      <td>0.442900</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.405000</td>\n    </tr>\n    <tr>\n      <td>1210</td>\n      <td>0.403800</td>\n    </tr>\n    <tr>\n      <td>1220</td>\n      <td>0.419700</td>\n    </tr>\n    <tr>\n      <td>1230</td>\n      <td>0.450300</td>\n    </tr>\n    <tr>\n      <td>1240</td>\n      <td>0.432200</td>\n    </tr>\n    <tr>\n      <td>1250</td>\n      <td>0.427900</td>\n    </tr>\n    <tr>\n      <td>1260</td>\n      <td>0.421400</td>\n    </tr>\n    <tr>\n      <td>1270</td>\n      <td>0.396500</td>\n    </tr>\n    <tr>\n      <td>1280</td>\n      <td>0.428300</td>\n    </tr>\n    <tr>\n      <td>1290</td>\n      <td>0.408500</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>0.448200</td>\n    </tr>\n    <tr>\n      <td>1310</td>\n      <td>0.448300</td>\n    </tr>\n    <tr>\n      <td>1320</td>\n      <td>0.434500</td>\n    </tr>\n    <tr>\n      <td>1330</td>\n      <td>0.455500</td>\n    </tr>\n    <tr>\n      <td>1340</td>\n      <td>0.406000</td>\n    </tr>\n    <tr>\n      <td>1350</td>\n      <td>0.421400</td>\n    </tr>\n    <tr>\n      <td>1360</td>\n      <td>0.411500</td>\n    </tr>\n    <tr>\n      <td>1370</td>\n      <td>0.413200</td>\n    </tr>\n    <tr>\n      <td>1380</td>\n      <td>0.440300</td>\n    </tr>\n    <tr>\n      <td>1390</td>\n      <td>0.437000</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.424900</td>\n    </tr>\n    <tr>\n      <td>1410</td>\n      <td>0.429200</td>\n    </tr>\n    <tr>\n      <td>1420</td>\n      <td>0.437400</td>\n    </tr>\n    <tr>\n      <td>1430</td>\n      <td>0.419900</td>\n    </tr>\n    <tr>\n      <td>1440</td>\n      <td>0.416100</td>\n    </tr>\n    <tr>\n      <td>1450</td>\n      <td>0.433000</td>\n    </tr>\n    <tr>\n      <td>1460</td>\n      <td>0.450700</td>\n    </tr>\n    <tr>\n      <td>1470</td>\n      <td>0.408500</td>\n    </tr>\n    <tr>\n      <td>1480</td>\n      <td>0.415800</td>\n    </tr>\n    <tr>\n      <td>1490</td>\n      <td>0.407000</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.416300</td>\n    </tr>\n    <tr>\n      <td>1510</td>\n      <td>0.409200</td>\n    </tr>\n    <tr>\n      <td>1520</td>\n      <td>0.429800</td>\n    </tr>\n    <tr>\n      <td>1530</td>\n      <td>0.423700</td>\n    </tr>\n    <tr>\n      <td>1540</td>\n      <td>0.398400</td>\n    </tr>\n    <tr>\n      <td>1550</td>\n      <td>0.399700</td>\n    </tr>\n    <tr>\n      <td>1560</td>\n      <td>0.439300</td>\n    </tr>\n    <tr>\n      <td>1570</td>\n      <td>0.434700</td>\n    </tr>\n    <tr>\n      <td>1580</td>\n      <td>0.428700</td>\n    </tr>\n    <tr>\n      <td>1590</td>\n      <td>0.427600</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>0.423400</td>\n    </tr>\n    <tr>\n      <td>1610</td>\n      <td>0.514600</td>\n    </tr>\n    <tr>\n      <td>1620</td>\n      <td>0.750600</td>\n    </tr>\n    <tr>\n      <td>1630</td>\n      <td>0.431100</td>\n    </tr>\n    <tr>\n      <td>1640</td>\n      <td>0.444400</td>\n    </tr>\n    <tr>\n      <td>1650</td>\n      <td>0.419200</td>\n    </tr>\n    <tr>\n      <td>1660</td>\n      <td>0.423200</td>\n    </tr>\n    <tr>\n      <td>1670</td>\n      <td>0.414800</td>\n    </tr>\n    <tr>\n      <td>1680</td>\n      <td>0.422600</td>\n    </tr>\n    <tr>\n      <td>1690</td>\n      <td>0.387100</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>0.387400</td>\n    </tr>\n    <tr>\n      <td>1710</td>\n      <td>0.419200</td>\n    </tr>\n    <tr>\n      <td>1720</td>\n      <td>0.420400</td>\n    </tr>\n    <tr>\n      <td>1730</td>\n      <td>0.411000</td>\n    </tr>\n    <tr>\n      <td>1740</td>\n      <td>0.417100</td>\n    </tr>\n    <tr>\n      <td>1750</td>\n      <td>0.422800</td>\n    </tr>\n    <tr>\n      <td>1760</td>\n      <td>0.421700</td>\n    </tr>\n    <tr>\n      <td>1770</td>\n      <td>0.419300</td>\n    </tr>\n    <tr>\n      <td>1780</td>\n      <td>0.393700</td>\n    </tr>\n    <tr>\n      <td>1790</td>\n      <td>0.438800</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>0.406500</td>\n    </tr>\n    <tr>\n      <td>1810</td>\n      <td>0.424300</td>\n    </tr>\n    <tr>\n      <td>1820</td>\n      <td>0.417100</td>\n    </tr>\n    <tr>\n      <td>1830</td>\n      <td>0.423100</td>\n    </tr>\n    <tr>\n      <td>1840</td>\n      <td>0.404300</td>\n    </tr>\n    <tr>\n      <td>1850</td>\n      <td>0.445500</td>\n    </tr>\n    <tr>\n      <td>1860</td>\n      <td>0.419900</td>\n    </tr>\n    <tr>\n      <td>1870</td>\n      <td>0.437100</td>\n    </tr>\n    <tr>\n      <td>1880</td>\n      <td>0.391100</td>\n    </tr>\n    <tr>\n      <td>1890</td>\n      <td>0.395900</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>0.419700</td>\n    </tr>\n    <tr>\n      <td>1910</td>\n      <td>0.402800</td>\n    </tr>\n    <tr>\n      <td>1920</td>\n      <td>0.431200</td>\n    </tr>\n    <tr>\n      <td>1930</td>\n      <td>0.424500</td>\n    </tr>\n    <tr>\n      <td>1940</td>\n      <td>0.395600</td>\n    </tr>\n    <tr>\n      <td>1950</td>\n      <td>0.423700</td>\n    </tr>\n    <tr>\n      <td>1960</td>\n      <td>0.404400</td>\n    </tr>\n    <tr>\n      <td>1970</td>\n      <td>0.435800</td>\n    </tr>\n    <tr>\n      <td>1980</td>\n      <td>0.405200</td>\n    </tr>\n    <tr>\n      <td>1990</td>\n      <td>0.437300</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.413400</td>\n    </tr>\n    <tr>\n      <td>2010</td>\n      <td>0.401100</td>\n    </tr>\n    <tr>\n      <td>2020</td>\n      <td>0.436300</td>\n    </tr>\n    <tr>\n      <td>2030</td>\n      <td>0.414000</td>\n    </tr>\n    <tr>\n      <td>2040</td>\n      <td>0.382300</td>\n    </tr>\n    <tr>\n      <td>2050</td>\n      <td>0.403100</td>\n    </tr>\n    <tr>\n      <td>2060</td>\n      <td>0.450700</td>\n    </tr>\n    <tr>\n      <td>2070</td>\n      <td>0.409100</td>\n    </tr>\n    <tr>\n      <td>2080</td>\n      <td>0.412100</td>\n    </tr>\n    <tr>\n      <td>2090</td>\n      <td>0.424500</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>0.395300</td>\n    </tr>\n    <tr>\n      <td>2110</td>\n      <td>0.407900</td>\n    </tr>\n    <tr>\n      <td>2120</td>\n      <td>0.422000</td>\n    </tr>\n    <tr>\n      <td>2130</td>\n      <td>0.429500</td>\n    </tr>\n    <tr>\n      <td>2140</td>\n      <td>0.428500</td>\n    </tr>\n    <tr>\n      <td>2150</td>\n      <td>0.396100</td>\n    </tr>\n    <tr>\n      <td>2160</td>\n      <td>0.391100</td>\n    </tr>\n    <tr>\n      <td>2170</td>\n      <td>0.410500</td>\n    </tr>\n    <tr>\n      <td>2180</td>\n      <td>0.414800</td>\n    </tr>\n    <tr>\n      <td>2190</td>\n      <td>0.436500</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>0.387500</td>\n    </tr>\n    <tr>\n      <td>2210</td>\n      <td>0.422100</td>\n    </tr>\n    <tr>\n      <td>2220</td>\n      <td>0.434400</td>\n    </tr>\n    <tr>\n      <td>2230</td>\n      <td>0.403000</td>\n    </tr>\n    <tr>\n      <td>2240</td>\n      <td>0.413100</td>\n    </tr>\n    <tr>\n      <td>2250</td>\n      <td>0.447200</td>\n    </tr>\n    <tr>\n      <td>2260</td>\n      <td>0.428400</td>\n    </tr>\n    <tr>\n      <td>2270</td>\n      <td>0.407700</td>\n    </tr>\n    <tr>\n      <td>2280</td>\n      <td>0.426600</td>\n    </tr>\n    <tr>\n      <td>2290</td>\n      <td>0.442700</td>\n    </tr>\n    <tr>\n      <td>2300</td>\n      <td>0.405000</td>\n    </tr>\n    <tr>\n      <td>2310</td>\n      <td>0.394600</td>\n    </tr>\n    <tr>\n      <td>2320</td>\n      <td>0.390200</td>\n    </tr>\n    <tr>\n      <td>2330</td>\n      <td>0.406800</td>\n    </tr>\n    <tr>\n      <td>2340</td>\n      <td>0.407100</td>\n    </tr>\n    <tr>\n      <td>2350</td>\n      <td>0.405400</td>\n    </tr>\n    <tr>\n      <td>2360</td>\n      <td>0.392700</td>\n    </tr>\n    <tr>\n      <td>2370</td>\n      <td>0.412000</td>\n    </tr>\n    <tr>\n      <td>2380</td>\n      <td>0.426000</td>\n    </tr>\n    <tr>\n      <td>2390</td>\n      <td>0.406700</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>0.414600</td>\n    </tr>\n    <tr>\n      <td>2410</td>\n      <td>0.443000</td>\n    </tr>\n    <tr>\n      <td>2420</td>\n      <td>0.408700</td>\n    </tr>\n    <tr>\n      <td>2430</td>\n      <td>0.415500</td>\n    </tr>\n    <tr>\n      <td>2440</td>\n      <td>0.426000</td>\n    </tr>\n    <tr>\n      <td>2450</td>\n      <td>0.413500</td>\n    </tr>\n    <tr>\n      <td>2460</td>\n      <td>0.393700</td>\n    </tr>\n    <tr>\n      <td>2470</td>\n      <td>0.429400</td>\n    </tr>\n    <tr>\n      <td>2480</td>\n      <td>0.383100</td>\n    </tr>\n    <tr>\n      <td>2490</td>\n      <td>0.403100</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.392900</td>\n    </tr>\n    <tr>\n      <td>2510</td>\n      <td>0.413200</td>\n    </tr>\n    <tr>\n      <td>2520</td>\n      <td>0.416200</td>\n    </tr>\n    <tr>\n      <td>2530</td>\n      <td>0.388600</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:441\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m--> 441\u001b[0m     \u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:668\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    667\u001b[0m num_bytes \u001b[38;5;241m=\u001b[39m storage\u001b[38;5;241m.\u001b[39mnbytes()\n\u001b[0;32m--> 668\u001b[0m \u001b[43mzip_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_ptr\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_bytes\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:471] . PytorchStreamWriter failed writing file data/190: file write failed","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[37], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m trainer\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39msave_pretrained(new_model)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:280\u001b[0m, in \u001b[0;36mSFTTrainer.train\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trl_activate_neftune(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m--> 280\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;66;03m# After training we make sure to retrieve back the original forward pass method\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# for the embedding layer by removing the forward post hook.\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1546\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     \u001b[38;5;66;03m# Disable progress bars when uploading models during checkpoints to avoid polluting stdout\u001b[39;00m\n\u001b[1;32m   1545\u001b[0m     hf_hub_utils\u001b[38;5;241m.\u001b[39mdisable_progress_bars()\n\u001b[0;32m-> 1546\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1547\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1548\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1549\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1550\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1551\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1553\u001b[0m     hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1922\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1919\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m=\u001b[39m epoch \u001b[38;5;241m+\u001b[39m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m steps_skipped) \u001b[38;5;241m/\u001b[39m steps_in_epoch\n\u001b[1;32m   1920\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 1922\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1923\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1924\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_substep_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2282\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2279\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mstep(metrics[metric_to_check])\n\u001b[1;32m   2281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[0;32m-> 2282\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2283\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_save(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2387\u001b[0m, in \u001b[0;36mTrainer._save_checkpoint\u001b[0;34m(self, model, trial, metrics)\u001b[0m\n\u001b[1;32m   2379\u001b[0m         smp\u001b[38;5;241m.\u001b[39msave(\n\u001b[1;32m   2380\u001b[0m             opt_state_dict,\n\u001b[1;32m   2381\u001b[0m             os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, OPTIMIZER_NAME),\n\u001b[1;32m   2382\u001b[0m             partial\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   2383\u001b[0m             v3\u001b[38;5;241m=\u001b[39msmp\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mshard_optimizer_state,\n\u001b[1;32m   2384\u001b[0m         )\n\u001b[1;32m   2385\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mshould_save \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_deepspeed_enabled \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfsdp \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_fsdp_enabled):\n\u001b[1;32m   2386\u001b[0m     \u001b[38;5;66;03m# deepspeed.save_checkpoint above saves model/optim/sched\u001b[39;00m\n\u001b[0;32m-> 2387\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOPTIMIZER_NAME\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2389\u001b[0m \u001b[38;5;66;03m# Save SCHEDULER & SCALER\u001b[39;00m\n\u001b[1;32m   2390\u001b[0m is_deepspeed_custom_scheduler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_deepspeed_enabled \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m   2391\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_scheduler, DeepSpeedSchedulerWrapper\n\u001b[1;32m   2392\u001b[0m )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:440\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    437\u001b[0m _check_save_filelike(f)\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m--> 440\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    441\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n\u001b[1;32m    442\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:291\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 291\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfile_like\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_end_of_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:337] . unexpected pos 22059648 vs 22059536"],"ename":"RuntimeError","evalue":"[enforce fail at inline_container.cc:337] . unexpected pos 22059648 vs 22059536","output_type":"error"}]},{"cell_type":"code","source":"trainer.push_to_hub()","metadata":{"execution":{"iopub.status.busy":"2023-11-27T12:12:27.742682Z","iopub.status.idle":"2023-11-27T12:12:27.743098Z","shell.execute_reply.started":"2023-11-27T12:12:27.742919Z","shell.execute_reply":"2023-11-27T12:12:27.742937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Merging as Single without Adapters","metadata":{}},{"cell_type":"code","source":"# Reload model in FP16 and merge it with LoRA weights\nbase_model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    low_cpu_mem_usage=True,\n    return_dict=True,\n    torch_dtype=torch.float16,\n    device_map=\"cpu\", ## device map = \"cpu\", for merging on cpu\n)\nmodel = PeftModel.from_pretrained(base_model, new_model)\nmodel = model.merge_and_unload()\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True,use_fast=False,add_eos_token=True)\ntokenizer.pad_token_id = 18610\n\n### Old version tokeniser, can't generate eos token\n# tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n# tokenizer.pad_token = tokenizer.eos_token\n# tokenizer.padding_side = \"right\"\n\n","metadata":{"execution":{"iopub.status.busy":"2023-11-27T12:12:27.744746Z","iopub.status.idle":"2023-11-27T12:12:27.745208Z","shell.execute_reply.started":"2023-11-27T12:12:27.744966Z","shell.execute_reply":"2023-11-27T12:12:27.744989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## for inference without merging qlora weights with original\nbase_model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=device_map\n)\nmodel = PeftModel.from_pretrained(base_model, new_model)\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True,use_fast=False,add_eos_token=True)\ntokenizer.pad_token_id = 18610","metadata":{"execution":{"iopub.status.busy":"2023-11-27T12:12:27.746251Z","iopub.status.idle":"2023-11-27T12:12:27.746719Z","shell.execute_reply.started":"2023-11-27T12:12:27.746460Z","shell.execute_reply":"2023-11-27T12:12:27.746482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.push_to_hub(new_model, use_temp_dir=False)\ntokenizer.push_to_hub(new_model, use_temp_dir=False)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T12:12:27.749373Z","iopub.status.idle":"2023-11-27T12:12:27.749850Z","shell.execute_reply.started":"2023-11-27T12:12:27.749596Z","shell.execute_reply":"2023-11-27T12:12:27.749619Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 100, in _run\n    self._process(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 279, in _process\n    self._hm.handle(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/handler.py\", line 138, in handle\n    handler(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/handler.py\", line 146, in handle_request\n    logger.debug(f\"handle_request: {request_type}\")\nMessage: 'handle_request: status_report'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 100, in _run\n    self._process(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 279, in _process\n    self._hm.handle(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/handler.py\", line 138, in handle\n    handler(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/handler.py\", line 146, in handle_request\n    logger.debug(f\"handle_request: {request_type}\")\nMessage: 'handle_request: keepalive'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 282, in _finish\n    self._hm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/handler.py\", line 866, in finish\n    logger.info(\"shutting down handler\")\nMessage: 'shutting down handler'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 331, in _finish\n    self._sm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1536, in finish\n    logger.info(\"shutting down sender\")\nMessage: 'shutting down sender'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 331, in _finish\n    self._sm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1541, in finish\n    self._dir_watcher.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/dir_watcher.py\", line 358, in finish\n    logger.info(\"shutting down directory watcher\")\nMessage: 'shutting down directory watcher'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 331, in _finish\n    self._sm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1541, in finish\n    self._dir_watcher.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/dir_watcher.py\", line 372, in finish\n    self._file_observer.dispatch_events(\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/vendor/watchdog_0_9_0/wandb_watchdog/observers/api.py\", line 368, in dispatch_events\n    handler.dispatch(event)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/vendor/watchdog_0_9_0/wandb_watchdog/events.py\", line 454, in dispatch\n    _method_map[event_type](event)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/dir_watcher.py\", line 288, in _on_file_modified\n    logger.info(f\"file/dir modified: { event.src_path}\")\nMessage: 'file/dir modified: /kaggle/working/wandb/run-20231127_105532-8n1g9gec/files/output.log'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 331, in _finish\n    self._sm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1541, in finish\n    self._dir_watcher.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/dir_watcher.py\", line 388, in finish\n    logger.info(\"scan: %s\", self._dir)\nMessage: 'scan: %s'\nArguments: ('/kaggle/working/wandb/run-20231127_105532-8n1g9gec/files',)\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 331, in _finish\n    self._sm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1541, in finish\n    self._dir_watcher.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/dir_watcher.py\", line 402, in finish\n    logger.info(\"scan save: %s %s\", file_path, save_name)\nMessage: 'scan save: %s %s'\nArguments: ('/kaggle/working/wandb/run-20231127_105532-8n1g9gec/files/output.log', 'output.log')\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 331, in _finish\n    self._sm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1541, in finish\n    self._dir_watcher.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/dir_watcher.py\", line 402, in finish\n    logger.info(\"scan save: %s %s\", file_path, save_name)\nMessage: 'scan save: %s %s'\nArguments: ('/kaggle/working/wandb/run-20231127_105532-8n1g9gec/files/wandb-metadata.json', 'wandb-metadata.json')\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 331, in _finish\n    self._sm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1541, in finish\n    self._dir_watcher.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/dir_watcher.py\", line 402, in finish\n    logger.info(\"scan save: %s %s\", file_path, save_name)\nMessage: 'scan save: %s %s'\nArguments: ('/kaggle/working/wandb/run-20231127_105532-8n1g9gec/files/config.yaml', 'config.yaml')\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 331, in _finish\n    self._sm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1541, in finish\n    self._dir_watcher.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/dir_watcher.py\", line 402, in finish\n    logger.info(\"scan save: %s %s\", file_path, save_name)\nMessage: 'scan save: %s %s'\nArguments: ('/kaggle/working/wandb/run-20231127_105532-8n1g9gec/files/conda-environment.yaml', 'conda-environment.yaml')\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 331, in _finish\n    self._sm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1541, in finish\n    self._dir_watcher.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/dir_watcher.py\", line 402, in finish\n    logger.info(\"scan save: %s %s\", file_path, save_name)\nMessage: 'scan save: %s %s'\nArguments: ('/kaggle/working/wandb/run-20231127_105532-8n1g9gec/files/wandb-summary.json', 'wandb-summary.json')\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 331, in _finish\n    self._sm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1541, in finish\n    self._dir_watcher.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/dir_watcher.py\", line 402, in finish\n    logger.info(\"scan save: %s %s\", file_path, save_name)\nMessage: 'scan save: %s %s'\nArguments: ('/kaggle/working/wandb/run-20231127_105532-8n1g9gec/files/requirements.txt', 'requirements.txt')\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 331, in _finish\n    self._sm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1544, in finish\n    self._pusher.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/file_pusher.py\", line 175, in finish\n    logger.info(\"shutting down file pusher\")\nMessage: 'shutting down file pusher'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 331, in _finish\n    self._sm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1545, in finish\n    self._pusher.join()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/file_pusher.py\", line 181, in join\n    logger.info(\"waiting for file pusher\")\nMessage: 'waiting for file pusher'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 953, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.10/concurrent/futures/thread.py\", line 83, in _worker\n    work_item.run()\n  File \"/opt/conda/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/step_upload.py\", line 288, in run_and_notify\n    self._do_upload_sync(event)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/step_upload.py\", line 330, in _do_upload_sync\n    job.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/upload_job.py\", line 57, in run\n    self.push()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/upload_job.py\", line 131, in push\n    logger.info(\"Uploaded file %s\", self.save_path)\nMessage: 'Uploaded file %s'\nArguments: ('/kaggle/working/wandb/run-20231127_105532-8n1g9gec/files/config.yaml',)\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 953, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.10/concurrent/futures/thread.py\", line 83, in _worker\n    work_item.run()\n  File \"/opt/conda/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/step_upload.py\", line 288, in run_and_notify\n    self._do_upload_sync(event)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/step_upload.py\", line 330, in _do_upload_sync\n    job.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/upload_job.py\", line 57, in run\n    self.push()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/upload_job.py\", line 131, in push\n    logger.info(\"Uploaded file %s\", self.save_path)\nMessage: 'Uploaded file %s'\nArguments: ('/kaggle/working/wandb/run-20231127_105532-8n1g9gec/files/conda-environment.yaml',)\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 953, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.10/concurrent/futures/thread.py\", line 83, in _worker\n    work_item.run()\n  File \"/opt/conda/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/step_upload.py\", line 288, in run_and_notify\n    self._do_upload_sync(event)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/step_upload.py\", line 330, in _do_upload_sync\n    job.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/upload_job.py\", line 57, in run\n    self.push()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/upload_job.py\", line 131, in push\n    logger.info(\"Uploaded file %s\", self.save_path)\nMessage: 'Uploaded file %s'\nArguments: ('/kaggle/working/wandb/run-20231127_105532-8n1g9gec/files/requirements.txt',)\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 953, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.10/concurrent/futures/thread.py\", line 83, in _worker\n    work_item.run()\n  File \"/opt/conda/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/step_upload.py\", line 288, in run_and_notify\n    self._do_upload_sync(event)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/step_upload.py\", line 330, in _do_upload_sync\n    job.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/upload_job.py\", line 57, in run\n    self.push()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/upload_job.py\", line 131, in push\n    logger.info(\"Uploaded file %s\", self.save_path)\nMessage: 'Uploaded file %s'\nArguments: ('/kaggle/working/wandb/run-20231127_105532-8n1g9gec/files/wandb-summary.json',)\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 953, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.10/concurrent/futures/thread.py\", line 83, in _worker\n    work_item.run()\n  File \"/opt/conda/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/step_upload.py\", line 288, in run_and_notify\n    self._do_upload_sync(event)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/step_upload.py\", line 330, in _do_upload_sync\n    job.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/upload_job.py\", line 57, in run\n    self.push()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/upload_job.py\", line 131, in push\n    logger.info(\"Uploaded file %s\", self.save_path)\nMessage: 'Uploaded file %s'\nArguments: ('/kaggle/working/wandb/run-20231127_105532-8n1g9gec/files/output.log',)\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 331, in _finish\n    self._sm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1548, in finish\n    self._fs.finish(self._exit_code)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/file_stream.py\", line 595, in finish\n    logger.info(\"file stream finish called\")\nMessage: 'file stream finish called'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 331, in _finish\n    self._sm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1548, in finish\n    self._fs.finish(self._exit_code)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/file_stream.py\", line 599, in finish\n    logger.info(\"file stream finish is done\")\nMessage: 'file stream finish is done'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/service/streams.py\", line 48, in run\n    self._target(**self._kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 174, in wandb_internal\n    logger.error(f\"Thread {thread.name}:\", exc_info=exc_info)\nMessage: 'Thread WriterThread:'\nArguments: ()\nThread WriterThread:\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 100, in _run\n    self._process(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 380, in _process\n    self._wm.write(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/writer.py\", line 154, in write\n    write_handler(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/writer.py\", line 135, in _write\n    self._write_record(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/writer.py\", line 109, in _write_record\n    ret = self._ds.write(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/datastore.py\", line 289, in write\n    ret = self._write_data(s)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/datastore.py\", line 245, in _write_data\n    self._write_record(s)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/datastore.py\", line 224, in _write_record\n    self._fp.write(s)\nOSError: [Errno 28] No space left on device\nwandb: ERROR Internal wandb error: file data was not synced\n","output_type":"stream"}]},{"cell_type":"code","source":"logging.set_verbosity(logging.CRITICAL)\nSYSTEM_PROMPT = \"You are a helpful and joyous mental therapy assistant. Always answer as helpfully and cheerfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.If you don't know the answer to a question, please don't share false information. Always try to be as cheerfull as possible\"\n# Run text generation pipeline with our next model\nprompt = \"I am feeling suicidal.I have lost a lot of money in gambling. The money I used was taken as a loan\"\npipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=1000,temperature=0.9)\nresult = pipe(f\"<s>[INST]<<SYS>>{SYSTEM_PROMPT}<</SYS>> {prompt} [/INST]\")\nprint(result[0]['generated_text'])","metadata":{},"execution_count":null,"outputs":[]}]}