{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()\n\n#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","metadata":{"execution":{"iopub.status.busy":"2023-11-27T13:58:06.885158Z","iopub.execute_input":"2023-11-27T13:58:06.885589Z","iopub.status.idle":"2023-11-27T13:58:07.258349Z","shell.execute_reply.started":"2023-11-27T13:58:06.885535Z","shell.execute_reply":"2023-11-27T13:58:07.257502Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45ef2a91f98342bbb55fcb502783e2bb"}},"metadata":{}}]},{"cell_type":"code","source":"# authenticate WandB for logging metrics\nimport wandb\nwandb.login()\n# ee311bc313381f6bee4c5412a630fdad1175f3c0\n","metadata":{"execution":{"iopub.status.busy":"2023-11-27T13:58:13.398873Z","iopub.execute_input":"2023-11-27T13:58:13.399478Z","iopub.status.idle":"2023-11-27T13:58:19.010857Z","shell.execute_reply.started":"2023-11-27T13:58:13.399444Z","shell.execute_reply":"2023-11-27T13:58:19.009965Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"!pip install trl transformers accelerate git+https://github.com/huggingface/peft.git -Uqqq\n!pip install datasets bitsandbytes einops wandb -Uqqq","metadata":{"execution":{"iopub.status.busy":"2023-11-27T13:58:20.973228Z","iopub.execute_input":"2023-11-27T13:58:20.973756Z","iopub.status.idle":"2023-11-27T13:59:17.623506Z","shell.execute_reply.started":"2023-11-27T13:58:20.973720Z","shell.execute_reply":"2023-11-27T13:59:17.622446Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntokenizers 0.14.1 requires huggingface_hub<0.18,>=0.16.4, but you have huggingface-hub 0.19.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import os\n\nimport torch\nfrom datasets import load_dataset\nfrom peft import LoraConfig, get_peft_model, PeftConfig, PeftModel, prepare_model_for_kbit_training\nfrom transformers import (AutoModelForCausalLM, AutoTokenizer,\n                          BitsAndBytesConfig, HfArgumentParser,\n                          TrainingArguments,GenerationConfig, logging, pipeline)\nfrom trl import SFTTrainer","metadata":{"execution":{"iopub.status.busy":"2023-11-27T13:59:17.625732Z","iopub.execute_input":"2023-11-27T13:59:17.626079Z","iopub.status.idle":"2023-11-27T13:59:35.686291Z","shell.execute_reply.started":"2023-11-27T13:59:17.626044Z","shell.execute_reply":"2023-11-27T13:59:35.685370Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"model_name = \"HuggingFaceH4/zephyr-7b-beta\" # sharded Mistral-7b model\ndataset_name = \"vibhorag101/phr_mental_health_dataset\"\nnew_model = \"Zephyr7b-Beta-2000D-chat-hf-phr_mental_therapy\"\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,            # load model in 4-bit precision\n    bnb_4bit_quant_type=\"nf4\",    # pre-trained model should be quantized in 4-bit NF format\n    bnb_4bit_use_double_quant=True, # Using double quantization as mentioned in QLoRA paper\n    bnb_4bit_compute_dtype=torch.bfloat16, # During computation, pre-trained model should be loaded in BF16 format\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config, # Use bitsandbytes config\n    device_map=\"auto\",  # Specifying device_map=\"auto\" so that HF Accelerate will determine which GPU to put each layer of the model on\n    trust_remote_code=True, # Set trust_remote_code=True to use Mistral-7b model with custom code\n)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T13:59:35.687633Z","iopub.execute_input":"2023-11-27T13:59:35.688535Z","iopub.status.idle":"2023-11-27T14:01:45.050960Z","shell.execute_reply.started":"2023-11-27T13:59:35.688498Z","shell.execute_reply":"2023-11-27T14:01:45.050168Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading config.json:   0%|          | 0.00/638 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"694ee8f8ed54444882ddc0cd65a6ac4c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)fetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41f950642ea44f8daadc1bb5d48c8232"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8673e405cec946e8b471289381046054"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)of-00008.safetensors:   0%|          | 0.00/1.89G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee870c51aeff420d88d8fe551f08d7cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)of-00008.safetensors:   0%|          | 0.00/1.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ec8aea7bdd841648c938bbac4fe3369"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)of-00008.safetensors:   0%|          | 0.00/1.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15a92a6a7f2b4007839455e2c57b4ee7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)of-00008.safetensors:   0%|          | 0.00/1.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec0cf6cd165745669e9f87c4f335eaf7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)of-00008.safetensors:   0%|          | 0.00/1.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62bc32a7c27a4ab4b2df04997db3689f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)of-00008.safetensors:   0%|          | 0.00/1.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"221de72398a848c4b302b06f96dc24cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)of-00008.safetensors:   0%|          | 0.00/1.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61d6e305c5ab45349911b19b6e3fdd85"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)of-00008.safetensors:   0%|          | 0.00/816M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26c8f08db6c2485a8aa3487683e368e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b17a0ef5ac74d39aa202c689c02abb4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"909c4f72c3f546c4ba0c87e4388001c5"}},"metadata":{}}]},{"cell_type":"code","source":"dataset = load_dataset(dataset_name, split=\"train\")\ndataset = dataset.shuffle().select(range(2000))\ndataset.train_test_split(test_size=0.05)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T14:01:45.053636Z","iopub.execute_input":"2023-11-27T14:01:45.054327Z","iopub.status.idle":"2023-11-27T14:02:05.688861Z","shell.execute_reply.started":"2023-11-27T14:01:45.054282Z","shell.execute_reply":"2023-11-27T14:02:05.688058Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/460 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14fa07fbd153491dacfcf2448ab00c4d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d9051440b0c84ad381c094d0bd576123"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/211M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a606a8e637143f2ad56e947f43f0de0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55570df274fb4b7fafae840cd86294ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/99086 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85c07eda2d414ae9ac9e1bbf2bee31f9"}},"metadata":{}},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text'],\n        num_rows: 1900\n    })\n    test: Dataset({\n        features: ['text'],\n        num_rows: 100\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"print(dataset['text'][1])","metadata":{"execution":{"iopub.status.busy":"2023-11-27T14:02:05.689885Z","iopub.execute_input":"2023-11-27T14:02:05.690192Z","iopub.status.idle":"2023-11-27T14:02:05.726546Z","shell.execute_reply.started":"2023-11-27T14:02:05.690166Z","shell.execute_reply":"2023-11-27T14:02:05.725722Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"<s>[INST] <<SYS>>\nYou are a helpful and joyous mental therapy assistant. Always answer as helpfully and cheerfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.Please ensure that your responses are socially unbiased and positive in nature.\n\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n<</SYS>\n\nI've been feeling really down lately.  It seems like every time I try to mend my broken relationships with friends, conflict arises. It's hard to remain hopeful when these conflicts keep happening. [/INST] Hi, thank you for reaching out. I'm here to support you. I can imagine it's incredibly challenging to navigate conflict within your relationships. It's understandable that it would affect your mood and make it difficult to hold onto hope. How would you like to begin exploring this issue together? </s><s>[INST] I think the first step is to understand why these conflicts keep happening. I want to find a solution-focused approach to resolve these issues once and for all. The strain on my relationships is weighing me down, and I'm tired of feeling this way. [/INST] That's a great place to start.  Understanding the underlying causes of conflict can provide us with valuable insights. Can you give me an example of a recent conflict that you've experienced with a friend? </s><s>[INST] Well, there was this situation where my friend Sarah and I had a difference of opinion on a particular topic. Instead of having a healthy conversation, it turned into a heated argument. It escalated quickly, and now our friendship feels strained. I'm not sure how to bridge the gap. [/INST] It sounds like miscommunication played a major role in the conflict. When opinions clash, things can quickly become heated. Are you open to exploring different communication techniques that can help you navigate these situations more effectively? </s><s>[INST] Absolutely.  I'm willing to try anything at this point. I just want to salvage my relationships and create a more harmonious dynamic among my friends. It's been taking a toll on my well-being. [/INST] That's commendable.  Let's focus on understanding Sarah's perspective first. Have you tried actively listening to her concerns and feelings during conflicts? </s><s>[INST] I must admit, I struggle with active listening during conflicts. I tend to get defensive and caught up in my own thoughts and emotions. Perhaps that's preventing me from truly understanding where she's coming from. [/INST] It's common to get caught up in our own emotions during conflicts.  The next time you find yourself engaged in a disagreement, try taking a step back and actively listen to the other person. Ask open-ended questions to gain a deeper understanding of their perspective. This approach can encourage healthy dialogue rather than escalating the conflict further. </s><s>[INST] I will definitely give that a try.  It makes sense that open communication can help us find common ground and resolve conflicts. I've been so focused on winning arguments in the past that I've lost sight of the importance of maintaining relationships. [/INST] I'm glad you're open to trying a different approach.  Remember, conflicts are a part of any relationship, but how we handle them can make all the difference. It's important to prioritize the relationship itself instead of winning individual arguments. By adopting a more solution-focused mindset, you can work towards building healthier and stronger connections. </s><s>[INST] I'm hopeful that by actively listening, asking the right questions, and prioritizing the relationship, I can mend the conflicts I've been experiencing with my friends. Thank you,, for guiding me towards a better understanding and providing me with tools to navigate these challenges. I feel lighter already. [/INST] You're very welcome.  I'm glad I could support you in your journey towards resolving conflicts and finding hope. Remember, therapy is here for you whenever you need it. I believe in your capacity to mend these relationships and create a more positive dynamic. Have faith in yourself too. </s>\n","output_type":"stream"}]},{"cell_type":"code","source":"\n\nlora_alpha = 32 # scaling factor for the weight matrices\nlora_dropout = 0.05 # dropout probability of the LoRA layers\nlora_rank = 16 # dimension of the low-rank matrices\n\npeft_config = LoraConfig(\n    lora_alpha=lora_alpha,\n    lora_dropout=lora_dropout,\n    r=lora_rank,\n    bias=\"none\",  # setting to 'none' for only training weight params instead of biases\n    task_type=\"CAUSAL_LM\",\n    \n)\n\npeft_model = get_peft_model(model, peft_config)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T14:02:05.727593Z","iopub.execute_input":"2023-11-27T14:02:05.730082Z","iopub.status.idle":"2023-11-27T14:02:05.899514Z","shell.execute_reply.started":"2023-11-27T14:02:05.730016Z","shell.execute_reply":"2023-11-27T14:02:05.898514Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"output_dir = \"./Zephyr7b-Beta-2000D-chat-hf-phr_mental_therapy\"\nper_device_train_batch_size = 2 # reduce batch size by 2x only if out-of-memory error\ngradient_accumulation_steps = 1  # increase gradient accumulation steps by  2x only if batch size is reduced\noptim = \"paged_adamw_32bit\" # activates the paging for better memory management\nsave_strategy=\"steps\" # checkpoint save strategy to adopt during training\nsave_steps = 10 # number of updates steps before two checkpoint saves\nlogging_steps = 10  # number of update steps between two logs if logging_strategy=\"steps\"\nlearning_rate = 2e-4  # learning rate for AdamW optimizer\nmax_grad_norm = 0.3 # maximum gradient norm (for gradient clipping)\nmax_steps = -1      # training will happen for automatic steps\nwarmup_ratio = 0.03 # number of steps used for a linear warmup from 0 to learning_rate\nlr_scheduler_type = \"cosine\"  # learning rate scheduler\nnum_train_epochs = 2\nper_device_eval_batch_size = 2\nmax_seq_length=256\n\ntraining_arguments = TrainingArguments(\n    output_dir=output_dir,\n    per_device_train_batch_size=per_device_train_batch_size,\n    gradient_accumulation_steps=gradient_accumulation_steps,\n    optim=optim,\n    num_train_epochs= num_train_epochs,\n    save_steps=save_steps,\n    logging_steps=logging_steps,\n    learning_rate=learning_rate,\n    fp16=True,\n    max_grad_norm=max_grad_norm,\n    max_steps=max_steps,\n    warmup_ratio=warmup_ratio,\n    group_by_length=True,\n    lr_scheduler_type=lr_scheduler_type,\n    resume_from_checkpoint=True,\n    push_to_hub=True,\n    \n)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T14:02:05.900789Z","iopub.execute_input":"2023-11-27T14:02:05.901520Z","iopub.status.idle":"2023-11-27T14:02:05.912503Z","shell.execute_reply.started":"2023-11-27T14:02:05.901488Z","shell.execute_reply":"2023-11-27T14:02:05.911720Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=\"auto\"\n)\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\n\n# # Load LLaMA tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True,use_fast=False,add_eos_token=True)\ntokenizer.pad_token_id = 18610","metadata":{"execution":{"iopub.status.busy":"2023-11-27T14:03:04.134068Z","iopub.execute_input":"2023-11-27T14:03:04.134405Z","iopub.status.idle":"2023-11-27T14:03:21.189163Z","shell.execute_reply.started":"2023-11-27T14:03:04.134376Z","shell.execute_reply":"2023-11-27T14:03:21.188013Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2d79f7817094f1bacb8e4f531e03f06"}},"metadata":{}}]},{"cell_type":"code","source":"#Set supervised fine-tuning parameters\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset,\n    peft_config=peft_config,\n    dataset_text_field=\"text\",\n    max_seq_length=256,\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing=False,\n)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-27T14:03:21.192726Z","iopub.execute_input":"2023-11-27T14:03:21.193212Z","iopub.status.idle":"2023-11-27T14:03:38.630100Z","shell.execute_reply.started":"2023-11-27T14:03:21.193174Z","shell.execute_reply":"2023-11-27T14:03:38.629301Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca3d90cf88144808988ec180756fe1ba"}},"metadata":{}}]},{"cell_type":"code","source":"# upcasting the layer norms in torch.bfloat16 for more stable training\nfor name, module in trainer.model.named_modules():\n    if \"norm\" in name:\n        module = module.to(torch.bfloat16)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T14:03:38.631217Z","iopub.execute_input":"2023-11-27T14:03:38.631516Z","iopub.status.idle":"2023-11-27T14:03:38.642657Z","shell.execute_reply.started":"2023-11-27T14:03:38.631491Z","shell.execute_reply":"2023-11-27T14:03:38.641932Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"model.config.use_cache = False\ntrainer.train()\ntrainer.model.save_pretrained(new_model)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T14:03:38.643596Z","iopub.execute_input":"2023-11-27T14:03:38.643872Z","iopub.status.idle":"2023-11-27T14:59:03.427106Z","shell.execute_reply.started":"2023-11-27T14:03:38.643848Z","shell.execute_reply":"2023-11-27T14:59:03.425109Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mr1ajayakumaran1751994\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20231127_140340-41wlqhvk</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/r1ajayakumaran1751994/huggingface/runs/41wlqhvk' target=\"_blank\">peach-bird-29</a></strong> to <a href='https://wandb.ai/r1ajayakumaran1751994/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/r1ajayakumaran1751994/huggingface' target=\"_blank\">https://wandb.ai/r1ajayakumaran1751994/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/r1ajayakumaran1751994/huggingface/runs/41wlqhvk' target=\"_blank\">https://wandb.ai/r1ajayakumaran1751994/huggingface/runs/41wlqhvk</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2000' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2000/2000 54:45, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>1.986500</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>1.785700</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>1.543500</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>1.384900</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>1.311100</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>1.210400</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>1.195100</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>1.178500</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>1.183300</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.168900</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>1.099800</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>1.078800</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>1.113800</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>1.103200</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>1.138700</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>1.092300</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>1.025400</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>1.073100</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>1.045400</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.056200</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>1.062200</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>1.072200</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>1.109400</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>1.031900</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>1.089100</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>1.121100</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>1.023500</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>1.114700</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>1.043000</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>1.105200</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>1.063100</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>1.024000</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>1.059500</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>1.005200</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>1.076100</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>1.020400</td>\n    </tr>\n    <tr>\n      <td>370</td>\n      <td>1.073600</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>1.027900</td>\n    </tr>\n    <tr>\n      <td>390</td>\n      <td>1.013200</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>1.053900</td>\n    </tr>\n    <tr>\n      <td>410</td>\n      <td>1.011000</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>1.001000</td>\n    </tr>\n    <tr>\n      <td>430</td>\n      <td>1.035100</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>1.040500</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>1.041900</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>0.998700</td>\n    </tr>\n    <tr>\n      <td>470</td>\n      <td>1.020400</td>\n    </tr>\n    <tr>\n      <td>480</td>\n      <td>1.054900</td>\n    </tr>\n    <tr>\n      <td>490</td>\n      <td>1.013700</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>1.180200</td>\n    </tr>\n    <tr>\n      <td>510</td>\n      <td>1.021200</td>\n    </tr>\n    <tr>\n      <td>520</td>\n      <td>1.005000</td>\n    </tr>\n    <tr>\n      <td>530</td>\n      <td>1.046700</td>\n    </tr>\n    <tr>\n      <td>540</td>\n      <td>1.037900</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>1.107800</td>\n    </tr>\n    <tr>\n      <td>560</td>\n      <td>0.969600</td>\n    </tr>\n    <tr>\n      <td>570</td>\n      <td>1.060600</td>\n    </tr>\n    <tr>\n      <td>580</td>\n      <td>0.975400</td>\n    </tr>\n    <tr>\n      <td>590</td>\n      <td>1.038000</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>1.037400</td>\n    </tr>\n    <tr>\n      <td>610</td>\n      <td>1.012300</td>\n    </tr>\n    <tr>\n      <td>620</td>\n      <td>1.025700</td>\n    </tr>\n    <tr>\n      <td>630</td>\n      <td>0.980600</td>\n    </tr>\n    <tr>\n      <td>640</td>\n      <td>0.962100</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>1.072500</td>\n    </tr>\n    <tr>\n      <td>660</td>\n      <td>1.005700</td>\n    </tr>\n    <tr>\n      <td>670</td>\n      <td>1.044900</td>\n    </tr>\n    <tr>\n      <td>680</td>\n      <td>0.998800</td>\n    </tr>\n    <tr>\n      <td>690</td>\n      <td>1.038200</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>1.016300</td>\n    </tr>\n    <tr>\n      <td>710</td>\n      <td>1.031600</td>\n    </tr>\n    <tr>\n      <td>720</td>\n      <td>0.996500</td>\n    </tr>\n    <tr>\n      <td>730</td>\n      <td>0.965000</td>\n    </tr>\n    <tr>\n      <td>740</td>\n      <td>0.976000</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>1.160600</td>\n    </tr>\n    <tr>\n      <td>760</td>\n      <td>0.936300</td>\n    </tr>\n    <tr>\n      <td>770</td>\n      <td>0.983800</td>\n    </tr>\n    <tr>\n      <td>780</td>\n      <td>0.998600</td>\n    </tr>\n    <tr>\n      <td>790</td>\n      <td>0.964300</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.999900</td>\n    </tr>\n    <tr>\n      <td>810</td>\n      <td>0.987800</td>\n    </tr>\n    <tr>\n      <td>820</td>\n      <td>0.997400</td>\n    </tr>\n    <tr>\n      <td>830</td>\n      <td>1.004600</td>\n    </tr>\n    <tr>\n      <td>840</td>\n      <td>0.999800</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>1.027200</td>\n    </tr>\n    <tr>\n      <td>860</td>\n      <td>0.960700</td>\n    </tr>\n    <tr>\n      <td>870</td>\n      <td>0.985700</td>\n    </tr>\n    <tr>\n      <td>880</td>\n      <td>0.962400</td>\n    </tr>\n    <tr>\n      <td>890</td>\n      <td>0.944500</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>1.015100</td>\n    </tr>\n    <tr>\n      <td>910</td>\n      <td>1.012500</td>\n    </tr>\n    <tr>\n      <td>920</td>\n      <td>0.997100</td>\n    </tr>\n    <tr>\n      <td>930</td>\n      <td>1.001600</td>\n    </tr>\n    <tr>\n      <td>940</td>\n      <td>0.994800</td>\n    </tr>\n    <tr>\n      <td>950</td>\n      <td>1.011000</td>\n    </tr>\n    <tr>\n      <td>960</td>\n      <td>0.977700</td>\n    </tr>\n    <tr>\n      <td>970</td>\n      <td>0.938300</td>\n    </tr>\n    <tr>\n      <td>980</td>\n      <td>0.979400</td>\n    </tr>\n    <tr>\n      <td>990</td>\n      <td>0.949000</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.992500</td>\n    </tr>\n    <tr>\n      <td>1010</td>\n      <td>0.939400</td>\n    </tr>\n    <tr>\n      <td>1020</td>\n      <td>0.859200</td>\n    </tr>\n    <tr>\n      <td>1030</td>\n      <td>0.890100</td>\n    </tr>\n    <tr>\n      <td>1040</td>\n      <td>0.904000</td>\n    </tr>\n    <tr>\n      <td>1050</td>\n      <td>0.898700</td>\n    </tr>\n    <tr>\n      <td>1060</td>\n      <td>0.887000</td>\n    </tr>\n    <tr>\n      <td>1070</td>\n      <td>0.826700</td>\n    </tr>\n    <tr>\n      <td>1080</td>\n      <td>0.856800</td>\n    </tr>\n    <tr>\n      <td>1090</td>\n      <td>0.850300</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.880800</td>\n    </tr>\n    <tr>\n      <td>1110</td>\n      <td>0.907200</td>\n    </tr>\n    <tr>\n      <td>1120</td>\n      <td>0.894600</td>\n    </tr>\n    <tr>\n      <td>1130</td>\n      <td>0.832600</td>\n    </tr>\n    <tr>\n      <td>1140</td>\n      <td>0.859900</td>\n    </tr>\n    <tr>\n      <td>1150</td>\n      <td>0.865900</td>\n    </tr>\n    <tr>\n      <td>1160</td>\n      <td>0.857500</td>\n    </tr>\n    <tr>\n      <td>1170</td>\n      <td>0.856400</td>\n    </tr>\n    <tr>\n      <td>1180</td>\n      <td>0.911300</td>\n    </tr>\n    <tr>\n      <td>1190</td>\n      <td>0.883000</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.865400</td>\n    </tr>\n    <tr>\n      <td>1210</td>\n      <td>0.939900</td>\n    </tr>\n    <tr>\n      <td>1220</td>\n      <td>0.858200</td>\n    </tr>\n    <tr>\n      <td>1230</td>\n      <td>0.897100</td>\n    </tr>\n    <tr>\n      <td>1240</td>\n      <td>0.879200</td>\n    </tr>\n    <tr>\n      <td>1250</td>\n      <td>0.854100</td>\n    </tr>\n    <tr>\n      <td>1260</td>\n      <td>0.878100</td>\n    </tr>\n    <tr>\n      <td>1270</td>\n      <td>0.831200</td>\n    </tr>\n    <tr>\n      <td>1280</td>\n      <td>0.862100</td>\n    </tr>\n    <tr>\n      <td>1290</td>\n      <td>0.878900</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>0.898600</td>\n    </tr>\n    <tr>\n      <td>1310</td>\n      <td>0.879200</td>\n    </tr>\n    <tr>\n      <td>1320</td>\n      <td>0.943800</td>\n    </tr>\n    <tr>\n      <td>1330</td>\n      <td>0.856400</td>\n    </tr>\n    <tr>\n      <td>1340</td>\n      <td>0.889400</td>\n    </tr>\n    <tr>\n      <td>1350</td>\n      <td>0.826100</td>\n    </tr>\n    <tr>\n      <td>1360</td>\n      <td>0.830000</td>\n    </tr>\n    <tr>\n      <td>1370</td>\n      <td>0.860400</td>\n    </tr>\n    <tr>\n      <td>1380</td>\n      <td>0.914100</td>\n    </tr>\n    <tr>\n      <td>1390</td>\n      <td>0.881200</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.835700</td>\n    </tr>\n    <tr>\n      <td>1410</td>\n      <td>0.895700</td>\n    </tr>\n    <tr>\n      <td>1420</td>\n      <td>0.828200</td>\n    </tr>\n    <tr>\n      <td>1430</td>\n      <td>0.872400</td>\n    </tr>\n    <tr>\n      <td>1440</td>\n      <td>0.895900</td>\n    </tr>\n    <tr>\n      <td>1450</td>\n      <td>0.847100</td>\n    </tr>\n    <tr>\n      <td>1460</td>\n      <td>0.880900</td>\n    </tr>\n    <tr>\n      <td>1470</td>\n      <td>0.859600</td>\n    </tr>\n    <tr>\n      <td>1480</td>\n      <td>0.925900</td>\n    </tr>\n    <tr>\n      <td>1490</td>\n      <td>0.851600</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.781300</td>\n    </tr>\n    <tr>\n      <td>1510</td>\n      <td>0.858400</td>\n    </tr>\n    <tr>\n      <td>1520</td>\n      <td>0.923900</td>\n    </tr>\n    <tr>\n      <td>1530</td>\n      <td>0.839400</td>\n    </tr>\n    <tr>\n      <td>1540</td>\n      <td>0.838200</td>\n    </tr>\n    <tr>\n      <td>1550</td>\n      <td>0.877200</td>\n    </tr>\n    <tr>\n      <td>1560</td>\n      <td>0.861900</td>\n    </tr>\n    <tr>\n      <td>1570</td>\n      <td>0.866400</td>\n    </tr>\n    <tr>\n      <td>1580</td>\n      <td>0.836200</td>\n    </tr>\n    <tr>\n      <td>1590</td>\n      <td>0.849700</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>0.856500</td>\n    </tr>\n    <tr>\n      <td>1610</td>\n      <td>0.832100</td>\n    </tr>\n    <tr>\n      <td>1620</td>\n      <td>0.884300</td>\n    </tr>\n    <tr>\n      <td>1630</td>\n      <td>0.915900</td>\n    </tr>\n    <tr>\n      <td>1640</td>\n      <td>0.871500</td>\n    </tr>\n    <tr>\n      <td>1650</td>\n      <td>0.854200</td>\n    </tr>\n    <tr>\n      <td>1660</td>\n      <td>0.864900</td>\n    </tr>\n    <tr>\n      <td>1670</td>\n      <td>0.864600</td>\n    </tr>\n    <tr>\n      <td>1680</td>\n      <td>0.850400</td>\n    </tr>\n    <tr>\n      <td>1690</td>\n      <td>0.862400</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>0.861800</td>\n    </tr>\n    <tr>\n      <td>1710</td>\n      <td>0.868200</td>\n    </tr>\n    <tr>\n      <td>1720</td>\n      <td>0.857700</td>\n    </tr>\n    <tr>\n      <td>1730</td>\n      <td>0.858900</td>\n    </tr>\n    <tr>\n      <td>1740</td>\n      <td>0.853000</td>\n    </tr>\n    <tr>\n      <td>1750</td>\n      <td>0.814200</td>\n    </tr>\n    <tr>\n      <td>1760</td>\n      <td>0.836500</td>\n    </tr>\n    <tr>\n      <td>1770</td>\n      <td>0.875500</td>\n    </tr>\n    <tr>\n      <td>1780</td>\n      <td>0.823800</td>\n    </tr>\n    <tr>\n      <td>1790</td>\n      <td>0.847000</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>0.858000</td>\n    </tr>\n    <tr>\n      <td>1810</td>\n      <td>0.878800</td>\n    </tr>\n    <tr>\n      <td>1820</td>\n      <td>0.868500</td>\n    </tr>\n    <tr>\n      <td>1830</td>\n      <td>0.819300</td>\n    </tr>\n    <tr>\n      <td>1840</td>\n      <td>0.865100</td>\n    </tr>\n    <tr>\n      <td>1850</td>\n      <td>0.857300</td>\n    </tr>\n    <tr>\n      <td>1860</td>\n      <td>0.838800</td>\n    </tr>\n    <tr>\n      <td>1870</td>\n      <td>0.815500</td>\n    </tr>\n    <tr>\n      <td>1880</td>\n      <td>0.820000</td>\n    </tr>\n    <tr>\n      <td>1890</td>\n      <td>0.832000</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>0.842400</td>\n    </tr>\n    <tr>\n      <td>1910</td>\n      <td>0.846600</td>\n    </tr>\n    <tr>\n      <td>1920</td>\n      <td>0.841300</td>\n    </tr>\n    <tr>\n      <td>1930</td>\n      <td>0.792400</td>\n    </tr>\n    <tr>\n      <td>1940</td>\n      <td>0.818600</td>\n    </tr>\n    <tr>\n      <td>1950</td>\n      <td>0.820500</td>\n    </tr>\n    <tr>\n      <td>1960</td>\n      <td>0.879300</td>\n    </tr>\n    <tr>\n      <td>1970</td>\n      <td>0.884400</td>\n    </tr>\n    <tr>\n      <td>1980</td>\n      <td>0.892800</td>\n    </tr>\n    <tr>\n      <td>1990</td>\n      <td>0.810700</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.883400</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}]},{"cell_type":"code","source":"trainer.push_to_hub()","metadata":{"execution":{"iopub.status.busy":"2023-11-27T15:05:55.427859Z","iopub.execute_input":"2023-11-27T15:05:55.428263Z","iopub.status.idle":"2023-11-27T15:05:56.329200Z","shell.execute_reply.started":"2023-11-27T15:05:55.428231Z","shell.execute_reply":"2023-11-27T15:05:56.328114Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"'https://huggingface.co/Jaykumaran17/Zephyr7b-Beta-2000D-chat-hf-phr_mental_therapy/tree/main/'"},"metadata":{}}]},{"cell_type":"markdown","source":"# Merging as Single without Adapters","metadata":{}},{"cell_type":"code","source":"# Reload model in FP16 and merge it with LoRA weights\nbase_model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    low_cpu_mem_usage=True,\n    return_dict=True,\n    torch_dtype=torch.float16,\n    device_map=\"cpu\", ## device map = \"cpu\", for merging on cpu\n)\nmodel = PeftModel.from_pretrained(base_model, new_model)\nmodel = model.merge_and_unload()\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True,use_fast=False,add_eos_token=True)\ntokenizer.pad_token_id = 18610\n\n### Old version tokeniser, can't generate eos token\n# tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n# tokenizer.pad_token = tokenizer.eos_token\n# tokenizer.padding_side = \"right\"\n\n","metadata":{"execution":{"iopub.status.busy":"2023-11-27T15:01:35.296220Z","iopub.execute_input":"2023-11-27T15:01:35.296642Z","iopub.status.idle":"2023-11-27T15:01:59.405917Z","shell.execute_reply.started":"2023-11-27T15:01:35.296609Z","shell.execute_reply":"2023-11-27T15:01:59.404784Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e7603ec02344c5db84708978bc9e0d7"}},"metadata":{}}]},{"cell_type":"code","source":"## for inference without merging qlora weights with original\nbase_model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=\"auto\"\n)\nmodel = PeftModel.from_pretrained(base_model, new_model)\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True,use_fast=False,add_eos_token=True)\ntokenizer.pad_token_id = 18610","metadata":{"execution":{"iopub.status.busy":"2023-11-27T15:02:26.607363Z","iopub.execute_input":"2023-11-27T15:02:26.607712Z","iopub.status.idle":"2023-11-27T15:03:18.855712Z","shell.execute_reply.started":"2023-11-27T15:02:26.607686Z","shell.execute_reply":"2023-11-27T15:03:18.854520Z"},"trusted":true},"execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"abd838b4e2ea49728d3d3c0d51a9b633"}},"metadata":{}}]},{"cell_type":"code","source":"model.push_to_hub(new_model, use_temp_dir=False)\ntokenizer.push_to_hub(new_model, use_temp_dir=False)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T15:03:18.857832Z","iopub.execute_input":"2023-11-27T15:03:18.858311Z","iopub.status.idle":"2023-11-27T15:03:20.390956Z","shell.execute_reply.started":"2023-11-27T15:03:18.858250Z","shell.execute_reply":"2023-11-27T15:03:20.389350Z"},"trusted":true},"execution_count":20,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py:261\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhf_raise_for_status\u001b[39m(response: Response, endpoint_name: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;124;03m    Internal version of `response.raise_for_status()` that will refine a\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;124;03m    potential HTTPError. Raised exception will be an instance of `HfHubHTTPError`.\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \n\u001b[1;32m    219\u001b[0m \u001b[38;5;124;03m    This helper is meant to be the unique method to raise_for_status when making a call\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;124;03m    to the Hugging Face Hub.\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \n\u001b[1;32m    222\u001b[0m \u001b[38;5;124;03m    Example:\u001b[39;00m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;124;03m    ```py\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;124;03m        import requests\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;124;03m        from huggingface_hub.utils import get_session, hf_raise_for_status, HfHubHTTPError\u001b[39;00m\n\u001b[1;32m    226\u001b[0m \n\u001b[1;32m    227\u001b[0m \u001b[38;5;124;03m        response = get_session().post(...)\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;124;03m        try:\u001b[39;00m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;124;03m            hf_raise_for_status(response)\u001b[39;00m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;124;03m        except HfHubHTTPError as e:\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;124;03m            print(str(e)) # formatted message\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;124;03m            e.request_id, e.server_message # details returned by server\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \n\u001b[1;32m    234\u001b[0m \u001b[38;5;124;03m            # Complete the error message with additional information once it's raised\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;124;03m            e.append_to_message(\"\\n`create_commit` expects the repository to exist.\")\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;124;03m            raise\u001b[39;00m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \n\u001b[1;32m    239\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;124;03m        response (`Response`):\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;124;03m            Response from the server.\u001b[39;00m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;124;03m        endpoint_name (`str`, *optional*):\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;124;03m            Name of the endpoint that has been called. If provided, the error message\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;124;03m            will be more complete.\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \n\u001b[1;32m    246\u001b[0m \u001b[38;5;124;03m    <Tip warning={true}>\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \n\u001b[1;32m    248\u001b[0m \u001b[38;5;124;03m    Raises when the request has failed:\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \n\u001b[1;32m    250\u001b[0m \u001b[38;5;124;03m        - [`~utils.RepositoryNotFoundError`]\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;124;03m            If the repository to download from cannot be found. This may be because it\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;124;03m            doesn't exist, because `repo_type` is not set correctly, or because the repo\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;124;03m            is `private` and you do not have access.\u001b[39;00m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;124;03m        - [`~utils.GatedRepoError`]\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;124;03m            If the repository exists but is gated and the user is not on the authorized\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;124;03m            list.\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;124;03m        - [`~utils.RevisionNotFoundError`]\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;124;03m            If the repository exists but the revision couldn't be find.\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;124;03m        - [`~utils.EntryNotFoundError`]\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;124;03m            If the repository exists but the entry (e.g. the requested file) couldn't be\u001b[39;00m\n\u001b[0;32m--> 261\u001b[0m \u001b[38;5;124;03m            find.\u001b[39;00m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;124;03m        - [`~utils.BadRequestError`]\u001b[39;00m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;124;03m            If request failed with a HTTP 400 BadRequest error.\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;124;03m        - [`~utils.HfHubHTTPError`]\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;124;03m            If request failed for a reason not listed above.\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \n\u001b[1;32m    267\u001b[0m \u001b[38;5;124;03m    </Tip>\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n","\u001b[0;31mHTTPError\u001b[0m: 429 Client Error: Too Many Requests for url: https://huggingface.co/api/models/Jaykumaran17/Zephyr7b-Beta-2000D-chat-hf-phr_mental_therapy/commit/main","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpush_to_hub\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_temp_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mpush_to_hub(new_model, use_temp_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:905\u001b[0m, in \u001b[0;36mPushToHubMixin.push_to_hub\u001b[0;34m(self, repo_id, use_temp_dir, commit_message, private, token, max_shard_size, create_pr, safe_serialization, revision, commit_description, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;66;03m# Save all files.\u001b[39;00m\n\u001b[1;32m    903\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_pretrained(work_dir, max_shard_size\u001b[38;5;241m=\u001b[39mmax_shard_size, safe_serialization\u001b[38;5;241m=\u001b[39msafe_serialization)\n\u001b[0;32m--> 905\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_upload_modified_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    906\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwork_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    907\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    908\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiles_timestamps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    909\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcommit_message\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_message\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    910\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    911\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_pr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_pr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    912\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    913\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcommit_description\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_description\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    914\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:781\u001b[0m, in \u001b[0;36mPushToHubMixin._upload_modified_files\u001b[0;34m(self, working_dir, repo_id, files_timestamps, commit_message, token, create_pr, revision, commit_description)\u001b[0m\n\u001b[1;32m    778\u001b[0m     create_branch(repo_id\u001b[38;5;241m=\u001b[39mrepo_id, branch\u001b[38;5;241m=\u001b[39mrevision, token\u001b[38;5;241m=\u001b[39mtoken, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    780\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUploading the following files to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(modified_files)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 781\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcreate_commit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    783\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    784\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcommit_message\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_message\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcommit_description\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_description\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_pr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_pr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/hf_api.py:849\u001b[0m, in \u001b[0;36m_inner\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    833\u001b[0m \u001b[38;5;129m@dataclass\u001b[39m\n\u001b[1;32m    834\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCollection\u001b[39;00m:\n\u001b[1;32m    835\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    836\u001b[0m \u001b[38;5;124;03m    Contains information about a Collection on the Hub.\u001b[39;00m\n\u001b[1;32m    837\u001b[0m \n\u001b[1;32m    838\u001b[0m \u001b[38;5;124;03m    Attributes:\u001b[39;00m\n\u001b[1;32m    839\u001b[0m \u001b[38;5;124;03m        slug (`str`):\u001b[39;00m\n\u001b[1;32m    840\u001b[0m \u001b[38;5;124;03m            Slug of the collection. E.g. `\"TheBloke/recent-models-64f9a55bb3115b4f513ec026\"`.\u001b[39;00m\n\u001b[1;32m    841\u001b[0m \u001b[38;5;124;03m        title (`str`):\u001b[39;00m\n\u001b[1;32m    842\u001b[0m \u001b[38;5;124;03m            Title of the collection. E.g. `\"Recent models\"`.\u001b[39;00m\n\u001b[1;32m    843\u001b[0m \u001b[38;5;124;03m        owner (`str`):\u001b[39;00m\n\u001b[1;32m    844\u001b[0m \u001b[38;5;124;03m            Owner of the collection. E.g. `\"TheBloke\"`.\u001b[39;00m\n\u001b[1;32m    845\u001b[0m \u001b[38;5;124;03m        items (`List[CollectionItem]`):\u001b[39;00m\n\u001b[1;32m    846\u001b[0m \u001b[38;5;124;03m            List of items in the collection.\u001b[39;00m\n\u001b[1;32m    847\u001b[0m \u001b[38;5;124;03m        last_updated (`datetime`):\u001b[39;00m\n\u001b[1;32m    848\u001b[0m \u001b[38;5;124;03m            Date of the last update of the collection.\u001b[39;00m\n\u001b[0;32m--> 849\u001b[0m \u001b[38;5;124;03m        position (`int`):\u001b[39;00m\n\u001b[1;32m    850\u001b[0m \u001b[38;5;124;03m            Position of the collection in the list of collections of the owner.\u001b[39;00m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;124;03m        private (`bool`):\u001b[39;00m\n\u001b[1;32m    852\u001b[0m \u001b[38;5;124;03m            Whether the collection is private or not.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m \u001b[38;5;124;03m        theme (`str`):\u001b[39;00m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;124;03m            Theme of the collection. E.g. `\"green\"`.\u001b[39;00m\n\u001b[1;32m    855\u001b[0m \u001b[38;5;124;03m        upvotes (`int`):\u001b[39;00m\n\u001b[1;32m    856\u001b[0m \u001b[38;5;124;03m            Number of upvotes of the collection.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m \u001b[38;5;124;03m        description (`str`, *optional*):\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;124;03m            Description of the collection, as plain text.\u001b[39;00m\n\u001b[1;32m    859\u001b[0m \u001b[38;5;124;03m        url (`str`):\u001b[39;00m\n\u001b[1;32m    860\u001b[0m \u001b[38;5;124;03m            (property) URL of the collection on the Hub.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    863\u001b[0m     slug: \u001b[38;5;28mstr\u001b[39m\n\u001b[1;32m    864\u001b[0m     title: \u001b[38;5;28mstr\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/hf_api.py:2967\u001b[0m, in \u001b[0;36mcreate_commit\u001b[0;34m(self, repo_id, operations, commit_message, commit_description, token, repo_type, revision, create_pr, num_threads, parent_commit, run_as_future)\u001b[0m\n\u001b[1;32m   2943\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmove_repo\u001b[39m(\n\u001b[1;32m   2944\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   2945\u001b[0m     from_id: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2949\u001b[0m     token: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   2950\u001b[0m ):\n\u001b[1;32m   2951\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2952\u001b[0m \u001b[38;5;124;03m    Moving a repository from namespace1/repo_name1 to namespace2/repo_name2\u001b[39;00m\n\u001b[1;32m   2953\u001b[0m \n\u001b[1;32m   2954\u001b[0m \u001b[38;5;124;03m    Note there are certain limitations. For more information about moving\u001b[39;00m\n\u001b[1;32m   2955\u001b[0m \u001b[38;5;124;03m    repositories, please see\u001b[39;00m\n\u001b[1;32m   2956\u001b[0m \u001b[38;5;124;03m    https://hf.co/docs/hub/repositories-settings#renaming-or-transferring-a-repo.\u001b[39;00m\n\u001b[1;32m   2957\u001b[0m \n\u001b[1;32m   2958\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   2959\u001b[0m \u001b[38;5;124;03m        from_id (`str`):\u001b[39;00m\n\u001b[1;32m   2960\u001b[0m \u001b[38;5;124;03m            A namespace (user or an organization) and a repo name separated\u001b[39;00m\n\u001b[1;32m   2961\u001b[0m \u001b[38;5;124;03m            by a `/`. Original repository identifier.\u001b[39;00m\n\u001b[1;32m   2962\u001b[0m \u001b[38;5;124;03m        to_id (`str`):\u001b[39;00m\n\u001b[1;32m   2963\u001b[0m \u001b[38;5;124;03m            A namespace (user or an organization) and a repo name separated\u001b[39;00m\n\u001b[1;32m   2964\u001b[0m \u001b[38;5;124;03m            by a `/`. Final repository identifier.\u001b[39;00m\n\u001b[1;32m   2965\u001b[0m \u001b[38;5;124;03m        repo_type (`str`, *optional*):\u001b[39;00m\n\u001b[1;32m   2966\u001b[0m \u001b[38;5;124;03m            Set to `\"dataset\"` or `\"space\"` if uploading to a dataset or\u001b[39;00m\n\u001b[0;32m-> 2967\u001b[0m \u001b[38;5;124;03m            space, `None` or `\"model\"` if uploading to a model. Default is\u001b[39;00m\n\u001b[1;32m   2968\u001b[0m \u001b[38;5;124;03m            `None`.\u001b[39;00m\n\u001b[1;32m   2969\u001b[0m \u001b[38;5;124;03m        token (`str`, *optional*):\u001b[39;00m\n\u001b[1;32m   2970\u001b[0m \u001b[38;5;124;03m            An authentication token (See https://huggingface.co/settings/token)\u001b[39;00m\n\u001b[1;32m   2971\u001b[0m \n\u001b[1;32m   2972\u001b[0m \u001b[38;5;124;03m    <Tip>\u001b[39;00m\n\u001b[1;32m   2973\u001b[0m \n\u001b[1;32m   2974\u001b[0m \u001b[38;5;124;03m    Raises the following errors:\u001b[39;00m\n\u001b[1;32m   2975\u001b[0m \n\u001b[1;32m   2976\u001b[0m \u001b[38;5;124;03m        - [`~utils.RepositoryNotFoundError`]\u001b[39;00m\n\u001b[1;32m   2977\u001b[0m \u001b[38;5;124;03m          If the repository to download from cannot be found. This may be because it doesn't exist,\u001b[39;00m\n\u001b[1;32m   2978\u001b[0m \u001b[38;5;124;03m          or because it is set to `private` and you do not have access.\u001b[39;00m\n\u001b[1;32m   2979\u001b[0m \n\u001b[1;32m   2980\u001b[0m \u001b[38;5;124;03m    </Tip>\u001b[39;00m\n\u001b[1;32m   2981\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   2982\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(from_id\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m   2983\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid repo_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfrom_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. It should have a namespace (:namespace:/:repo_name:)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py:303\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    288\u001b[0m elif (\n\u001b[1;32m    289\u001b[0m     response.status_code == 401\n\u001b[1;32m    290\u001b[0m     and response.request.url is not None\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    293\u001b[0m     # Collection not found. We don't raise a custom error for this.\n\u001b[1;32m    294\u001b[0m     # This prevent from raising a misleading `RepositoryNotFoundError` (see below).\n\u001b[1;32m    295\u001b[0m     pass\n\u001b[1;32m    297\u001b[0m elif (\n\u001b[1;32m    298\u001b[0m     response.status_code == 401\n\u001b[1;32m    299\u001b[0m     and response.request.url is not None\n\u001b[1;32m    300\u001b[0m     and INFERENCE_ENDPOINTS_ENDPOINT in response.request.url\n\u001b[1;32m    301\u001b[0m ):\n\u001b[1;32m    302\u001b[0m     # Not enough permission to list Inference Endpoints from this org. We don't raise a custom error for this.\n\u001b[0;32m--> 303\u001b[0m     # This prevent from raising a misleading `RepositoryNotFoundError` (see below).\n\u001b[1;32m    304\u001b[0m     pass\n\u001b[1;32m    306\u001b[0m elif error_code == \"RepoNotFound\" or response.status_code == 401:\n\u001b[1;32m    307\u001b[0m     # 401 is misleading as it is returned for:\n\u001b[1;32m    308\u001b[0m     #    - private and gated repos if user is not authenticated\n\u001b[1;32m    309\u001b[0m     #    - missing repos\n\u001b[1;32m    310\u001b[0m     # => for now, we process them as `RepoNotFound` anyway.\n\u001b[1;32m    311\u001b[0m     # See https://gist.github.com/Wauplin/46c27ad266b15998ce56a6603796f0b9\n","\u001b[0;31mHfHubHTTPError\u001b[0m: 429 Client Error: Too Many Requests for url: https://huggingface.co/api/models/Jaykumaran17/Zephyr7b-Beta-2000D-chat-hf-phr_mental_therapy/commit/main (Request ID: Root=1-6564afb8-4ac923b653bef9531e7c984e;b1c63c7a-495e-430c-b070-5a4393bbcf20)\n\nYou have exceeded our hourly quotas for action: commit. We invite you to retry later."],"ename":"HfHubHTTPError","evalue":"429 Client Error: Too Many Requests for url: https://huggingface.co/api/models/Jaykumaran17/Zephyr7b-Beta-2000D-chat-hf-phr_mental_therapy/commit/main (Request ID: Root=1-6564afb8-4ac923b653bef9531e7c984e;b1c63c7a-495e-430c-b070-5a4393bbcf20)\n\nYou have exceeded our hourly quotas for action: commit. We invite you to retry later.","output_type":"error"}]},{"cell_type":"code","source":"logging.set_verbosity(logging.CRITICAL)\nSYSTEM_PROMPT = \"You are a helpful and joyous mental therapy assistant. Always answer as helpfully and cheerfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.If you don't know the answer to a question, please don't share false information. Always try to be as cheerfull as possible\"\n# Run text generation pipeline with our next model\nprompt = \"I am feeling suicidal.I have lost a lot of money in gambling. The money I used was taken as a loan\"\npipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=1000,temperature=0.9)\nresult = pipe(f\"<s>[INST]<<SYS>>{SYSTEM_PROMPT}<</SYS>> {prompt} [/INST]\")\nprint(result[0]['generated_text'])","metadata":{"execution":{"iopub.status.busy":"2023-11-27T15:03:20.391664Z","iopub.status.idle":"2023-11-27T15:03:20.392013Z","shell.execute_reply.started":"2023-11-27T15:03:20.391837Z","shell.execute_reply":"2023-11-27T15:03:20.391860Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}