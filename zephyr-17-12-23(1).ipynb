{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()\n\n#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","metadata":{"execution":{"iopub.status.busy":"2023-12-17T09:50:28.253442Z","iopub.execute_input":"2023-12-17T09:50:28.253983Z","iopub.status.idle":"2023-12-17T09:50:28.678272Z","shell.execute_reply.started":"2023-12-17T09:50:28.253956Z","shell.execute_reply":"2023-12-17T09:50:28.677425Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2faff74f1144e748b40c540e76f3820"}},"metadata":{}}]},{"cell_type":"code","source":"# authenticate WandB for logging metrics\nimport wandb\nwandb.login()\n# ee311bc313381f6bee4c5412a630fdad1175f3c0\n","metadata":{"execution":{"iopub.status.busy":"2023-12-17T09:27:10.695832Z","iopub.execute_input":"2023-12-17T09:27:10.696213Z","iopub.status.idle":"2023-12-17T09:27:18.016841Z","shell.execute_reply.started":"2023-12-17T09:27:10.696183Z","shell.execute_reply":"2023-12-17T09:27:18.015975Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"!pip install trl transformers accelerate peft==0.6.2 -Uqqq\n!pip install datasets bitsandbytes einops wandb -Uqqq","metadata":{"execution":{"iopub.status.busy":"2023-12-17T09:50:36.455764Z","iopub.execute_input":"2023-12-17T09:50:36.456558Z","iopub.status.idle":"2023-12-17T09:51:17.970597Z","shell.execute_reply.started":"2023-12-17T09:50:36.456528Z","shell.execute_reply":"2023-12-17T09:51:17.969668Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\ncuml 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\ndask-cuda 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\ndask-cudf 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\ngcsfs 2023.6.0 requires fsspec==2023.6.0, but you have fsspec 2023.10.0 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\nraft-dask 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\ns3fs 2023.12.2 requires fsspec==2023.12.2, but you have fsspec 2023.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"!pip install flash-attn --no-build-isolation","metadata":{"execution":{"iopub.status.busy":"2023-12-17T09:51:17.972492Z","iopub.execute_input":"2023-12-17T09:51:17.972813Z","iopub.status.idle":"2023-12-17T09:51:42.533169Z","shell.execute_reply.started":"2023-12-17T09:51:17.972785Z","shell.execute_reply":"2023-12-17T09:51:42.532088Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting flash-attn\n  Downloading flash_attn-2.3.6.tar.gz (2.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from flash-attn) (2.0.0)\nRequirement already satisfied: einops in /opt/conda/lib/python3.10/site-packages (from flash-attn) (0.7.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from flash-attn) (21.3)\nRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from flash-attn) (1.11.1.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->flash-attn) (3.0.9)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (4.5.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->flash-attn) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->flash-attn) (1.3.0)\nBuilding wheels for collected packages: flash-attn\n  Building wheel for flash-attn (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for flash-attn: filename=flash_attn-2.3.6-cp310-cp310-linux_x86_64.whl size=56561532 sha256=0552c92bb12d549f9319792444903662f3a95d3276dc0eea99f52479dad1c901\n  Stored in directory: /root/.cache/pip/wheels/24/5f/16/5044cdddb6dfb3331dfbffa28ab6096ec2900777af5cb0253a\nSuccessfully built flash-attn\nInstalling collected packages: flash-attn\nSuccessfully installed flash-attn-2.3.6\n","output_type":"stream"}]},{"cell_type":"code","source":"from flash_attn import flash_attn_qkvpacked_func, flash_attn_func\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2023-12-17T09:51:42.534477Z","iopub.execute_input":"2023-12-17T09:51:42.534795Z","iopub.status.idle":"2023-12-17T09:51:44.366974Z","shell.execute_reply.started":"2023-12-17T09:51:42.534768Z","shell.execute_reply":"2023-12-17T09:51:44.366155Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import os\n\nimport torch\nfrom datasets import load_dataset,Dataset\nfrom peft import LoraConfig, get_peft_model, PeftConfig, PeftModel, prepare_model_for_kbit_training\nfrom transformers import (AutoModelForCausalLM, AutoTokenizer,\n                          BitsAndBytesConfig, HfArgumentParser,\n                          TrainingArguments,GenerationConfig, pipeline)\nfrom trl import SFTTrainer\n# from transformers import logging","metadata":{"execution":{"iopub.status.busy":"2023-12-17T09:51:44.368966Z","iopub.execute_input":"2023-12-17T09:51:44.369491Z","iopub.status.idle":"2023-12-17T09:51:59.194640Z","shell.execute_reply.started":"2023-12-17T09:51:44.369460Z","shell.execute_reply":"2023-12-17T09:51:59.193672Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\n\n# Set a random seed for reproducibility\nseed = 42\ntorch.manual_seed(seed)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-17T09:51:59.195726Z","iopub.execute_input":"2023-12-17T09:51:59.195977Z","iopub.status.idle":"2023-12-17T09:51:59.204533Z","shell.execute_reply.started":"2023-12-17T09:51:59.195955Z","shell.execute_reply":"2023-12-17T09:51:59.203712Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x7c704d712d90>"},"metadata":{}}]},{"cell_type":"code","source":"model_name = \"HuggingFaceH4/zephyr-7b-beta\" \ndataset_name = \"jerryjalapeno/nart-100k-synthetic\"\nnew_model = \"Zephyr-Try2-17-12\"\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,            # load model in 4-bit precision\n    bnb_4bit_quant_type=\"nf4\",    # pre-trained model should be quantized in 4-bit NF format\n    bnb_4bit_use_double_quant=True, # Using double quantization as mentioned in QLoRA paper\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    # During computation, pre-trained model should be loaded in BF16 format\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config = bnb_config,\n    device_map = 0,\n    use_cache=True,\n    trust_remote_code=True\n)\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\ntokenizer.pad_token = tokenizer.eos_token\n\ntokenizer.padding_side = \"right\"","metadata":{"execution":{"iopub.status.busy":"2023-12-17T09:51:59.205762Z","iopub.execute_input":"2023-12-17T09:51:59.206051Z","iopub.status.idle":"2023-12-17T09:53:58.069809Z","shell.execute_reply.started":"2023-12-17T09:51:59.206028Z","shell.execute_reply":"2023-12-17T09:53:58.068996Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/638 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c7ec521a9f24f61aa9e1273ec93ee53"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7389b8b7126a4480ad4435707cd4fc9b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75b35cd861e9446ab8d87637f75c84e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00008.safetensors:   0%|          | 0.00/1.89G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3447e0be08fa4cbf9fb4460ea7af704a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00008.safetensors:   0%|          | 0.00/1.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20119c07c8cb41b7894cdb1b0e04c4ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00008.safetensors:   0%|          | 0.00/1.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1de2cfd15f754b5999843f444a8843f2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00008.safetensors:   0%|          | 0.00/1.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41defb77e0f54bd08385e6ba31a6f38c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00005-of-00008.safetensors:   0%|          | 0.00/1.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec10cadeee7d492c9f39e217ad200f01"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00006-of-00008.safetensors:   0%|          | 0.00/1.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9613fa4a118a49dab9d67a395f72938f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00007-of-00008.safetensors:   0%|          | 0.00/1.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5735dbf6324d474e9b813316421aa0fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00008-of-00008.safetensors:   0%|          | 0.00/816M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83de60bf95c34a19939dd96646383a27"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33fa069219a64447af04de1c17bc9b1a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3abe49ddb197432584c9924a0555bbcc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.43k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5d803745b4841c6a95d613daebf9717"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2cacb02e1ea449d942b8b53d7e73bef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03cc0db64b674635badc8ae3050e3e05"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"044d0a3170d0478b9ec522db1b2af4c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/168 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0113c7c517654ab4b60c8833f53b0944"}},"metadata":{}}]},{"cell_type":"code","source":"\ntokenizer.unk_token = \"<unk>\"\ntokenizer.pad_token = tokenizer.unk_token\ntokenizer.clean_up_tokenization_spaces = False\ntokenizer.add_bos_token = True\ntokenizer.add_eos_token=True\ntokenizer.truncation_side: \"left\"\ntokenizer.pad_token = \"</s>\"\ntokenizer.bos_token= \"<s>\"\ntokenizer.eos_token = \"</s>\"\n# tokenizer.use_default_system_prompt=True,\ntokenizer.chat_template =\"{% for message in messages %}\\n{% if message['role'] == 'user' %}\\n{{ '<|user|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'system' %}\\n{{ '<|system|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '<|assistant|>\\n'  + message['content'] + eos_token }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\n{{ '<|assistant|>' }}\\n{% endif %}\\n{% endfor %}\"\n\n","metadata":{"execution":{"iopub.status.busy":"2023-12-17T09:53:58.070952Z","iopub.execute_input":"2023-12-17T09:53:58.071238Z","iopub.status.idle":"2023-12-17T09:53:58.076895Z","shell.execute_reply.started":"2023-12-17T09:53:58.071213Z","shell.execute_reply":"2023-12-17T09:53:58.076002Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# tokenizer.chat_template = \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST] ' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token + ' ' }}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\"\n# tokenizer.pad_token = tokenizer.unk_token\n# tokenizer.clean_up_tokenization_spaces = True\n# tokenizer.add_bos_token = False\n# tokenizer.padding_side = \"right\"\n# tokenizer.pad_token","metadata":{"execution":{"iopub.status.busy":"2023-12-17T09:53:58.078012Z","iopub.execute_input":"2023-12-17T09:53:58.078275Z","iopub.status.idle":"2023-12-17T09:53:58.088983Z","shell.execute_reply.started":"2023-12-17T09:53:58.078253Z","shell.execute_reply":"2023-12-17T09:53:58.088166Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"tokenizer.chat_template","metadata":{"execution":{"iopub.status.busy":"2023-12-17T09:53:58.090044Z","iopub.execute_input":"2023-12-17T09:53:58.090335Z","iopub.status.idle":"2023-12-17T09:53:58.099280Z","shell.execute_reply.started":"2023-12-17T09:53:58.090303Z","shell.execute_reply":"2023-12-17T09:53:58.098346Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"\"{% for message in messages %}\\n{% if message['role'] == 'user' %}\\n{{ '<|user|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'system' %}\\n{{ '<|system|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '<|assistant|>\\n'  + message['content'] + eos_token }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\n{{ '<|assistant|>' }}\\n{% endif %}\\n{% endfor %}\""},"metadata":{}}]},{"cell_type":"code","source":"dataset_name = \"jerryjalapeno/nart-100k-synthetic\"\nprint(f\"\\nLoading {dataset_name} dataset...\")\ndataset = load_dataset(dataset_name, split=\"train\", streaming=True)\n\ndataset = dataset.take(1000)\ntexts = []\n\ndataset_name = \"jerryjalapeno/nart-100k-synthetic\"\nprint(f\"\\nLoading {dataset_name} dataset...\")\ndataset = load_dataset(dataset_name, split=\"train\", streaming=True)\n\ndataset = dataset.take(1000)\ntexts = []\n\nfor row in dataset:\n    messages_keep = []\n    for message in row[\"conversations\"]:\n        if message[\"from\"] == \"human\":\n            message[\"value\"] = message[\"value\"].replace(\"Alex\", \"\")\n            message[\"value\"] = message[\"value\"].replace(\"Charlie\", \"\")\n            message[\"value\"] = message[\"value\"].replace(\" ,\", \",\")\n            message[\"value\"] = message[\"value\"].replace(\", .\", \". \")\n            messages_keep.append({\"role\": \"user\", \"content\": message[\"value\"]})\n        if message[\"from\"] == \"gpt\":\n            message[\"value\"] = message[\"value\"].replace(\"Alex\", \"\")\n            message[\"value\"] = message[\"value\"].replace(\"Charlie\", \"\")\n            message[\"value\"] = message[\"value\"].replace(\" ,\", \",\")\n            message[\"value\"] = message[\"value\"].replace(\", .\", \". \")\n            messages_keep.append({\"role\": \"assistant\", \"content\": message[\"value\"]})\n\n    text = tokenizer.apply_chat_template(\n        messages_keep, tokenize=False, add_generation_prompt=False\n    )\n    texts.append(text)\n\npandas_dataset = pd.DataFrame([texts]).T\npandas_dataset.columns = [\"text\"]\npandas_dataset\n\n# formatting_func(dataset)\n# texts","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-12-17T09:53:58.104031Z","iopub.execute_input":"2023-12-17T09:53:58.104346Z","iopub.status.idle":"2023-12-17T09:54:09.524785Z","shell.execute_reply.started":"2023-12-17T09:53:58.104321Z","shell.execute_reply":"2023-12-17T09:54:09.523638Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"\nLoading jerryjalapeno/nart-100k-synthetic dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/285 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ad3ac4c75214801805828795e3027f1"}},"metadata":{}},{"name":"stdout","text":"\nLoading jerryjalapeno/nart-100k-synthetic dataset...\n","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"                                                  text\n0    <|user|>\\nI've been feeling so sad and overwhe...\n1    <|user|>\\nHi, I'm feeling really scared about ...\n2    <|user|>\\nHey, I hope you're doing well. I wan...\n3    <|user|>\\nI'm feeling really overwhelmed, and ...\n4    <|user|>\\nI'm feeling really upset and confuse...\n..                                                 ...\n995  <|user|>\\nI'm just so disgusted with myself. I...\n996  <|user|>\\nI've been feeling really stressed la...\n997  <|user|>\\nI've been feeling frustrated lately....\n998  <|user|>\\nHi, I'm feeling really angry right n...\n999  <|user|>\\nI'm exhausted. I feel like I've been...\n\n[1000 rows x 1 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>&lt;|user|&gt;\\nI've been feeling so sad and overwhe...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>&lt;|user|&gt;\\nHi, I'm feeling really scared about ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>&lt;|user|&gt;\\nHey, I hope you're doing well. I wan...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>&lt;|user|&gt;\\nI'm feeling really overwhelmed, and ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>&lt;|user|&gt;\\nI'm feeling really upset and confuse...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>995</th>\n      <td>&lt;|user|&gt;\\nI'm just so disgusted with myself. I...</td>\n    </tr>\n    <tr>\n      <th>996</th>\n      <td>&lt;|user|&gt;\\nI've been feeling really stressed la...</td>\n    </tr>\n    <tr>\n      <th>997</th>\n      <td>&lt;|user|&gt;\\nI've been feeling frustrated lately....</td>\n    </tr>\n    <tr>\n      <th>998</th>\n      <td>&lt;|user|&gt;\\nHi, I'm feeling really angry right n...</td>\n    </tr>\n    <tr>\n      <th>999</th>\n      <td>&lt;|user|&gt;\\nI'm exhausted. I feel like I've been...</td>\n    </tr>\n  </tbody>\n</table>\n<p>1000 rows × 1 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# dataset = dataset.map(formatting_func)\n# # dataset","metadata":{"execution":{"iopub.status.busy":"2023-12-17T09:54:09.526219Z","iopub.execute_input":"2023-12-17T09:54:09.527006Z","iopub.status.idle":"2023-12-17T09:54:09.532511Z","shell.execute_reply.started":"2023-12-17T09:54:09.526958Z","shell.execute_reply":"2023-12-17T09:54:09.531645Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"pandas_train_dataset=pandas_dataset\npandas_train_dataset","metadata":{"execution":{"iopub.status.busy":"2023-12-17T09:54:09.533783Z","iopub.execute_input":"2023-12-17T09:54:09.534214Z","iopub.status.idle":"2023-12-17T09:54:09.547178Z","shell.execute_reply.started":"2023-12-17T09:54:09.534169Z","shell.execute_reply":"2023-12-17T09:54:09.546248Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"                                                  text\n0    <|user|>\\nI've been feeling so sad and overwhe...\n1    <|user|>\\nHi, I'm feeling really scared about ...\n2    <|user|>\\nHey, I hope you're doing well. I wan...\n3    <|user|>\\nI'm feeling really overwhelmed, and ...\n4    <|user|>\\nI'm feeling really upset and confuse...\n..                                                 ...\n995  <|user|>\\nI'm just so disgusted with myself. I...\n996  <|user|>\\nI've been feeling really stressed la...\n997  <|user|>\\nI've been feeling frustrated lately....\n998  <|user|>\\nHi, I'm feeling really angry right n...\n999  <|user|>\\nI'm exhausted. I feel like I've been...\n\n[1000 rows x 1 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>&lt;|user|&gt;\\nI've been feeling so sad and overwhe...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>&lt;|user|&gt;\\nHi, I'm feeling really scared about ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>&lt;|user|&gt;\\nHey, I hope you're doing well. I wan...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>&lt;|user|&gt;\\nI'm feeling really overwhelmed, and ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>&lt;|user|&gt;\\nI'm feeling really upset and confuse...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>995</th>\n      <td>&lt;|user|&gt;\\nI'm just so disgusted with myself. I...</td>\n    </tr>\n    <tr>\n      <th>996</th>\n      <td>&lt;|user|&gt;\\nI've been feeling really stressed la...</td>\n    </tr>\n    <tr>\n      <th>997</th>\n      <td>&lt;|user|&gt;\\nI've been feeling frustrated lately....</td>\n    </tr>\n    <tr>\n      <th>998</th>\n      <td>&lt;|user|&gt;\\nHi, I'm feeling really angry right n...</td>\n    </tr>\n    <tr>\n      <th>999</th>\n      <td>&lt;|user|&gt;\\nI'm exhausted. I feel like I've been...</td>\n    </tr>\n  </tbody>\n</table>\n<p>1000 rows × 1 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train_dataset = Dataset.from_pandas(pandas_train_dataset)\n\n# remove old text cols\ntrain_dataset = train_dataset.remove_columns(\n    [col for col in train_dataset.column_names if col not in [\"text\"]]\n)\ntrain_dataset\n\n\nprint(\"Final train dataset:\")\nprint(train_dataset)\nprint(train_dataset[0])\nprint(train_dataset[-1])","metadata":{"execution":{"iopub.status.busy":"2023-12-17T09:54:09.548278Z","iopub.execute_input":"2023-12-17T09:54:09.548546Z","iopub.status.idle":"2023-12-17T09:54:09.568988Z","shell.execute_reply.started":"2023-12-17T09:54:09.548522Z","shell.execute_reply":"2023-12-17T09:54:09.568142Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Final train dataset:\nDataset({\n    features: ['text'],\n    num_rows: 1000\n})\n{'text': \"<|user|>\\nI've been feeling so sad and overwhelmed lately. Work has become such a massive source of stress for me.</s>\\n<|assistant|>\\nHey there, I'm here to listen and support you. It sounds like work has been really challenging lately. Can you tell me more about what's been going on?</s>\\n<|user|>\\nI recently got a promotion at work, which I thought would be exciting. But the added responsibilities and pressure have just taken a toll on my mental health. It's been a really moving experience for me.</s>\\n<|assistant|>\\nI can understand how it can be overwhelming when we're faced with higher expectations. It's okay to acknowledge your emotions and allow yourself to feel sad in this situation. It's an important part of the healing process. What specific challenges have you been facing at work?</s>\\n<|user|>\\nWell, the workload has increased significantly, and I find it hard to maintain a work-life balance. I've been staying late at the office, and it feels like I'm constantly under a pile of never-ending tasks. It's just so hard to keep up, and it's impacting my overall well-being.</s>\\n<|assistant|>\\nIt sounds like you're dealing with a lot of pressure to perform and succeed. Remember, it's crucial to take care of yourself, both mentally and physically. A healthy work-life balance is essential. Have you tried any strategies to cope with the added workload?</s>\\n<|user|>\\nI've been trying to prioritize my tasks and delegate whenever possible. I've also started practicing meditation during my breaks to help manage stress. But sometimes, it feels like no matter what I do, I can't catch a break. It's been a constant struggle.</s>\\n<|assistant|>\\nIt's great to hear that you're already implementing some helpful strategies. Remember, progress takes time, and it's okay to have setbacks. In addition to what you're already doing, I encourage you to also communicate with your supervisor or team about your workload and discuss possible solutions together.</s>\\n<|user|>\\nYou're right. I haven't really opened up about my struggles to my coworkers or supervisor. I guess I've been afraid of appearing weak or incapable. How can I approach this discussion without feeling vulnerable?</s>\\n<|assistant|>\\nIt's completely normal to feel that way, but remember, asking for support is a strength, not a weakness. Start by scheduling a conversation with your supervisor or a trusted colleague in a private and comfortable setting. Be honest about your challenges and express your willingness to find solutions together. Remember, you're not alone in this.</s>\\n<|user|>\\nThank you for your understanding and guidance. I appreciate the reminder that I don't have to face this alone. I'll gather my courage and initiate that conversation soon. I need to prioritize my well-being and find a healthier balance.</s>\\n<|assistant|>\\nYou're very welcome! I'm here to support you every step of the way. Taking care of yourself should always be a priority. Remember to be kind to yourself and celebrate your progress, no matter how small it may seem. You've got this!</s>\\n\"}\n{'text': \"<|user|>\\nI'm exhausted. I feel like I've been carrying the weight of the world on my shoulders. It's overwhelming.</s>\\n<|assistant|>\\nHello . I'm here to listen and support you. It sounds like you've been through a lot. Can you tell me more about why you're feeling exhausted?</s>\\n<|user|>\\nThere's just so much going on in my life. I never seem to catch a break. And on top of everything, I've been dealing with a history of abuse that still haunts me.</s>\\n<|assistant|>\\nI'm really sorry to hear that you've experienced abuse.  It must have been incredibly difficult for you. How has it been impacting your life recently?</s>\\n<|user|>\\nIt's like a heavy cloud that follows me no matter where I go. It affects my relationships, my work, and even my sleep. I can't seem to escape the memories and the fear.</s>\\n<|assistant|>\\nIt sounds like the abuse has had a profound impact on your overall well-being. You mentioned feeling overwhelmed earlier. Can you expand on that a little?</s>\\n<|user|>\\nMy anxiety always gets triggered whenever I find myself in situations that remind me of my past trauma. It's a constant struggle to keep my fears under control. I feel like there's no escape from it all.</s>\\n<|assistant|>\\nAnxiety can be incredibly challenging to deal with, especially when it's connected to past trauma. Are there any specific situations that tend to trigger your anxiety?</s>\\n<|user|>\\nIt could be as simple as being in a crowded space, or someone raising their voice. Even certain smells or sounds can transport me back to those painful memories. It's like my body automatically goes into fight or flight mode.</s>\\n<|assistant|>\\nIt sounds like your anxiety response is quite sensitive.  Have you noticed any patterns or themes in terms of what triggers your anxiety the most?</s>\\n<|user|>\\nI've noticed that whenever I feel a loss of control or when I'm in situations where I feel vulnerable, my anxiety tends to skyrocket. It's like I'm always on edge, waiting for something bad to happen.</s>\\n<|assistant|>\\nThat hyper-vigilance and constant anticipation of danger can be mentally and emotionally exhausting. Have you found any coping mechanisms or strategies that help you manage your anxiety?</s>\\n<|user|>\\nI've tried deep breathing exercises and keeping a journal to express my feelings. Sometimes listening to calming music or practicing mindfulness helps, but it's often a temporary relief.</s>\\n<|assistant|>\\nIt's great that you're exploring different techniques to manage your anxiety.  Sometimes finding what works for you can take time and persistence. Have you considered seeking support from a therapist who specializes in trauma?</s>\\n<|user|>\\nI've thought about it, but I've always been hesitant. I guess I'm afraid of opening up about all the painful memories and emotions. It feels easier to just keep it all to myself.</s>\\n<|assistant|>\\nIt's completely understandable to feel scared and hesitant about confronting those painful memories. The healing process can be challenging, but it's important to remember that you don't have to face it alone. A qualified therapist can provide a safe space for you to explore and process your trauma at a pace that feels comfortable for you.</s>\\n<|user|>\\nI appreciate your support and understanding.  It's difficult for me to take that first step, but I know deep down that I deserve healing and a chance at a better life.</s>\\n<|assistant|>\\nYou absolutely deserve healing and a chance at a better life.  Remember that healing is a journey, and it takes time. I'm here for you whenever you're ready to take that first step towards healing and finding peace within yourself.</s>\\n<|user|>\\nThank you.  Your words mean a lot to me. I'll think about what you've said and consider reaching out to a therapist. It's time for me to prioritize my well-being.</s>\\n<|assistant|>\\nYou're welcome.  Remember, you're not alone in this. Whenever you're ready, I'll be here to support you throughout your journey. Take care of yourself and reach out whenever you need someone to talk to.</s>\\n\"}\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenizer.chat_template","metadata":{"execution":{"iopub.status.busy":"2023-12-17T09:54:09.570496Z","iopub.execute_input":"2023-12-17T09:54:09.570798Z","iopub.status.idle":"2023-12-17T09:54:09.576048Z","shell.execute_reply.started":"2023-12-17T09:54:09.570773Z","shell.execute_reply":"2023-12-17T09:54:09.575106Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"\"{% for message in messages %}\\n{% if message['role'] == 'user' %}\\n{{ '<|user|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'system' %}\\n{{ '<|system|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '<|assistant|>\\n'  + message['content'] + eos_token }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\n{{ '<|assistant|>' }}\\n{% endif %}\\n{% endfor %}\""},"metadata":{}}]},{"cell_type":"code","source":"print(tokenizer.decode(tokenizer.encode(train_dataset[0][\"text\"])))","metadata":{"execution":{"iopub.status.busy":"2023-12-17T09:54:09.577390Z","iopub.execute_input":"2023-12-17T09:54:09.577797Z","iopub.status.idle":"2023-12-17T09:54:09.599253Z","shell.execute_reply.started":"2023-12-17T09:54:09.577752Z","shell.execute_reply":"2023-12-17T09:54:09.598465Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"<s> <|user|>\nI've been feeling so sad and overwhelmed lately. Work has become such a massive source of stress for me.</s> \n<|assistant|>\nHey there, I'm here to listen and support you. It sounds like work has been really challenging lately. Can you tell me more about what's been going on?</s> \n<|user|>\nI recently got a promotion at work, which I thought would be exciting. But the added responsibilities and pressure have just taken a toll on my mental health. It's been a really moving experience for me.</s> \n<|assistant|>\nI can understand how it can be overwhelming when we're faced with higher expectations. It's okay to acknowledge your emotions and allow yourself to feel sad in this situation. It's an important part of the healing process. What specific challenges have you been facing at work?</s> \n<|user|>\nWell, the workload has increased significantly, and I find it hard to maintain a work-life balance. I've been staying late at the office, and it feels like I'm constantly under a pile of never-ending tasks. It's just so hard to keep up, and it's impacting my overall well-being.</s> \n<|assistant|>\nIt sounds like you're dealing with a lot of pressure to perform and succeed. Remember, it's crucial to take care of yourself, both mentally and physically. A healthy work-life balance is essential. Have you tried any strategies to cope with the added workload?</s> \n<|user|>\nI've been trying to prioritize my tasks and delegate whenever possible. I've also started practicing meditation during my breaks to help manage stress. But sometimes, it feels like no matter what I do, I can't catch a break. It's been a constant struggle.</s> \n<|assistant|>\nIt's great to hear that you're already implementing some helpful strategies. Remember, progress takes time, and it's okay to have setbacks. In addition to what you're already doing, I encourage you to also communicate with your supervisor or team about your workload and discuss possible solutions together.</s> \n<|user|>\nYou're right. I haven't really opened up about my struggles to my coworkers or supervisor. I guess I've been afraid of appearing weak or incapable. How can I approach this discussion without feeling vulnerable?</s> \n<|assistant|>\nIt's completely normal to feel that way, but remember, asking for support is a strength, not a weakness. Start by scheduling a conversation with your supervisor or a trusted colleague in a private and comfortable setting. Be honest about your challenges and express your willingness to find solutions together. Remember, you're not alone in this.</s> \n<|user|>\nThank you for your understanding and guidance. I appreciate the reminder that I don't have to face this alone. I'll gather my courage and initiate that conversation soon. I need to prioritize my well-being and find a healthier balance.</s> \n<|assistant|>\nYou're very welcome! I'm here to support you every step of the way. Taking care of yourself should always be a priority. Remember to be kind to yourself and celebrate your progress, no matter how small it may seem. You've got this!</s> \n</s>\n","output_type":"stream"}]},{"cell_type":"code","source":"encoded_train_dataset = train_dataset.map(\n    lambda examples: tokenizer(examples[\"text\"]), batched=True\n)\nencoded_train_dataset","metadata":{"execution":{"iopub.status.busy":"2023-12-17T09:54:09.600241Z","iopub.execute_input":"2023-12-17T09:54:09.600559Z","iopub.status.idle":"2023-12-17T09:54:11.106492Z","shell.execute_reply.started":"2023-12-17T09:54:09.600527Z","shell.execute_reply":"2023-12-17T09:54:11.105534Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"102fa2f7ddd34261b20988ce6e652fa0"}},"metadata":{}},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['text', 'input_ids', 'attention_mask'],\n    num_rows: 1000\n})"},"metadata":{}}]},{"cell_type":"code","source":"import tqdm\nimport numpy as np\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-12-17T09:54:11.107870Z","iopub.execute_input":"2023-12-17T09:54:11.108586Z","iopub.status.idle":"2023-12-17T09:54:11.112868Z","shell.execute_reply.started":"2023-12-17T09:54:11.108548Z","shell.execute_reply":"2023-12-17T09:54:11.111918Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# num_dropped = 0\n# rows_to_drop = []\n# max_num_tokens_taken = []\n# for i in tqdm.tqdm(range(len(pandas_train_dataset))):\n#     row = encoded_train_dataset[i]\n#     num_tokens = len(row[\"input_ids\"])\n#     if num_tokens > 500:\n#         rows_to_drop.append(i)\n#         num_dropped += 1\n#     else:\n#         max_num_tokens_taken.append(num_tokens)\n\n# num_dropped","metadata":{"execution":{"iopub.status.busy":"2023-12-17T09:54:11.114050Z","iopub.execute_input":"2023-12-17T09:54:11.114343Z","iopub.status.idle":"2023-12-17T09:54:11.123038Z","shell.execute_reply.started":"2023-12-17T09:54:11.114318Z","shell.execute_reply":"2023-12-17T09:54:11.122128Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# np.max(max_num_tokens_taken)\n# pandas_train_dataset = pandas_train_dataset.drop(rows_to_drop).reset_index(drop=True)\npandas_train_dataset","metadata":{"execution":{"iopub.status.busy":"2023-12-17T09:54:11.124420Z","iopub.execute_input":"2023-12-17T09:54:11.124874Z","iopub.status.idle":"2023-12-17T09:54:11.139172Z","shell.execute_reply.started":"2023-12-17T09:54:11.124839Z","shell.execute_reply":"2023-12-17T09:54:11.138141Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"                                                  text\n0    <|user|>\\nI've been feeling so sad and overwhe...\n1    <|user|>\\nHi, I'm feeling really scared about ...\n2    <|user|>\\nHey, I hope you're doing well. I wan...\n3    <|user|>\\nI'm feeling really overwhelmed, and ...\n4    <|user|>\\nI'm feeling really upset and confuse...\n..                                                 ...\n995  <|user|>\\nI'm just so disgusted with myself. I...\n996  <|user|>\\nI've been feeling really stressed la...\n997  <|user|>\\nI've been feeling frustrated lately....\n998  <|user|>\\nHi, I'm feeling really angry right n...\n999  <|user|>\\nI'm exhausted. I feel like I've been...\n\n[1000 rows x 1 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>&lt;|user|&gt;\\nI've been feeling so sad and overwhe...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>&lt;|user|&gt;\\nHi, I'm feeling really scared about ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>&lt;|user|&gt;\\nHey, I hope you're doing well. I wan...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>&lt;|user|&gt;\\nI'm feeling really overwhelmed, and ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>&lt;|user|&gt;\\nI'm feeling really upset and confuse...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>995</th>\n      <td>&lt;|user|&gt;\\nI'm just so disgusted with myself. I...</td>\n    </tr>\n    <tr>\n      <th>996</th>\n      <td>&lt;|user|&gt;\\nI've been feeling really stressed la...</td>\n    </tr>\n    <tr>\n      <th>997</th>\n      <td>&lt;|user|&gt;\\nI've been feeling frustrated lately....</td>\n    </tr>\n    <tr>\n      <th>998</th>\n      <td>&lt;|user|&gt;\\nHi, I'm feeling really angry right n...</td>\n    </tr>\n    <tr>\n      <th>999</th>\n      <td>&lt;|user|&gt;\\nI'm exhausted. I feel like I've been...</td>\n    </tr>\n  </tbody>\n</table>\n<p>1000 rows × 1 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train_dataset = Dataset.from_pandas(pandas_train_dataset)\n\nprint(\"Train dataset:\")\ntrain_dataset = train_dataset.shuffle(seed=seed)\nprint(train_dataset)\nprint(train_dataset[0])\nprint(train_dataset[-1])","metadata":{"execution":{"iopub.status.busy":"2023-12-17T09:54:11.140284Z","iopub.execute_input":"2023-12-17T09:54:11.140553Z","iopub.status.idle":"2023-12-17T09:54:11.157937Z","shell.execute_reply.started":"2023-12-17T09:54:11.140528Z","shell.execute_reply":"2023-12-17T09:54:11.157103Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Train dataset:\nDataset({\n    features: ['text'],\n    num_rows: 1000\n})\n{'text': \"<|user|>\\nI'm feeling really upset... and sorry. I just can't seem to shake this feeling of constant sadness. It's like a dark cloud hanging over me, suffocating any glimmer of joy. I don't know what to do anymore.</s>\\n<|assistant|>\\n, I'm here to listen and support you. It sounds like you're experiencing deep sadness and it's weighing you down. Can you tell me more about what's been going on lately?</s>\\n<|user|>\\nWell, it feels like this depression has taken over my life. I wake up each day feeling tired, unmotivated, and unable to find any enjoyment in things I used to love. It's become hard to even get out of bed in the morning. I've lost interest in hobbies, friendships seem distant, and work feels like a burden.</s>\\n<|assistant|>\\nIt sounds like depression has made even the simplest tasks seem overwhelming for you.  With this cloud of sadness surrounding you, I can understand why everything feels heavy. Have you noticed any specific triggers or events that might have contributed to these feelings?</s>\\n<|user|>\\nI've been trying to reflect on that, but it feels like this darkness just crept up on me. I can't pinpoint a specific event or trigger that triggered it, which makes it harder to understand and cope with. It's like a constant state of hopelessness and despair that I can't seem to escape from.</s>\\n<|assistant|>\\nIt can be frustrating when the cause of depression isn't clear.  Sometimes it can develop gradually over time, making it harder to identify a single triggering event. It's important to remember that depression is a multifaceted condition and can arise from a combination of factors, such as genetics, life circumstances, and brain chemistry.</s>\\n<|user|>\\nI've heard that before, but it doesn't make it any less difficult to deal with. I just wish I could feel like myself again, find joy in the little things, and have the energy to pursue my passions. It feels like I'm stuck in this never-ending cycle of darkness, and I'm not sure how to break free.</s>\\n<|assistant|>\\nI can sense your longing for relief,, and I want you to know that there is hope. While navigating through depression can be challenging, there are strategies and support available to help you start your journey towards healing. Have you considered seeking professional help from a therapist or counselor who specializes in depression?</s>\\n<|user|>\\nI've thought about it, but part of me feels hesitant. I worry that even therapy won't be enough to lift this heavy weight off my shoulders. What if it doesn't work? What if I'm just stuck like this forever?</s>\\n<|assistant|>\\nIt's completely understandable to feel hesitant.  Depression can create doubts and uncertainties about the effectiveness of treatment. However, therapy has been shown to be effective in helping individuals cope with and overcome depression. It's important to remember that therapy is a collaborative process, and we will work together to find the best approach for your unique situation.</s>\\n<|user|>\\nI guess it's worth a try. I really want to get better, but it's hard to believe that things can change when I've felt this way for so long. How long does therapy usually take to help with depression?</s>\\n<|assistant|>\\nThe duration of therapy can vary for each individual.  It depends on various factors such as the severity of depression, your commitment to the process, and the effectiveness of different therapeutic techniques. Therapy is a journey, and it's important to be patient and compassionate with yourself as you navigate through it. We will work at a pace that feels comfortable for you, and together we can strive for improvement and lasting change.</s>\\n<|user|>\\nI appreciate your reassurance.  It's comforting to know that I won't be alone in this journey. I've been isolating myself from friends and family, feeling like a burden. Will therapy help me rebuild my support system and feel connected again?</s>\\n<|assistant|>\\nAbsolutely.  Rebuilding your support system and restoring your sense of connection is a vital part of the therapeutic process. Therapy can provide you with tools and strategies to communicate your emotions effectively, set healthy boundaries, and cultivate meaningful relationships. Together, we can work on finding ways to reconnect with your loved ones and rebuild a strong support network.</s>\\n<|user|>\\nThat sounds hopeful. I've forgotten what it's like to have a strong support network around me. I closed myself off for so long, it's going to be a challenge to open up and let people back in.</s>\\n<|assistant|>\\nIt's completely understandable.  Opening up and trusting others after experiencing depression can be daunting. We will take it one step at a time, at a pace that feels comfortable for you. Remember, healing takes time, and it's important to be gentle with yourself throughout this process. Is there anything specific you would like to explore in therapy?</s>\\n<|user|>\\nI think one of the things I'd like to work on is changing my negative thought patterns. My mind is constantly filled with self-critical thoughts, and I want to learn how to reframe them and cultivate more self-compassion.</s>\\n<|assistant|>\\nThat's a great goal.  Challenging negative thought patterns and developing self-compassion are essential steps towards healing from depression. In therapy, we can explore techniques such as cognitive-behavioral therapy (CBT) to identify and reframe negative thoughts and cultivate a more positive and compassionate mindset. Are there any other areas you'd like to focus on?</s>\\n<|user|>\\nI think it would be helpful to address the physical symptoms of depression as well. The lack of energy and motivation has been taking a toll on my overall well-being. I would like to explore ways to improve my physical health alongside my emotional well-being.</s>\\n<|assistant|>\\nAbsolutely.  Taking care of your physical health is an important aspect of overall well-being. We can discuss strategies such as incorporating exercise, proper nutrition, and healthy sleep habits into your routine. By nurturing your body, you can create a foundation for emotional wellness. Remember, small steps can lead to significant changes over time.</s>\\n<|user|>\\nThank you.  It's refreshing to hear that change is possible and that there is hope beyond this darkness. I appreciate your guidance and support. I'm willing to give therapy a chance and start taking those small steps towards healing.</s>\\n<|assistant|>\\nYou're very welcome.  Remember, I'm here for you every step of the way. Together, we will navigate through these storms of depression and find the light that lies within you. You've shown incredible strength and resilience by reaching out for help today. Your journey towards healing has already begun, and I'm honored to be a part of it.</s>\\n\"}\n{'text': \"<|user|>\\nI don't know where to begin.  I'm feeling so confused and overwhelmed. There's this constant internal battle between my dreams and the harsh reality of life. It's like I'm being pulled in two different directions.</s>\\n<|assistant|>\\nI can sense your frustration.  It's not easy when you feel torn between your dreams and the challenges that come with pursuing them. Can you tell me more about this inner conflict you're experiencing?</s>\\n<|user|>\\nWell, on one hand, I have this deep desire to pursue my dreams and make them come true. I can feel the passion and excitement bubbling inside me. But on the other hand, there's this nagging voice of criticism that keeps holding me back. It tells me that my dreams are foolish and unattainable.</s>\\n<|assistant|>\\nIt seems like you're facing a battle between your desires and self-doubt.  That inner critic can be quite loud and discouraging. Let's explore that further. What does this voice of criticism say to you?</s>\\n<|user|>\\nIt constantly questions my abilities and tells me that I'll never be good enough. It highlights all my flaws and past failures, making me doubt whether my dreams are worth pursuing at all. It's like a constant barrage of negativity that drains my motivation.</s>\\n<|assistant|>\\nI understand how disheartening that can be.  It's not easy to fight against such internal negativity. Have you ever thought about where this critical voice might come from? Is it something you've internalized from others?</s>\\n<|user|>\\nI think part of it comes from the fear of judgment and rejection. Growing up, I faced a lot of criticism from those around me. Their words left wounds that still ache to this day. Maybe that's why I struggle with self-doubt. It's like I expect criticism to come pouring in the moment I start pursuing my dreams.</s>\\n<|assistant|>\\nIt sounds like those past experiences have deeply impacted your beliefs about yourself.  The fear of criticism can be paralyzing, preventing you from taking steps towards your dreams. How do you think you can overcome this fear and start believing in yourself?</s>\\n<|user|>\\nIt's a tough question.  But maybe by acknowledging that the past does not define me, and that I am capable of growth and change. Building a support system of positive and encouraging people around me could also help counterbalance the critical voice. Perhaps focusing on my strengths and past achievements could boost my self-confidence as well.</s>\\n<|assistant|>\\nThose are all wonderful suggestions.  Surrounding yourself with supportive individuals who believe in your dreams can provide the inspiration and encouragement you need. Celebrating your successes, no matter how small, can help counteract the critical voice within. Remember, dreams are often built on stepping stones of courage and self-belief.</s>\\n<|user|>\\nYou're right.  I need to remind myself that I don't have to have it all figured out right away. Slowly but surely, with belief in myself and perseverance, I can work towards making my dreams come true. It's time to silence that critical voice and embrace the possibilities that lie ahead.</s>\\n<|assistant|>\\nThat's a beautiful realization.  It takes courage to challenge our inner critic and nurture a positive mindset. It won't be an easy journey, but I believe in you and your capacity to create the life you've always dreamed of. Remember, every step forward is a step closer to your dream coming true.</s>\\n<|user|>\\nThank you,, for guiding me through this confusion and criticism. Your support and understanding mean the world to me. I'm feeling more empowered to tackle the challenges that come my way and pursue my dreams wholeheartedly.</s>\\n<|assistant|>\\nIt's been an honor to be here with you.  Always remember that the journey towards your dream is just as important as the destination. I'll be here to support you every step of the way. Take care, and believe in yourself. You have the power to make your dreams come true.</s>\\n\"}\n","output_type":"stream"}]},{"cell_type":"code","source":"len(tokenizer.encode(train_dataset[100][\"text\"]))","metadata":{"execution":{"iopub.status.busy":"2023-12-17T09:54:11.159051Z","iopub.execute_input":"2023-12-17T09:54:11.159331Z","iopub.status.idle":"2023-12-17T09:54:11.166751Z","shell.execute_reply.started":"2023-12-17T09:54:11.159307Z","shell.execute_reply":"2023-12-17T09:54:11.165818Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"864"},"metadata":{}}]},{"cell_type":"code","source":"from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n\n# we set our lora config to be the same as qlora\npeft_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.1,\n    #  The modules to apply the LoRA update matrices.\n    target_modules = [\"gate_proj\", \"down_proj\", \"up_proj\"],\n    task_type=\"CAUSAL_LM\"\n)\n\n\n\npeft_model = get_peft_model(model, peft_config)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T09:54:11.168209Z","iopub.execute_input":"2023-12-17T09:54:11.168561Z","iopub.status.idle":"2023-12-17T09:54:11.600231Z","shell.execute_reply.started":"2023-12-17T09:54:11.168524Z","shell.execute_reply":"2023-12-17T09:54:11.599067Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# # ensure none over 500 tokens\n\n# # check that above worked\n# lens = []\n# encoded_train_dataset = train_dataset.map(\n#     lambda examples: tokenizer(examples[\"text\"]), batched=True\n# )\n# for row in encoded_train_dataset:\n#     lens.append(len(row[\"input_ids\"]))\n# np.max(lens)\n# # remove old text cols\n# train_dataset = train_dataset.remove_columns(\n#     [col for col in train_dataset.column_names if col not in [\"text\"]]\n# )\n","metadata":{"execution":{"iopub.status.busy":"2023-12-17T09:49:40.988644Z","iopub.status.idle":"2023-12-17T09:49:40.989115Z","shell.execute_reply.started":"2023-12-17T09:49:40.988858Z","shell.execute_reply":"2023-12-17T09:49:40.988880Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output_dir = \"./Zephyr-Try2-17-12\"\n\ntraining_args = TrainingArguments(\n        output_dir=output_dir,\n#         evaluation_strategy=\"steps\",\n        do_eval=True,\n        auto_find_batch_size=True,\n        log_level=\"debug\",\n        optim=\"paged_adamw_32bit\",\n        save_steps=10,\n        logging_steps=10,\n        learning_rate=3e-4,\n        weight_decay=0.01,\n        # basically just train for 2 epochs, you should train for longer\n#         max_steps=2000,\n        num_train_epochs=2,\n        warmup_steps=150,\n        fp16=True,\n#         tf32=True,\n        gradient_checkpointing=True,\n        max_grad_norm=0.3, #from the paper\n        lr_scheduler_type=\"cosine\",\n        push_to_hub=True,\n        evaluation_strategy=\"no\"\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T09:54:11.601565Z","iopub.execute_input":"2023-12-17T09:54:11.601892Z","iopub.status.idle":"2023-12-17T09:54:11.611468Z","shell.execute_reply.started":"2023-12-17T09:54:11.601863Z","shell.execute_reply":"2023-12-17T09:54:11.610492Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=0,\n    trust_remote_code=True\n)\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\n\n# # Load LLaMA tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True,use_fast=False,add_eos_token=True)\ntokenizer.pad_token_id = 18610","metadata":{"execution":{"iopub.status.busy":"2023-12-17T09:54:11.612573Z","iopub.execute_input":"2023-12-17T09:54:11.612912Z","iopub.status.idle":"2023-12-17T09:54:28.447168Z","shell.execute_reply.started":"2023-12-17T09:54:11.612874Z","shell.execute_reply":"2023-12-17T09:54:28.446309Z"},"trusted":true},"execution_count":25,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c15c334582042679f3c4449e78f49c5"}},"metadata":{}}]},{"cell_type":"code","source":"# %%\nimport torch\nimport pandas as pd\nimport tqdm\nimport numpy as np\nimport copy\nfrom datasets import load_dataset, Dataset\nfrom peft import LoraConfig\nfrom trl import SFTTrainer, DataCollatorForCompletionOnlyLM\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    AutoTokenizer,\n    TrainingArguments,\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T09:54:28.448527Z","iopub.execute_input":"2023-12-17T09:54:28.449014Z","iopub.status.idle":"2023-12-17T09:54:28.455576Z","shell.execute_reply.started":"2023-12-17T09:54:28.448975Z","shell.execute_reply":"2023-12-17T09:54:28.454527Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# # [markdown]\n# # Then finally pass everthing to the trainer\n\n# # %%\n# max_seq_length = 256\n\n# response_template = \"<|assistant|>\"\n# print(f\"Response template for collator: {response_template}\")\n# collator = DataCollatorForCompletionOnlyLM(\n#     response_template=response_template, tokenizer=tokenizer, mlm=False\n# )","metadata":{"execution":{"iopub.status.busy":"2023-12-17T09:49:40.998146Z","iopub.status.idle":"2023-12-17T09:49:40.998456Z","shell.execute_reply.started":"2023-12-17T09:49:40.998300Z","shell.execute_reply":"2023-12-17T09:49:40.998315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer = SFTTrainer(\n    model=model,\n    args=training_args,\n    peft_config=peft_config,\n    tokenizer=tokenizer,\n    dataset_text_field='text',\n    train_dataset=train_dataset,\n#     eval_dataset = train_dataset[:20],\n    \n    \n    max_seq_length=256,\n    dataset_num_proc=os.cpu_count(),\n#     formatting_func=formatting_func\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T09:54:28.456986Z","iopub.execute_input":"2023-12-17T09:54:28.457346Z","iopub.status.idle":"2023-12-17T09:54:32.838876Z","shell.execute_reply.started":"2023-12-17T09:54:28.457312Z","shell.execute_reply":"2023-12-17T09:54:32.838006Z"},"trusted":true},"execution_count":27,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2906861be3a841108836b01f8391feb7"}},"metadata":{}},{"name":"stderr","text":"Using auto half precision backend\n","output_type":"stream"}]},{"cell_type":"code","source":"# upcasting the layer norms in torch.bfloat16 for more stable training\nfor name, module in trainer.model.named_modules():\n    if \"norm\" in name:\n        module = module.to(torch.bfloat16)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T09:54:32.840423Z","iopub.execute_input":"2023-12-17T09:54:32.840877Z","iopub.status.idle":"2023-12-17T09:54:32.855906Z","shell.execute_reply.started":"2023-12-17T09:54:32.840837Z","shell.execute_reply":"2023-12-17T09:54:32.854884Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"model.config.use_cache = False\ntrainer.train()\ntrainer.model.save_pretrained(new_model)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-12-17T09:54:32.860430Z","iopub.execute_input":"2023-12-17T09:54:32.860767Z","iopub.status.idle":"2023-12-17T12:00:04.355367Z","shell.execute_reply.started":"2023-12-17T09:54:32.860742Z","shell.execute_reply":"2023-12-17T12:00:04.354038Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stderr","text":"Currently training with a batch size of: 16\n***** Running training *****\n  Num examples = 1,000\n  Num Epochs = 2\n  Instantaneous batch size per device = 8\n  Training with DataParallel so batch size has been adjusted to: 16\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 1\n  Total optimization steps = 126\n  Number of trainable parameters = 28,311,552\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20231217_095502-4dfl706b</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/r1ajayakumaran1751994/huggingface/runs/4dfl706b' target=\"_blank\">cosmic-cherry-40</a></strong> to <a href='https://wandb.ai/r1ajayakumaran1751994/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/r1ajayakumaran1751994/huggingface' target=\"_blank\">https://wandb.ai/r1ajayakumaran1751994/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/r1ajayakumaran1751994/huggingface/runs/4dfl706b' target=\"_blank\">https://wandb.ai/r1ajayakumaran1751994/huggingface/runs/4dfl706b</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='9' max='126' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  9/126 06:15 < 1:44:28, 0.02 it/s, Epoch 0.13/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Currently training with a batch size of: 8\n***** Running training *****\n  Num examples = 1,000\n  Num Epochs = 2\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 250\n  Number of trainable parameters = 28,311,552\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [250/250 1:56:34, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>1.492600</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>1.251500</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>1.123500</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>1.057000</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>1.008500</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.977500</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.945600</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.961200</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.936600</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.959900</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>0.950800</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.962300</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>0.906300</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.801000</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.816900</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.825800</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>0.840300</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.843000</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>0.832200</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.826800</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>0.810600</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>0.793400</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>0.794800</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>0.793400</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.785100</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Saving model checkpoint to ./Zephyr-Try2-17-12/tmp-checkpoint-10\ntokenizer config file saved in ./Zephyr-Try2-17-12/tmp-checkpoint-10/tokenizer_config.json\nSpecial tokens file saved in ./Zephyr-Try2-17-12/tmp-checkpoint-10/special_tokens_map.json\ntokenizer config file saved in ./Zephyr-Try2-17-12/tokenizer_config.json\nSpecial tokens file saved in ./Zephyr-Try2-17-12/special_tokens_map.json\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSaving model checkpoint to ./Zephyr-Try2-17-12/tmp-checkpoint-20\ntokenizer config file saved in ./Zephyr-Try2-17-12/tmp-checkpoint-20/tokenizer_config.json\nSpecial tokens file saved in ./Zephyr-Try2-17-12/tmp-checkpoint-20/special_tokens_map.json\ntokenizer config file saved in ./Zephyr-Try2-17-12/tokenizer_config.json\nSpecial tokens file saved in ./Zephyr-Try2-17-12/special_tokens_map.json\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSaving model checkpoint to ./Zephyr-Try2-17-12/tmp-checkpoint-30\ntokenizer config file saved in ./Zephyr-Try2-17-12/tmp-checkpoint-30/tokenizer_config.json\nSpecial tokens file saved in ./Zephyr-Try2-17-12/tmp-checkpoint-30/special_tokens_map.json\ntokenizer config file saved in ./Zephyr-Try2-17-12/tokenizer_config.json\nSpecial tokens file saved in ./Zephyr-Try2-17-12/special_tokens_map.json\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSaving model checkpoint to ./Zephyr-Try2-17-12/tmp-checkpoint-40\ntokenizer config file saved in ./Zephyr-Try2-17-12/tmp-checkpoint-40/tokenizer_config.json\nSpecial tokens file saved in ./Zephyr-Try2-17-12/tmp-checkpoint-40/special_tokens_map.json\ntokenizer config file saved in ./Zephyr-Try2-17-12/tokenizer_config.json\nSpecial tokens file saved in ./Zephyr-Try2-17-12/special_tokens_map.json\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSaving model checkpoint to ./Zephyr-Try2-17-12/tmp-checkpoint-50\ntokenizer config file saved in ./Zephyr-Try2-17-12/tmp-checkpoint-50/tokenizer_config.json\nSpecial tokens file saved in ./Zephyr-Try2-17-12/tmp-checkpoint-50/special_tokens_map.json\ntokenizer config file saved in ./Zephyr-Try2-17-12/tokenizer_config.json\nSpecial tokens file saved in ./Zephyr-Try2-17-12/special_tokens_map.json\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSaving model checkpoint to ./Zephyr-Try2-17-12/tmp-checkpoint-60\ntokenizer config file saved in ./Zephyr-Try2-17-12/tmp-checkpoint-60/tokenizer_config.json\nSpecial tokens file saved in ./Zephyr-Try2-17-12/tmp-checkpoint-60/special_tokens_map.json\ntokenizer config file saved in ./Zephyr-Try2-17-12/tokenizer_config.json\nSpecial tokens file saved in ./Zephyr-Try2-17-12/special_tokens_map.json\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSaving model checkpoint to ./Zephyr-Try2-17-12/tmp-checkpoint-70\ntokenizer config file saved in ./Zephyr-Try2-17-12/tmp-checkpoint-70/tokenizer_config.json\nSpecial tokens file saved in ./Zephyr-Try2-17-12/tmp-checkpoint-70/special_tokens_map.json\ntokenizer config file saved in ./Zephyr-Try2-17-12/tokenizer_config.json\nSpecial tokens file saved in ./Zephyr-Try2-17-12/special_tokens_map.json\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSaving model checkpoint to ./Zephyr-Try2-17-12/tmp-checkpoint-80\ntokenizer config file saved in ./Zephyr-Try2-17-12/tmp-checkpoint-80/tokenizer_config.json\nSpecial tokens file saved in ./Zephyr-Try2-17-12/tmp-checkpoint-80/special_tokens_map.json\ntokenizer config file saved in ./Zephyr-Try2-17-12/tokenizer_config.json\nSpecial tokens file saved in ./Zephyr-Try2-17-12/special_tokens_map.json\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSaving model checkpoint to ./Zephyr-Try2-17-12/tmp-checkpoint-90\ntokenizer config file saved in ./Zephyr-Try2-17-12/tmp-checkpoint-90/tokenizer_config.json\nSpecial tokens file saved in ./Zephyr-Try2-17-12/tmp-checkpoint-90/special_tokens_map.json\ntokenizer config file saved in ./Zephyr-Try2-17-12/tokenizer_config.json\nSpecial tokens file saved in ./Zephyr-Try2-17-12/special_tokens_map.json\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSaving model checkpoint to ./Zephyr-Try2-17-12/tmp-checkpoint-100\ntokenizer config file saved in ./Zephyr-Try2-17-12/tmp-checkpoint-100/tokenizer_config.json\nSpecial tokens file saved in ./Zephyr-Try2-17-12/tmp-checkpoint-100/special_tokens_map.json\ntokenizer config file saved in ./Zephyr-Try2-17-12/tokenizer_config.json\nSpecial tokens file saved in ./Zephyr-Try2-17-12/special_tokens_map.json\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSaving model checkpoint to ./Zephyr-Try2-17-12/tmp-checkpoint-110\ntokenizer config file saved in ./Zephyr-Try2-17-12/tmp-checkpoint-110/tokenizer_config.json\nSpecial tokens file saved in ./Zephyr-Try2-17-12/tmp-checkpoint-110/special_tokens_map.json\ntokenizer config file saved in ./Zephyr-Try2-17-12/tokenizer_config.json\nSpecial tokens file saved in ./Zephyr-Try2-17-12/special_tokens_map.json\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSaving model checkpoint to ./Zephyr-Try2-17-12/tmp-checkpoint-120\ntokenizer config file saved in ./Zephyr-Try2-17-12/tmp-checkpoint-120/tokenizer_config.json\nSpecial tokens file saved in ./Zephyr-Try2-17-12/tmp-checkpoint-120/special_tokens_map.json\ntokenizer config file saved in ./Zephyr-Try2-17-12/tokenizer_config.json\nSpecial tokens file saved in ./Zephyr-Try2-17-12/special_tokens_map.json\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSaving model checkpoint to ./Zephyr-Try2-17-12/tmp-checkpoint-130\ntokenizer config file saved in ./Zephyr-Try2-17-12/tmp-checkpoint-130/tokenizer_config.json\nSpecial tokens file saved in ./Zephyr-Try2-17-12/tmp-checkpoint-130/special_tokens_map.json\ntokenizer config file saved in ./Zephyr-Try2-17-12/tokenizer_config.json\nSpecial tokens file saved in ./Zephyr-Try2-17-12/special_tokens_map.json\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSaving model checkpoint to ./Zephyr-Try2-17-12/tmp-checkpoint-140\ntokenizer config file saved in ./Zephyr-Try2-17-12/tmp-checkpoint-140/tokenizer_config.json\nSpecial tokens file saved in ./Zephyr-Try2-17-12/tmp-checkpoint-140/special_tokens_map.json\ntokenizer config file saved in ./Zephyr-Try2-17-12/tokenizer_config.json\nSpecial tokens file saved in ./Zephyr-Try2-17-12/special_tokens_map.json\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSaving model checkpoint to ./Zephyr-Try2-17-12/tmp-checkpoint-150\ntokenizer config file saved in ./Zephyr-Try2-17-12/tmp-checkpoint-150/tokenizer_config.json\nSpecial tokens file saved in ./Zephyr-Try2-17-12/tmp-checkpoint-150/special_tokens_map.json\ntokenizer config file saved in ./Zephyr-Try2-17-12/tokenizer_config.json\nSpecial tokens file saved in ./Zephyr-Try2-17-12/special_tokens_map.json\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSaving model checkpoint to ./Zephyr-Try2-17-12/tmp-checkpoint-160\ntokenizer config file saved in ./Zephyr-Try2-17-12/tmp-checkpoint-160/tokenizer_config.json\nSpecial tokens file saved in ./Zephyr-Try2-17-12/tmp-checkpoint-160/special_tokens_map.json\ntokenizer config file saved in ./Zephyr-Try2-17-12/tokenizer_config.json\nSpecial tokens file saved in ./Zephyr-Try2-17-12/special_tokens_map.json\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSaving model checkpoint to ./Zephyr-Try2-17-12/tmp-checkpoint-170\ntokenizer config file saved in ./Zephyr-Try2-17-12/tmp-checkpoint-170/tokenizer_config.json\nSpecial tokens file saved in ./Zephyr-Try2-17-12/tmp-checkpoint-170/special_tokens_map.json\ntokenizer config file saved in ./Zephyr-Try2-17-12/tokenizer_config.json\nSpecial tokens file saved in ./Zephyr-Try2-17-12/special_tokens_map.json\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSaving model checkpoint to ./Zephyr-Try2-17-12/tmp-checkpoint-180\ntokenizer config file saved in ./Zephyr-Try2-17-12/tmp-checkpoint-180/tokenizer_config.json\nSpecial tokens file saved in ./Zephyr-Try2-17-12/tmp-checkpoint-180/special_tokens_map.json\ntokenizer config file saved in ./Zephyr-Try2-17-12/tokenizer_config.json\nSpecial tokens file saved in ./Zephyr-Try2-17-12/special_tokens_map.json\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSaving model checkpoint to ./Zephyr-Try2-17-12/tmp-checkpoint-190\ntokenizer config file saved in ./Zephyr-Try2-17-12/tmp-checkpoint-190/tokenizer_config.json\nSpecial tokens file saved in ./Zephyr-Try2-17-12/tmp-checkpoint-190/special_tokens_map.json\ntokenizer config file saved in ./Zephyr-Try2-17-12/tokenizer_config.json\nSpecial tokens file saved in ./Zephyr-Try2-17-12/special_tokens_map.json\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSaving model checkpoint to ./Zephyr-Try2-17-12/tmp-checkpoint-200\ntokenizer config file saved in ./Zephyr-Try2-17-12/tmp-checkpoint-200/tokenizer_config.json\nSpecial tokens file saved in ./Zephyr-Try2-17-12/tmp-checkpoint-200/special_tokens_map.json\ntokenizer config file saved in ./Zephyr-Try2-17-12/tokenizer_config.json\nSpecial tokens file saved in ./Zephyr-Try2-17-12/special_tokens_map.json\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSaving model checkpoint to ./Zephyr-Try2-17-12/tmp-checkpoint-210\ntokenizer config file saved in ./Zephyr-Try2-17-12/tmp-checkpoint-210/tokenizer_config.json\nSpecial tokens file saved in ./Zephyr-Try2-17-12/tmp-checkpoint-210/special_tokens_map.json\ntokenizer config file saved in ./Zephyr-Try2-17-12/tokenizer_config.json\nSpecial tokens file saved in ./Zephyr-Try2-17-12/special_tokens_map.json\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSaving model checkpoint to ./Zephyr-Try2-17-12/tmp-checkpoint-220\ntokenizer config file saved in ./Zephyr-Try2-17-12/tmp-checkpoint-220/tokenizer_config.json\nSpecial tokens file saved in ./Zephyr-Try2-17-12/tmp-checkpoint-220/special_tokens_map.json\ntokenizer config file saved in ./Zephyr-Try2-17-12/tokenizer_config.json\nSpecial tokens file saved in ./Zephyr-Try2-17-12/special_tokens_map.json\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSaving model checkpoint to ./Zephyr-Try2-17-12/tmp-checkpoint-230\ntokenizer config file saved in ./Zephyr-Try2-17-12/tmp-checkpoint-230/tokenizer_config.json\nSpecial tokens file saved in ./Zephyr-Try2-17-12/tmp-checkpoint-230/special_tokens_map.json\ntokenizer config file saved in ./Zephyr-Try2-17-12/tokenizer_config.json\nSpecial tokens file saved in ./Zephyr-Try2-17-12/special_tokens_map.json\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSaving model checkpoint to ./Zephyr-Try2-17-12/tmp-checkpoint-240\ntokenizer config file saved in ./Zephyr-Try2-17-12/tmp-checkpoint-240/tokenizer_config.json\nSpecial tokens file saved in ./Zephyr-Try2-17-12/tmp-checkpoint-240/special_tokens_map.json\ntokenizer config file saved in ./Zephyr-Try2-17-12/tokenizer_config.json\nSpecial tokens file saved in ./Zephyr-Try2-17-12/special_tokens_map.json\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSaving model checkpoint to ./Zephyr-Try2-17-12/tmp-checkpoint-250\ntokenizer config file saved in ./Zephyr-Try2-17-12/tmp-checkpoint-250/tokenizer_config.json\nSpecial tokens file saved in ./Zephyr-Try2-17-12/tmp-checkpoint-250/special_tokens_map.json\ntokenizer config file saved in ./Zephyr-Try2-17-12/tokenizer_config.json\nSpecial tokens file saved in ./Zephyr-Try2-17-12/special_tokens_map.json\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer.push_to_hub()","metadata":{"execution":{"iopub.status.busy":"2023-12-17T12:01:10.387102Z","iopub.execute_input":"2023-12-17T12:01:10.387490Z","iopub.status.idle":"2023-12-17T12:01:15.977673Z","shell.execute_reply.started":"2023-12-17T12:01:10.387459Z","shell.execute_reply":"2023-12-17T12:01:15.975708Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stderr","text":"Saving model checkpoint to ./Zephyr-Try2-17-12\ntokenizer config file saved in ./Zephyr-Try2-17-12/tokenizer_config.json\nSpecial tokens file saved in ./Zephyr-Try2-17-12/special_tokens_map.json\nDropping the following result as it does not have all the necessary fields:\n{}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/113M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bca1c7e1892648cab5d3d9021afe3267"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.bin:   0%|          | 0.00/113M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"753856512bb64bcf8e9d6fac629084db"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Upload 3 LFS files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1509e18a1ccd419faf3cb3b907db3c99"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"events.out.tfevents.1702806873.c514349ff8e8.42.0:   0%|          | 0.00/13.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98c2a21d49094fe19ab4ea0c4c9d43b1"}},"metadata":{}},{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"'https://huggingface.co/Jaykumaran17/Zephyr-Try2-17-12/tree/main/'"},"metadata":{}}]},{"cell_type":"markdown","source":"# Merging as Single without Adapters","metadata":{}},{"cell_type":"code","source":"# Reload model in FP16 and merge it with LoRA weights\nbase_model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    low_cpu_mem_usage=True,\n    return_dict=True,\n    torch_dtype=torch.float16,\n    device_map=0, ## device map = \"cpu\", for merging on cpu\n)\nmodel = PeftModel.from_pretrained(base_model, new_model)\nmodel = model.merge_and_unload()\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True,use_fast=False,add_eos_token=True)\ntokenizer.pad_token_id = 18610\n\n### Old version tokeniser, can't generate eos token\n# tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n# tokenizer.pad_token = tokenizer.eos_token\n# tokenizer.padding_side = \"right\"\n\n","metadata":{"execution":{"iopub.status.busy":"2023-12-17T12:02:21.991696Z","iopub.execute_input":"2023-12-17T12:02:21.992867Z","iopub.status.idle":"2023-12-17T12:02:23.848142Z","shell.execute_reply.started":"2023-12-17T12:02:21.992823Z","shell.execute_reply":"2023-12-17T12:02:23.846663Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--HuggingFaceH4--zephyr-7b-beta/snapshots/dc24cabd13eacd3ae3a5fe574bd645483a335a4a/config.json\nModel config MistralConfig {\n  \"_name_or_path\": \"HuggingFaceH4/zephyr-7b-beta\",\n  \"architectures\": [\n    \"MistralForCausalLM\"\n  ],\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 2,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 32768,\n  \"model_type\": \"mistral\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 2,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_theta\": 10000.0,\n  \"sliding_window\": 4096,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"float16\",\n  \"transformers_version\": \"4.36.1\",\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}\n\nloading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--HuggingFaceH4--zephyr-7b-beta/snapshots/dc24cabd13eacd3ae3a5fe574bd645483a335a4a/model.safetensors.index.json\nInstantiating MistralForCausalLM model under default dtype torch.float16.\nGenerate config GenerationConfig {\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 2,\n  \"pad_token_id\": 2\n}\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0062eacd00324789be57c7435177c775"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[34], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Reload model in FP16 and merge it with LoRA weights\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m base_model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m## device map = \"cpu\", for merging on cpu\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m model \u001b[38;5;241m=\u001b[39m PeftModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(base_model, new_model)\n\u001b[1;32m     10\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mmerge_and_unload()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:566\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    565\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 566\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    572\u001b[0m )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:3694\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3685\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3686\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   3687\u001b[0m     (\n\u001b[1;32m   3688\u001b[0m         model,\n\u001b[1;32m   3689\u001b[0m         missing_keys,\n\u001b[1;32m   3690\u001b[0m         unexpected_keys,\n\u001b[1;32m   3691\u001b[0m         mismatched_keys,\n\u001b[1;32m   3692\u001b[0m         offload_index,\n\u001b[1;32m   3693\u001b[0m         error_msgs,\n\u001b[0;32m-> 3694\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3695\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloaded_state_dict_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   3698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3701\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3702\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_fast_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fast_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3703\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3704\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3705\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3706\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3707\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3708\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_quantized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquantization_method\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mQuantizationMethod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBITS_AND_BYTES\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3709\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3710\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3712\u001b[0m model\u001b[38;5;241m.\u001b[39mis_loaded_in_4bit \u001b[38;5;241m=\u001b[39m load_in_4bit\n\u001b[1;32m   3713\u001b[0m model\u001b[38;5;241m.\u001b[39mis_loaded_in_8bit \u001b[38;5;241m=\u001b[39m load_in_8bit\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:4104\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, is_quantized, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m   4100\u001b[0m                     set_module_quantized_tensor_to_device(\n\u001b[1;32m   4101\u001b[0m                         model_to_load, key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;241m*\u001b[39mparam\u001b[38;5;241m.\u001b[39msize(), dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m   4102\u001b[0m                     )\n\u001b[1;32m   4103\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4104\u001b[0m         new_error_msgs, offload_index, state_dict_index \u001b[38;5;241m=\u001b[39m \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4105\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4106\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4107\u001b[0m \u001b[43m            \u001b[49m\u001b[43mloaded_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4108\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstart_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4109\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4110\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4111\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4112\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4113\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4114\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4115\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4116\u001b[0m \u001b[43m            \u001b[49m\u001b[43mis_quantized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_quantized\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4117\u001b[0m \u001b[43m            \u001b[49m\u001b[43mis_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4118\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4119\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4120\u001b[0m         error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m new_error_msgs\n\u001b[1;32m   4121\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:778\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, loaded_state_dict_keys, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, is_quantized, is_safetensors, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m    775\u001b[0m     state_dict_index \u001b[38;5;241m=\u001b[39m offload_weight(param, param_name, state_dict_folder, state_dict_index)\n\u001b[1;32m    776\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_quantized:\n\u001b[1;32m    777\u001b[0m     \u001b[38;5;66;03m# For backward compatibility with older versions of `accelerate`\u001b[39;00m\n\u001b[0;32m--> 778\u001b[0m     \u001b[43mset_module_tensor_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mset_module_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    779\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m param\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mint8 \u001b[38;5;129;01mand\u001b[39;00m param_name\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSCB\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01min\u001b[39;00m state_dict\u001b[38;5;241m.\u001b[39mkeys():\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/utils/modeling.py:317\u001b[0m, in \u001b[0;36mset_module_tensor_to_device\u001b[0;34m(module, tensor_name, device, value, dtype, fp16_statistics)\u001b[0m\n\u001b[1;32m    315\u001b[0m             module\u001b[38;5;241m.\u001b[39m_parameters[tensor_name] \u001b[38;5;241m=\u001b[39m param_cls(new_value, requires_grad\u001b[38;5;241m=\u001b[39mold_value\u001b[38;5;241m.\u001b[39mrequires_grad)\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 317\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m \u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    319\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(value, device\u001b[38;5;241m=\u001b[39mdevice)\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 250.00 MiB (GPU 0; 14.76 GiB total capacity; 13.54 GiB already allocated; 3.75 MiB free; 13.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 250.00 MiB (GPU 0; 14.76 GiB total capacity; 13.54 GiB already allocated; 3.75 MiB free; 13.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error"}]},{"cell_type":"code","source":"new_model = \"Zephyr-Try2-17-12\"","metadata":{"execution":{"iopub.status.busy":"2023-12-17T12:01:22.714038Z","iopub.status.idle":"2023-12-17T12:01:22.714365Z","shell.execute_reply.started":"2023-12-17T12:01:22.714206Z","shell.execute_reply":"2023-12-17T12:01:22.714221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.push_to_hub(new_model, use_temp_dir=False)\ntokenizer.push_to_hub(new_model, use_temp_dir=False)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T12:02:41.465671Z","iopub.execute_input":"2023-12-17T12:02:41.466601Z","iopub.status.idle":"2023-12-17T12:02:41.633939Z","shell.execute_reply.started":"2023-12-17T12:02:41.466560Z","shell.execute_reply":"2023-12-17T12:02:41.632107Z"},"trusted":true},"execution_count":36,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpush_to_hub\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_temp_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mpush_to_hub(new_model, use_temp_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:871\u001b[0m, in \u001b[0;36mPushToHubMixin.push_to_hub\u001b[0;34m(self, repo_id, use_temp_dir, commit_message, private, token, max_shard_size, create_pr, safe_serialization, revision, commit_description, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    868\u001b[0m files_timestamps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_files_timestamps(work_dir)\n\u001b[1;32m    870\u001b[0m \u001b[38;5;66;03m# Save all files.\u001b[39;00m\n\u001b[0;32m--> 871\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwork_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_shard_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_shard_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_serialization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafe_serialization\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_upload_modified_files(\n\u001b[1;32m    874\u001b[0m     work_dir,\n\u001b[1;32m    875\u001b[0m     repo_id,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    881\u001b[0m     commit_description\u001b[38;5;241m=\u001b[39mcommit_description,\n\u001b[1;32m    882\u001b[0m )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:2206\u001b[0m, in \u001b[0;36mPreTrainedModel.save_pretrained\u001b[0;34m(self, save_directory, is_main_process, state_dict, save_function, push_to_hub, max_shard_size, safe_serialization, variant, token, save_peft_format, **kwargs)\u001b[0m\n\u001b[1;32m   2204\u001b[0m \u001b[38;5;66;03m# If the model has adapters attached, you can save the adapters\u001b[39;00m\n\u001b[1;32m   2205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_loaded_in_4bit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _hf_peft_config_loaded:\n\u001b[0;32m-> 2206\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m   2207\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are calling `save_pretrained` on a 4-bit converted model. This is currently not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2208\u001b[0m     )\n\u001b[1;32m   2210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_awq_is_fused\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   2211\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot save an AWQ model that uses fused modules!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mNotImplementedError\u001b[0m: You are calling `save_pretrained` on a 4-bit converted model. This is currently not supported"],"ename":"NotImplementedError","evalue":"You are calling `save_pretrained` on a 4-bit converted model. This is currently not supported","output_type":"error"}]},{"cell_type":"code","source":"## for inference without merging qlora weights with original\nbase_model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=\"auto\"\n)\nmodel = PeftModel.from_pretrained(base_model, new_model)\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True,use_fast=False,add_eos_token=True)\ntokenizer.pad_token_id = 18610","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **LangchainMemory**","metadata":{}},{"cell_type":"code","source":"!pip install langchain\n# !pip install openai\n!pip install tiktoken","metadata":{"execution":{"iopub.status.busy":"2023-12-14T13:39:56.708484Z","iopub.execute_input":"2023-12-14T13:39:56.708762Z","iopub.status.idle":"2023-12-14T13:40:21.313279Z","shell.execute_reply.started":"2023-12-14T13:39:56.708740Z","shell.execute_reply":"2023-12-14T13:40:21.312083Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Requirement already satisfied: langchain in /opt/conda/lib/python3.10/site-packages (0.0.350)\nRequirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (6.0.1)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.0.20)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (3.8.5)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (4.0.3)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.10/site-packages (from langchain) (0.6.3)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.33)\nRequirement already satisfied: langchain-community<0.1,>=0.0.2 in /opt/conda/lib/python3.10/site-packages (from langchain) (0.0.3)\nRequirement already satisfied: langchain-core<0.2,>=0.1 in /opt/conda/lib/python3.10/site-packages (from langchain) (0.1.0)\nRequirement already satisfied: langsmith<0.1.0,>=0.0.63 in /opt/conda/lib/python3.10/site-packages (from langchain) (0.0.70)\nRequirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.24.3)\nRequirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.10.12)\nRequirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.31.0)\nRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (8.2.3)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.1)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\nRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.0)\nRequirement already satisfied: anyio<5,>=3 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.2,>=0.1->langchain) (3.7.1)\nRequirement already satisfied: packaging<24.0,>=23.2 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.2,>=0.1->langchain) (23.2)\nRequirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (4.5.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2023.11.17)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\nRequirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1->langchain) (1.3.0)\nRequirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1->langchain) (1.1.3)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\nRequirement already satisfied: tiktoken in /opt/conda/lib/python3.10/site-packages (0.5.2)\nRequirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.10/site-packages (from tiktoken) (2023.8.8)\nRequirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.10/site-packages (from tiktoken) (2.31.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2023.11.17)\n","output_type":"stream"}]},{"cell_type":"code","source":"from langchain.chains import RetrievalQA\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.llms import LlamaCpp\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.vectorstores import FAISS\nfrom langchain.document_loaders import PyPDFDirectoryLoader","metadata":{"execution":{"iopub.status.busy":"2023-12-14T13:40:21.316327Z","iopub.execute_input":"2023-12-14T13:40:21.316668Z","iopub.status.idle":"2023-12-14T13:40:23.073749Z","shell.execute_reply.started":"2023-12-14T13:40:21.316637Z","shell.execute_reply":"2023-12-14T13:40:23.072791Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"!pip install llama-cpp-python","metadata":{"execution":{"iopub.status.busy":"2023-12-14T13:40:35.186517Z","iopub.execute_input":"2023-12-14T13:40:35.187549Z","iopub.status.idle":"2023-12-14T13:40:47.440274Z","shell.execute_reply.started":"2023-12-14T13:40:35.187502Z","shell.execute_reply":"2023-12-14T13:40:47.439101Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Requirement already satisfied: llama-cpp-python in /opt/conda/lib/python3.10/site-packages (0.2.23)\nRequirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/lib/python3.10/site-packages (from llama-cpp-python) (4.5.0)\nRequirement already satisfied: numpy>=1.20.0 in /opt/conda/lib/python3.10/site-packages (from llama-cpp-python) (1.24.3)\nRequirement already satisfied: diskcache>=5.6.1 in /opt/conda/lib/python3.10/site-packages (from llama-cpp-python) (5.6.3)\n","output_type":"stream"}]},{"cell_type":"code","source":"base_model = model\nbase_tokenizer = tokenizer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"llm = llm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from langchain.memory import ConversationBufferMemory\n\nmemory = ConversationBufferMemory(memory_key=\"chat_history\", max_turns=10)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-14T13:49:13.586292Z","iopub.execute_input":"2023-12-14T13:49:13.587001Z","iopub.status.idle":"2023-12-14T13:49:13.591421Z","shell.execute_reply.started":"2023-12-14T13:49:13.586966Z","shell.execute_reply":"2023-12-14T13:49:13.590464Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"from typing import Any, List, Mapping, Optional\n\nfrom langchain.callbacks.manager import CallbackManagerForLLMRun\nfrom langchain_core.language_models.llms import LLM","metadata":{"execution":{"iopub.status.busy":"2023-12-14T13:40:47.448520Z","iopub.execute_input":"2023-12-14T13:40:47.448848Z","iopub.status.idle":"2023-12-14T13:40:47.460084Z","shell.execute_reply.started":"2023-12-14T13:40:47.448816Z","shell.execute_reply":"2023-12-14T13:40:47.459255Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"!pip install langchain ctransformers transformers\nfrom langchain.llms import CTransformers","metadata":{"execution":{"iopub.status.busy":"2023-12-14T13:40:47.461153Z","iopub.execute_input":"2023-12-14T13:40:47.461407Z","iopub.status.idle":"2023-12-14T13:41:00.069185Z","shell.execute_reply.started":"2023-12-14T13:40:47.461384Z","shell.execute_reply":"2023-12-14T13:41:00.067960Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Requirement already satisfied: langchain in /opt/conda/lib/python3.10/site-packages (0.0.350)\nRequirement already satisfied: ctransformers in /opt/conda/lib/python3.10/site-packages (0.2.27)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.35.2)\nRequirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (6.0.1)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.0.20)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (3.8.5)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (4.0.3)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.10/site-packages (from langchain) (0.6.3)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.33)\nRequirement already satisfied: langchain-community<0.1,>=0.0.2 in /opt/conda/lib/python3.10/site-packages (from langchain) (0.0.3)\nRequirement already satisfied: langchain-core<0.2,>=0.1 in /opt/conda/lib/python3.10/site-packages (from langchain) (0.1.0)\nRequirement already satisfied: langsmith<0.1.0,>=0.0.63 in /opt/conda/lib/python3.10/site-packages (from langchain) (0.0.70)\nRequirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.24.3)\nRequirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.10.12)\nRequirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.31.0)\nRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (8.2.3)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from ctransformers) (0.19.4)\nRequirement already satisfied: py-cpuinfo<10.0.0,>=9.0.0 in /opt/conda/lib/python3.10/site-packages (from ctransformers) (9.0.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.2)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (23.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.8.8)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.0)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.1)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->ctransformers) (2023.10.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->ctransformers) (4.5.0)\nRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.0)\nRequirement already satisfied: anyio<5,>=3 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.2,>=0.1->langchain) (3.7.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2023.11.17)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\nRequirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1->langchain) (1.3.0)\nRequirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1->langchain) (1.1.3)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"config = {'max_new_tokens': 4096, 'temperature': 0.7, 'context_length': 4096}\nllm = CTransformers(model=\"Jaykumaran17/Samantha-Try2-13-12\",\n#                         model_file=\"mistral-7b-instruct-v0.1.Q4_K_M.gguf\",\n                        config=config,\n                        threads=os.cpu_count())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"config = {'max_new_tokens': 4096, 'temperature': 0.7, 'context_length': 4096}\nllm = CTransformers(model=\"TheBloke/samantha-1.2-mistral-7B-GGUF\",\n                        model_file=\"samantha-1.2-mistral-7b.Q4_K_M.gguf\",\n                        config=config,\n                        model_type=\"mistral\",\n                        threads=os.cpu_count())","metadata":{"execution":{"iopub.status.busy":"2023-12-14T13:42:57.792090Z","iopub.execute_input":"2023-12-14T13:42:57.792463Z","iopub.status.idle":"2023-12-14T13:44:39.620777Z","shell.execute_reply.started":"2023-12-14T13:42:57.792433Z","shell.execute_reply":"2023-12-14T13:44:39.619784Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb421f19bfe546528bf7b789b436d748"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"174a784a64924b37a7cb8a0402d8ec90"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"samantha-1.2-mistral-7b.Q4_K_M.gguf:   0%|          | 0.00/4.37G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"144a184ff1bf4113a3d45c1f6f3bc6d2"}},"metadata":{}}]},{"cell_type":"code","source":"\nfrom textwrap import fill\nfrom IPython.display import Markdown, display\n\nfrom langchain.prompts.chat import (\n    ChatPromptTemplate,\n    HumanMessagePromptTemplate,\n    SystemMessagePromptTemplate,\n    )\n\nfrom langchain import PromptTemplate\nfrom langchain import HuggingFacePipeline\n\nfrom langchain.vectorstores import Chroma\nfrom langchain.schema import AIMessage, HumanMessage\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.document_loaders import UnstructuredMarkdownLoader, UnstructuredURLLoader\nfrom langchain.chains import LLMChain, SimpleSequentialChain, RetrievalQA, ConversationalRetrievalChain\n\nfrom transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2023-12-14T14:33:20.583565Z","iopub.execute_input":"2023-12-14T14:33:20.583955Z","iopub.status.idle":"2023-12-14T14:33:20.620277Z","shell.execute_reply.started":"2023-12-14T14:33:20.583925Z","shell.execute_reply":"2023-12-14T14:33:20.619282Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"!pip install chromadb","metadata":{"execution":{"iopub.status.busy":"2023-12-14T14:35:06.051863Z","iopub.execute_input":"2023-12-14T14:35:06.052597Z","iopub.status.idle":"2023-12-14T14:35:50.254639Z","shell.execute_reply.started":"2023-12-14T14:35:06.052564Z","shell.execute_reply":"2023-12-14T14:35:50.253563Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"Collecting chromadb\n  Obtaining dependency information for chromadb from https://files.pythonhosted.org/packages/0e/69/71e3aac0b3d170276a35d30d5f9cb6f29aa9edf872c80c8285530348e474/chromadb-0.4.19-py3-none-any.whl.metadata\n  Downloading chromadb-0.4.19-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: requests>=2.28 in /opt/conda/lib/python3.10/site-packages (from chromadb) (2.31.0)\nRequirement already satisfied: pydantic>=1.9 in /opt/conda/lib/python3.10/site-packages (from chromadb) (1.10.12)\nCollecting chroma-hnswlib==0.7.3 (from chromadb)\n  Obtaining dependency information for chroma-hnswlib==0.7.3 from https://files.pythonhosted.org/packages/2f/48/f7609a3cb15a24c5d8ec18911ce10ac94144e9a89584f0a86bf9871b024c/chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n  Downloading chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\nRequirement already satisfied: fastapi>=0.95.2 in /opt/conda/lib/python3.10/site-packages (from chromadb) (0.101.1)\nRequirement already satisfied: uvicorn[standard]>=0.18.3 in /opt/conda/lib/python3.10/site-packages (from chromadb) (0.23.2)\nRequirement already satisfied: numpy>=1.22.5 in /opt/conda/lib/python3.10/site-packages (from chromadb) (1.24.3)\nCollecting posthog>=2.4.0 (from chromadb)\n  Obtaining dependency information for posthog>=2.4.0 from https://files.pythonhosted.org/packages/3b/82/441cb77a43499661228048dcd0d21e0ae3235b442d0f1b9b606e29c2a5ed/posthog-3.1.0-py2.py3-none-any.whl.metadata\n  Downloading posthog-3.1.0-py2.py3-none-any.whl.metadata (2.0 kB)\nRequirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (4.5.0)\nCollecting pulsar-client>=3.1.0 (from chromadb)\n  Obtaining dependency information for pulsar-client>=3.1.0 from https://files.pythonhosted.org/packages/b7/54/ef01474b40f70f59b459497bdd48a28fc582c0cde1914fa3efa53053a23e/pulsar_client-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n  Downloading pulsar_client-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.0 kB)\nCollecting onnxruntime>=1.14.1 (from chromadb)\n  Obtaining dependency information for onnxruntime>=1.14.1 from https://files.pythonhosted.org/packages/7a/cf/6aa8c56fd63f53c2c485921e411269c7b501a2b4e634bd02f226ab2d5d8e/onnxruntime-1.16.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n  Downloading onnxruntime-1.16.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\nRequirement already satisfied: opentelemetry-api>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (1.19.0)\nRequirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (1.19.0)\nCollecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n  Obtaining dependency information for opentelemetry-instrumentation-fastapi>=0.41b0 from https://files.pythonhosted.org/packages/0a/47/411f4dd5560f004c4d59e977fb6e1f511babfc399f3656b6eafd8f67c0ce/opentelemetry_instrumentation_fastapi-0.42b0-py3-none-any.whl.metadata\n  Downloading opentelemetry_instrumentation_fastapi-0.42b0-py3-none-any.whl.metadata (2.3 kB)\nRequirement already satisfied: opentelemetry-sdk>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (1.19.0)\nRequirement already satisfied: tokenizers>=0.13.2 in /opt/conda/lib/python3.10/site-packages (from chromadb) (0.15.0)\nCollecting pypika>=0.48.9 (from chromadb)\n  Downloading PyPika-0.48.9.tar.gz (67 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: tqdm>=4.65.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (4.66.1)\nCollecting overrides>=7.3.1 (from chromadb)\n  Obtaining dependency information for overrides>=7.3.1 from https://files.pythonhosted.org/packages/da/28/3fa6ef8297302fc7b3844980b6c5dbc71cdbd4b61e9b2591234214d5ab39/overrides-7.4.0-py3-none-any.whl.metadata\n  Downloading overrides-7.4.0-py3-none-any.whl.metadata (5.7 kB)\nRequirement already satisfied: importlib-resources in /opt/conda/lib/python3.10/site-packages (from chromadb) (5.13.0)\nCollecting grpcio>=1.58.0 (from chromadb)\n  Obtaining dependency information for grpcio>=1.58.0 from https://files.pythonhosted.org/packages/ed/bd/4dbe2ae13ffba7eef2a3bd2dcebbc2255da18d1a972a89952d55e8ad3d4b/grpcio-1.60.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n  Downloading grpcio-1.60.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\nCollecting bcrypt>=4.0.1 (from chromadb)\n  Obtaining dependency information for bcrypt>=4.0.1 from https://files.pythonhosted.org/packages/af/82/96ffdbe0f56b12db0da8f1a9c869399d22231ed1313a84ea2ddc6381a498/bcrypt-4.1.1-cp37-abi3-manylinux_2_28_x86_64.whl.metadata\n  Downloading bcrypt-4.1.1-cp37-abi3-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\nRequirement already satisfied: typer>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (0.9.0)\nCollecting kubernetes>=28.1.0 (from chromadb)\n  Obtaining dependency information for kubernetes>=28.1.0 from https://files.pythonhosted.org/packages/f5/6a/1f69c2d8b1ff03f8d8e10d801f4ac3016ed4c1b00aa9795732c6ec900bba/kubernetes-28.1.0-py2.py3-none-any.whl.metadata\n  Downloading kubernetes-28.1.0-py2.py3-none-any.whl.metadata (1.5 kB)\nRequirement already satisfied: tenacity>=8.2.3 in /opt/conda/lib/python3.10/site-packages (from chromadb) (8.2.3)\nRequirement already satisfied: PyYAML>=6.0.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (6.0.1)\nRequirement already satisfied: mmh3>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from chromadb) (4.0.1)\nRequirement already satisfied: starlette<0.28.0,>=0.27.0 in /opt/conda/lib/python3.10/site-packages (from fastapi>=0.95.2->chromadb) (0.27.0)\nRequirement already satisfied: certifi>=14.05.14 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2023.11.17)\nRequirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\nRequirement already satisfied: python-dateutil>=2.5.3 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\nRequirement already satisfied: google-auth>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.22.0)\nRequirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.6.2)\nRequirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\nRequirement already satisfied: oauthlib>=3.2.2 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\nRequirement already satisfied: urllib3<2.0,>=1.24.2 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.26.15)\nCollecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: flatbuffers in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (23.2)\nRequirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\nRequirement already satisfied: deprecated>=1.2.6 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.14)\nRequirement already satisfied: importlib-metadata~=6.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-api>=1.2.0->chromadb) (6.8.0)\nRequirement already satisfied: backoff<3.0.0,>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (2.2.1)\nRequirement already satisfied: googleapis-common-protos~=1.52 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.60.0)\nRequirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.19.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.19.0)\nRequirement already satisfied: opentelemetry-proto==1.19.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.19.0)\nCollecting opentelemetry-instrumentation-asgi==0.42b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Obtaining dependency information for opentelemetry-instrumentation-asgi==0.42b0 from https://files.pythonhosted.org/packages/a9/ef/ac6ef3e2c03750588dab596247204e7f960b509af37f0001ac1e4ac23f2a/opentelemetry_instrumentation_asgi-0.42b0-py3-none-any.whl.metadata\n  Downloading opentelemetry_instrumentation_asgi-0.42b0-py3-none-any.whl.metadata (2.1 kB)\nCollecting opentelemetry-instrumentation==0.42b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Obtaining dependency information for opentelemetry-instrumentation==0.42b0 from https://files.pythonhosted.org/packages/84/33/8e6b97dcb807c1ba5fd84910c091ae4b1b52d74ea24b0574e19f58cce99c/opentelemetry_instrumentation-0.42b0-py3-none-any.whl.metadata\n  Downloading opentelemetry_instrumentation-0.42b0-py3-none-any.whl.metadata (5.9 kB)\nCollecting opentelemetry-semantic-conventions==0.42b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Obtaining dependency information for opentelemetry-semantic-conventions==0.42b0 from https://files.pythonhosted.org/packages/c3/0c/4c99cbe85b65fbba5a638cb7d913cb3acead3d83b4c47763be28d418bb95/opentelemetry_semantic_conventions-0.42b0-py3-none-any.whl.metadata\n  Downloading opentelemetry_semantic_conventions-0.42b0-py3-none-any.whl.metadata (2.3 kB)\nCollecting opentelemetry-util-http==0.42b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Obtaining dependency information for opentelemetry-util-http==0.42b0 from https://files.pythonhosted.org/packages/3f/33/7528bd28710e3fd669ee7c5d98bc3392eb4588d451e28352696bb23c1520/opentelemetry_util_http-0.42b0-py3-none-any.whl.metadata\n  Downloading opentelemetry_util_http-0.42b0-py3-none-any.whl.metadata (2.5 kB)\nRequirement already satisfied: setuptools>=16.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-instrumentation==0.42b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (68.1.2)\nRequirement already satisfied: wrapt<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-instrumentation==0.42b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.15.0)\nCollecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.42b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Obtaining dependency information for asgiref~=3.0 from https://files.pythonhosted.org/packages/9b/80/b9051a4a07ad231558fcd8ffc89232711b4e618c15cb7a392a17384bbeef/asgiref-3.7.2-py3-none-any.whl.metadata\n  Downloading asgiref-3.7.2-py3-none-any.whl.metadata (9.2 kB)\nINFO: pip is looking at multiple versions of opentelemetry-sdk to determine which version is compatible with other requirements. This could take a while.\nCollecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n  Obtaining dependency information for opentelemetry-instrumentation-fastapi>=0.41b0 from https://files.pythonhosted.org/packages/1e/ea/f355de7cdce359a07e9de488830126962e0e30120ad7effa5cd8bc60f325/opentelemetry_instrumentation_fastapi-0.41b0-py3-none-any.whl.metadata\n  Downloading opentelemetry_instrumentation_fastapi-0.41b0-py3-none-any.whl.metadata (2.3 kB)\nCollecting opentelemetry-instrumentation-asgi==0.41b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Obtaining dependency information for opentelemetry-instrumentation-asgi==0.41b0 from https://files.pythonhosted.org/packages/92/12/3a8a0540e0de80c1dcf7a2845cbeef01d2b86483c5bd89b502422787b2bb/opentelemetry_instrumentation_asgi-0.41b0-py3-none-any.whl.metadata\n  Downloading opentelemetry_instrumentation_asgi-0.41b0-py3-none-any.whl.metadata (2.1 kB)\nCollecting opentelemetry-instrumentation==0.41b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Obtaining dependency information for opentelemetry-instrumentation==0.41b0 from https://files.pythonhosted.org/packages/ca/c6/09f554c655bc93b37e54915257236d2462a120d12b01cf4699e178a5f612/opentelemetry_instrumentation-0.41b0-py3-none-any.whl.metadata\n  Downloading opentelemetry_instrumentation-0.41b0-py3-none-any.whl.metadata (5.9 kB)\nCollecting opentelemetry-semantic-conventions==0.41b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Obtaining dependency information for opentelemetry-semantic-conventions==0.41b0 from https://files.pythonhosted.org/packages/aa/78/7a7508d16d32f92d6b206b2e367c5f044b3e652e7f385bbf17f49baad189/opentelemetry_semantic_conventions-0.41b0-py3-none-any.whl.metadata\n  Downloading opentelemetry_semantic_conventions-0.41b0-py3-none-any.whl.metadata (2.3 kB)\nCollecting opentelemetry-util-http==0.41b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Obtaining dependency information for opentelemetry-util-http==0.41b0 from https://files.pythonhosted.org/packages/4b/80/592edc50d116250f5e9eb60ae9ea5143ffde52e52f84642cec5175587ffe/opentelemetry_util_http-0.41b0-py3-none-any.whl.metadata\n  Downloading opentelemetry_util_http-0.41b0-py3-none-any.whl.metadata (2.5 kB)\nCollecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n  Obtaining dependency information for opentelemetry-exporter-otlp-proto-grpc>=1.2.0 from https://files.pythonhosted.org/packages/75/59/ec3e39fe164c61306998cdd3cd30a857c4da2f8d3141204a929e57668eee/opentelemetry_exporter_otlp_proto_grpc-1.21.0-py3-none-any.whl.metadata\n  Downloading opentelemetry_exporter_otlp_proto_grpc-1.21.0-py3-none-any.whl.metadata (2.4 kB)\nCollecting opentelemetry-sdk>=1.2.0 (from chromadb)\n  Obtaining dependency information for opentelemetry-sdk>=1.2.0 from https://files.pythonhosted.org/packages/c3/08/ca8b1ef7a2fa3f1ea2f12770eca8976098066adb442b1da81fea3b370123/opentelemetry_sdk-1.21.0-py3-none-any.whl.metadata\n  Downloading opentelemetry_sdk-1.21.0-py3-none-any.whl.metadata (1.5 kB)\nCollecting opentelemetry-api>=1.2.0 (from chromadb)\n  Obtaining dependency information for opentelemetry-api>=1.2.0 from https://files.pythonhosted.org/packages/51/3a/945e6c21f405ac4ea526f91ee09cc1568c04e0c95d3392903e6984c8f0e0/opentelemetry_api-1.21.0-py3-none-any.whl.metadata\n  Downloading opentelemetry_api-1.21.0-py3-none-any.whl.metadata (1.4 kB)\nCollecting opentelemetry-exporter-otlp-proto-common==1.21.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n  Obtaining dependency information for opentelemetry-exporter-otlp-proto-common==1.21.0 from https://files.pythonhosted.org/packages/2a/60/ec618caf8fd8a4ac50500565eb49038ec42b7b168df9a316494085a740a6/opentelemetry_exporter_otlp_proto_common-1.21.0-py3-none-any.whl.metadata\n  Downloading opentelemetry_exporter_otlp_proto_common-1.21.0-py3-none-any.whl.metadata (1.8 kB)\nCollecting opentelemetry-proto==1.21.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n  Obtaining dependency information for opentelemetry-proto==1.21.0 from https://files.pythonhosted.org/packages/69/c2/d11b5fbf95adf68440ff4c953e2d8d027c9c62ece79b78372af95af590c9/opentelemetry_proto-1.21.0-py3-none-any.whl.metadata\n  Downloading opentelemetry_proto-1.21.0-py3-none-any.whl.metadata (2.3 kB)\nCollecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.28->chromadb) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.28->chromadb) (3.4)\nRequirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /opt/conda/lib/python3.10/site-packages (from tokenizers>=0.13.2->chromadb) (0.19.4)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.10/site-packages (from typer>=0.9.0->chromadb) (8.1.7)\nRequirement already satisfied: h11>=0.8 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.14.0)\nRequirement already satisfied: httptools>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.1)\nRequirement already satisfied: python-dotenv>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.0)\nRequirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.19.0)\nRequirement already satisfied: watchfiles>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\nRequirement already satisfied: websockets>=10.4 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (12.0)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.2.7)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.12.2)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2023.10.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata~=6.0->opentelemetry-api>=1.2.0->chromadb) (3.16.2)\nRequirement already satisfied: anyio<5,>=3.4.0 in /opt/conda/lib/python3.10/site-packages (from starlette<0.28.0,>=0.27.0->fastapi>=0.95.2->chromadb) (3.7.1)\nCollecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\nRequirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi>=0.95.2->chromadb) (1.3.0)\nRequirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi>=0.95.2->chromadb) (1.1.3)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.8)\nDownloading chromadb-0.4.19-py3-none-any.whl (505 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m506.0/506.0 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading bcrypt-4.1.1-cp37-abi3-manylinux_2_28_x86_64.whl (699 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m699.4/699.4 kB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading grpcio-1.60.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m66.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading kubernetes-28.1.0-py2.py3-none-any.whl (1.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading onnxruntime-1.16.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading opentelemetry_instrumentation_fastapi-0.42b0-py3-none-any.whl (11 kB)\nDownloading opentelemetry_instrumentation_asgi-0.42b0-py3-none-any.whl (13 kB)\nDownloading opentelemetry_semantic_conventions-0.42b0-py3-none-any.whl (36 kB)\nDownloading opentelemetry_instrumentation-0.42b0-py3-none-any.whl (25 kB)\nDownloading opentelemetry_util_http-0.42b0-py3-none-any.whl (6.9 kB)\nDownloading opentelemetry_sdk-1.21.0-py3-none-any.whl (105 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.3/105.3 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading opentelemetry_api-1.21.0-py3-none-any.whl (57 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.9/57.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.21.0-py3-none-any.whl (18 kB)\nDownloading opentelemetry_exporter_otlp_proto_common-1.21.0-py3-none-any.whl (17 kB)\nDownloading opentelemetry_proto-1.21.0-py3-none-any.whl (50 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading overrides-7.4.0-py3-none-any.whl (17 kB)\nDownloading posthog-3.1.0-py2.py3-none-any.whl (37 kB)\nDownloading pulsar_client-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0mmm\n\u001b[?25hDownloading asgiref-3.7.2-py3-none-any.whl (24 kB)\nBuilding wheels for collected packages: pypika\n  Building wheel for pypika (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53723 sha256=d2fd9b1b430aa1da9f207f3b9379ab6f0945be2e9dbedba579e5f61b74e35ec1\n  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\nSuccessfully built pypika\nInstalling collected packages: pypika, monotonic, pulsar-client, overrides, opentelemetry-util-http, opentelemetry-semantic-conventions, opentelemetry-proto, humanfriendly, grpcio, chroma-hnswlib, bcrypt, asgiref, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, coloredlogs, opentelemetry-sdk, opentelemetry-instrumentation, onnxruntime, kubernetes, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-instrumentation-fastapi, chromadb\n  Attempting uninstall: overrides\n    Found existing installation: overrides 6.5.0\n    Uninstalling overrides-6.5.0:\n      Successfully uninstalled overrides-6.5.0\n  Attempting uninstall: opentelemetry-semantic-conventions\n    Found existing installation: opentelemetry-semantic-conventions 0.40b0\n    Uninstalling opentelemetry-semantic-conventions-0.40b0:\n      Successfully uninstalled opentelemetry-semantic-conventions-0.40b0\n  Attempting uninstall: opentelemetry-proto\n    Found existing installation: opentelemetry-proto 1.19.0\n    Uninstalling opentelemetry-proto-1.19.0:\n      Successfully uninstalled opentelemetry-proto-1.19.0\n  Attempting uninstall: grpcio\n    Found existing installation: grpcio 1.51.1\n    Uninstalling grpcio-1.51.1:\n      Successfully uninstalled grpcio-1.51.1\n  Attempting uninstall: opentelemetry-exporter-otlp-proto-common\n    Found existing installation: opentelemetry-exporter-otlp-proto-common 1.19.0\n    Uninstalling opentelemetry-exporter-otlp-proto-common-1.19.0:\n      Successfully uninstalled opentelemetry-exporter-otlp-proto-common-1.19.0\n  Attempting uninstall: opentelemetry-api\n    Found existing installation: opentelemetry-api 1.19.0\n    Uninstalling opentelemetry-api-1.19.0:\n      Successfully uninstalled opentelemetry-api-1.19.0\n  Attempting uninstall: opentelemetry-sdk\n    Found existing installation: opentelemetry-sdk 1.19.0\n    Uninstalling opentelemetry-sdk-1.19.0:\n      Successfully uninstalled opentelemetry-sdk-1.19.0\n  Attempting uninstall: kubernetes\n    Found existing installation: kubernetes 26.1.0\n    Uninstalling kubernetes-26.1.0:\n      Successfully uninstalled kubernetes-26.1.0\n  Attempting uninstall: opentelemetry-exporter-otlp-proto-grpc\n    Found existing installation: opentelemetry-exporter-otlp-proto-grpc 1.19.0\n    Uninstalling opentelemetry-exporter-otlp-proto-grpc-1.19.0:\n      Successfully uninstalled opentelemetry-exporter-otlp-proto-grpc-1.19.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.7 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 23.2 which is incompatible.\ngoogle-cloud-pubsublite 1.8.3 requires overrides<7.0.0,>=6.0.1, but you have overrides 7.4.0 which is incompatible.\njupyterlab 4.0.5 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.0.1 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.0.1 requires jupyterlab<5.0.0a0,>=4.0.6, but you have jupyterlab 4.0.5 which is incompatible.\nkfp 2.0.1 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nkfp 2.0.1 requires kubernetes<27,>=8.0.0, but you have kubernetes 28.1.0 which is incompatible.\nopentelemetry-exporter-otlp 1.19.0 requires opentelemetry-exporter-otlp-proto-grpc==1.19.0, but you have opentelemetry-exporter-otlp-proto-grpc 1.21.0 which is incompatible.\nopentelemetry-exporter-otlp-proto-http 1.19.0 requires opentelemetry-exporter-otlp-proto-common==1.19.0, but you have opentelemetry-exporter-otlp-proto-common 1.21.0 which is incompatible.\nopentelemetry-exporter-otlp-proto-http 1.19.0 requires opentelemetry-proto==1.19.0, but you have opentelemetry-proto 1.21.0 which is incompatible.\nopentelemetry-exporter-otlp-proto-http 1.19.0 requires opentelemetry-sdk~=1.19.0, but you have opentelemetry-sdk 1.21.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed asgiref-3.7.2 bcrypt-4.1.1 chroma-hnswlib-0.7.3 chromadb-0.4.19 coloredlogs-15.0.1 grpcio-1.57.0 humanfriendly-10.0 kubernetes-28.1.0 monotonic-1.6 onnxruntime-1.16.3 opentelemetry-api-1.21.0 opentelemetry-exporter-otlp-proto-common-1.21.0 opentelemetry-exporter-otlp-proto-grpc-1.21.0 opentelemetry-instrumentation-0.42b0 opentelemetry-instrumentation-asgi-0.42b0 opentelemetry-instrumentation-fastapi-0.42b0 opentelemetry-proto-1.21.0 opentelemetry-sdk-1.21.0 opentelemetry-semantic-conventions-0.42b0 opentelemetry-util-http-0.42b0 overrides-7.4.0 posthog-3.1.0 pulsar-client-3.3.0 pypika-0.48.9\n","output_type":"stream"}]},{"cell_type":"code","source":"\nfrom langchain.chains import ConversationChain\npipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=1000,temperature=0.9)\nllm = HuggingFacePipeline(\n       pipeline=pipeline)\nconversation_with_summary = ConversationChain(\n    llm=llm,\n    memory=ConversationSummaryMemory(llm=llm),\n    verbose=True\n)\ncustom_template = \"\"\"You are an Hiberus Marketing Manager AI Assistant. Given the\nfollowing conversation and a follow up question, rephrase the follow up question\nto be a standalone question. At the end of standalone question add this\n'Answer the question in English language.' If you do not know the answer reply with 'I am sorry, I dont have enough information'.\nChat History:\n{chat_history}\nFollow Up Input: {question}\nStandalone question:\n\"\"\"\n\nCUSTOM_QUESTION_PROMPT = PromptTemplate.from_template(custom_template)\n\nmemory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\ndb = Chroma()\nqa_chain = ConversationalRetrievalChain.from_llm(\n    llm=llm,\n    retriever=db.as_retriever(search_kwargs={\"k\": 2}),\n    memory=memory,\n    condense_question_prompt=CUSTOM_QUESTION_PROMPT,\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-14T14:36:04.693457Z","iopub.execute_input":"2023-12-14T14:36:04.694084Z","iopub.status.idle":"2023-12-14T14:36:04.710211Z","shell.execute_reply.started":"2023-12-14T14:36:04.694050Z","shell.execute_reply":"2023-12-14T14:36:04.709328Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"query = \"Who you are?\"\nresult_ = qa_chain({\"question\": query})\nresult = result_[\"answer\"].strip()\n\n# display(Markdown(f\"<b>{query}</b>\"))\n# display(Markdown(f\"<p>{result}</p>\"))","metadata":{"execution":{"iopub.status.busy":"2023-12-14T14:36:49.709076Z","iopub.execute_input":"2023-12-14T14:36:49.709492Z","iopub.status.idle":"2023-12-14T14:36:49.926456Z","shell.execute_reply.started":"2023-12-14T14:36:49.709461Z","shell.execute_reply":"2023-12-14T14:36:49.924884Z"},"trusted":true},"execution_count":46,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[46], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWho you are?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m result_ \u001b[38;5;241m=\u001b[39m \u001b[43mqa_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m result \u001b[38;5;241m=\u001b[39m result_[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# display(Markdown(f\"<b>{query}</b>\"))\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# display(Markdown(f\"<p>{result}</p>\"))\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/chains/base.py:312\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    311\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 312\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    313\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    314\u001b[0m final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    315\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    316\u001b[0m )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/chains/base.py:306\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    299\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[1;32m    300\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    301\u001b[0m     inputs,\n\u001b[1;32m    302\u001b[0m     name\u001b[38;5;241m=\u001b[39mrun_name,\n\u001b[1;32m    303\u001b[0m )\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    305\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 306\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    309\u001b[0m     )\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    311\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/chains/conversational_retrieval/base.py:151\u001b[0m, in \u001b[0;36mBaseConversationalRetrievalChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    147\u001b[0m accepts_run_manager \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_docs)\u001b[38;5;241m.\u001b[39mparameters\n\u001b[1;32m    149\u001b[0m )\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accepts_run_manager:\n\u001b[0;32m--> 151\u001b[0m     docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_docs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_question\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_run_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    153\u001b[0m     docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_docs(new_question, inputs)  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/chains/conversational_retrieval/base.py:313\u001b[0m, in \u001b[0;36mConversationalRetrievalChain._get_docs\u001b[0;34m(self, question, inputs, run_manager)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_docs\u001b[39m(\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    307\u001b[0m     question: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    310\u001b[0m     run_manager: CallbackManagerForChainRun,\n\u001b[1;32m    311\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Document]:\n\u001b[1;32m    312\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get docs.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 313\u001b[0m     docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretriever\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_relevant_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reduce_tokens_below_limit(docs)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_core/retrievers.py:211\u001b[0m, in \u001b[0;36mBaseRetriever.get_relevant_documents\u001b[0;34m(self, query, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    210\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_retriever_error(e)\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    213\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_retriever_end(\n\u001b[1;32m    214\u001b[0m         result,\n\u001b[1;32m    215\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    216\u001b[0m     )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_core/retrievers.py:204\u001b[0m, in \u001b[0;36mBaseRetriever.get_relevant_documents\u001b[0;34m(self, query, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m _kwargs \u001b[38;5;241m=\u001b[39m kwargs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expects_other_args \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_arg_supported:\n\u001b[0;32m--> 204\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_relevant_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_kwargs\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_relevant_documents(query, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_core/vectorstores.py:656\u001b[0m, in \u001b[0;36mVectorStoreRetriever._get_relevant_documents\u001b[0;34m(self, query, run_manager)\u001b[0m\n\u001b[1;32m    652\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_relevant_documents\u001b[39m(\n\u001b[1;32m    653\u001b[0m     \u001b[38;5;28mself\u001b[39m, query: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m, run_manager: CallbackManagerForRetrieverRun\n\u001b[1;32m    654\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Document]:\n\u001b[1;32m    655\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msearch_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimilarity\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 656\u001b[0m         docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvectorstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimilarity_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    657\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msearch_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimilarity_score_threshold\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    658\u001b[0m         docs_and_similarities \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    659\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvectorstore\u001b[38;5;241m.\u001b[39msimilarity_search_with_relevance_scores(\n\u001b[1;32m    660\u001b[0m                 query, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msearch_kwargs\n\u001b[1;32m    661\u001b[0m             )\n\u001b[1;32m    662\u001b[0m         )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_community/vectorstores/chroma.py:348\u001b[0m, in \u001b[0;36mChroma.similarity_search\u001b[0;34m(self, query, k, filter, **kwargs)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msimilarity_search\u001b[39m(\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    333\u001b[0m     query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    337\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Document]:\n\u001b[1;32m    338\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Run similarity search with Chroma.\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \n\u001b[1;32m    340\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;124;03m        List[Document]: List of documents most similar to the query text.\u001b[39;00m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 348\u001b[0m     docs_and_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimilarity_search_with_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [doc \u001b[38;5;28;01mfor\u001b[39;00m doc, _ \u001b[38;5;129;01min\u001b[39;00m docs_and_scores]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_community/vectorstores/chroma.py:425\u001b[0m, in \u001b[0;36mChroma.similarity_search_with_score\u001b[0;34m(self, query, k, filter, where_document, **kwargs)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run similarity search with Chroma with distance.\u001b[39;00m\n\u001b[1;32m    413\u001b[0m \n\u001b[1;32m    414\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;124;03m    Lower score represents more similarity.\u001b[39;00m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embedding_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 425\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__query_collection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_texts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwhere_document\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere_document\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    432\u001b[0m     query_embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embedding_function\u001b[38;5;241m.\u001b[39membed_query(query)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_core/utils/utils.py:35\u001b[0m, in \u001b[0;36mxor_args.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m     invalid_group_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(arg_groups[i]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m invalid_groups]\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExactly one argument in each of the following\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m groups must be defined:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(invalid_group_names)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     34\u001b[0m     )\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_community/vectorstores/chroma.py:155\u001b[0m, in \u001b[0;36mChroma.__query_collection\u001b[0;34m(self, query_texts, query_embeddings, n_results, where, where_document, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    152\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import chromadb python package. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    153\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install it with `pip install chromadb`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    154\u001b[0m     )\n\u001b[0;32m--> 155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_collection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_texts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_texts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_results\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwhere_document\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere_document\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/chromadb/api/models/Collection.py:327\u001b[0m, in \u001b[0;36mCollection.query\u001b[0;34m(self, query_embeddings, query_texts, query_images, query_uris, n_results, where, where_document, include)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m valid_query_embeddings \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m query_texts \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 327\u001b[0m         valid_query_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_query_texts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    328\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m query_images \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    329\u001b[0m         valid_query_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embed(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mvalid_query_images)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/chromadb/api/models/Collection.py:626\u001b[0m, in \u001b[0;36mCollection._embed\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_embed\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Embeddings:\n\u001b[1;32m    625\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embedding_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 626\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    627\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou must provide an embedding function to compute embeddings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    628\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://docs.trychroma.com/embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    629\u001b[0m         )\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embedding_function(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m)\n","\u001b[0;31mValueError\u001b[0m: You must provide an embedding function to compute embeddings.https://docs.trychroma.com/embeddings"],"ename":"ValueError","evalue":"You must provide an embedding function to compute embeddings.https://docs.trychroma.com/embeddings","output_type":"error"}]},{"cell_type":"code","source":"from langchain.chains.conversation.memory import ConversationSummaryMemory\n\nfrom langchain.chains import ConversationChain\nconversation = ConversationChain(\n\tllm=llm,\n\tmemory=ConversationSummaryMemory(llm=llm)\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-14T13:49:49.829560Z","iopub.execute_input":"2023-12-14T13:49:49.829935Z","iopub.status.idle":"2023-12-14T13:49:49.834989Z","shell.execute_reply.started":"2023-12-14T13:49:49.829905Z","shell.execute_reply":"2023-12-14T13:49:49.834151Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# from langchain.llms import OpenAI\nfrom langchain.chains import ConversationChain\n# llm = OpenAI(temperature=0)\nconversation_with_summary = ConversationChain(\n    llm=llm,\n    memory=ConversationSummaryMemory(llm=llm),\n    verbose=True\n)\n# conversation_with_summary.predict(input=\"Hi, what's up?\")\n\n\nfrom langchain.tools import BaseTool\nfrom math import pi\nfrom typing import Union\n  \n\nclass CircumferenceTool(BaseTool):\n    name = \"Circumference calculator\"\n    description = \"use this tool when you need to calculate a circumference using the radius of a circle\"\n\n    def _run(self, radius: Union[int, float]):\n        return float(radius)*2.0*pi\n\n    def _arun(self, radius: int):\n        raise NotImplementedError(\"This tool does not support async\")\n        \nfrom langchain.chains.conversation.memory import ConversationBufferWindowMemory\n\nconversational_memory = ConversationBufferWindowMemory(\n        memory_key='chat_history',\n        k=5,\n        return_messages=True\n)\n\nfrom langchain.agents import initialize_agent\n\ntools = [CircumferenceTool()]\n\n# initialize agent with tools\nagent = initialize_agent(\n    agent='chat-conversational-react-description',\n    tools=tools,\n    llm=llm,\n    verbose=True,\n    max_iterations=3,\n    early_stopping_method='generate',\n    memory=conversational_memory,\n    handle_parsing_errors=True\n\n)\nagent(\"hi\")\n# # logging.set_verbosity(logging.CRITICAL)\n# SYSTEM_PROMPT = \"You are a helpful AI Sentient Mental Therapy Health Companion\"\n# # Run text generation pipeline with our next model\n# conversation_history=[]\n# while True:\n#     prompt = input(\"Human:\")\n#     pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=1000,temperature=0.9)\n#     result = pipe(f\"<s>[INST]<<SYS>>{SYSTEM_PROMPT}<</SYS>> {prompt} [/INST]\")\n#     print(result[0]['generated_text'])","metadata":{"execution":{"iopub.status.busy":"2023-12-14T14:08:48.913935Z","iopub.execute_input":"2023-12-14T14:08:48.914671Z","iopub.status.idle":"2023-12-14T14:11:35.304802Z","shell.execute_reply.started":"2023-12-14T14:08:48.914627Z","shell.execute_reply":"2023-12-14T14:11:35.302949Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"\n\n\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[26], line 51\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# initialize agent with tools\u001b[39;00m\n\u001b[1;32m     40\u001b[0m agent \u001b[38;5;241m=\u001b[39m initialize_agent(\n\u001b[1;32m     41\u001b[0m     agent\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchat-conversational-react-description\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     42\u001b[0m     tools\u001b[38;5;241m=\u001b[39mtools,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m \n\u001b[1;32m     50\u001b[0m )\n\u001b[0;32m---> 51\u001b[0m \u001b[43magent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhi\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# # logging.set_verbosity(logging.CRITICAL)\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# SYSTEM_PROMPT = \"You are a helpful AI Sentient Mental Therapy Health Companion\"\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# # Run text generation pipeline with our next model\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m#     result = pipe(f\"<s>[INST]<<SYS>>{SYSTEM_PROMPT}<</SYS>> {prompt} [/INST]\")\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m#     print(result[0]['generated_text'])\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/chains/base.py:312\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    311\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 312\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    313\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    314\u001b[0m final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    315\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    316\u001b[0m )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/chains/base.py:306\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    299\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[1;32m    300\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    301\u001b[0m     inputs,\n\u001b[1;32m    302\u001b[0m     name\u001b[38;5;241m=\u001b[39mrun_name,\n\u001b[1;32m    303\u001b[0m )\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    305\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 306\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    309\u001b[0m     )\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    311\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/agents/agent.py:1312\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m   1310\u001b[0m \u001b[38;5;66;03m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[1;32m   1311\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_continue(iterations, time_elapsed):\n\u001b[0;32m-> 1312\u001b[0m     next_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_take_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1315\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1318\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[1;32m   1320\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return(\n\u001b[1;32m   1321\u001b[0m             next_step_output, intermediate_steps, run_manager\u001b[38;5;241m=\u001b[39mrun_manager\n\u001b[1;32m   1322\u001b[0m         )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/agents/agent.py:1038\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_take_next_step\u001b[39m(\n\u001b[1;32m   1030\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1031\u001b[0m     name_to_tool_map: Dict[\u001b[38;5;28mstr\u001b[39m, BaseTool],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1035\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1036\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[AgentFinish, List[Tuple[AgentAction, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consume_next_step(\n\u001b[0;32m-> 1038\u001b[0m         [\n\u001b[1;32m   1039\u001b[0m             a\n\u001b[1;32m   1040\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_next_step(\n\u001b[1;32m   1041\u001b[0m                 name_to_tool_map,\n\u001b[1;32m   1042\u001b[0m                 color_mapping,\n\u001b[1;32m   1043\u001b[0m                 inputs,\n\u001b[1;32m   1044\u001b[0m                 intermediate_steps,\n\u001b[1;32m   1045\u001b[0m                 run_manager,\n\u001b[1;32m   1046\u001b[0m             )\n\u001b[1;32m   1047\u001b[0m         ]\n\u001b[1;32m   1048\u001b[0m     )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/agents/agent.py:1038\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_take_next_step\u001b[39m(\n\u001b[1;32m   1030\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1031\u001b[0m     name_to_tool_map: Dict[\u001b[38;5;28mstr\u001b[39m, BaseTool],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1035\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1036\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[AgentFinish, List[Tuple[AgentAction, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consume_next_step(\n\u001b[0;32m-> 1038\u001b[0m         [\n\u001b[1;32m   1039\u001b[0m             a\n\u001b[1;32m   1040\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_next_step(\n\u001b[1;32m   1041\u001b[0m                 name_to_tool_map,\n\u001b[1;32m   1042\u001b[0m                 color_mapping,\n\u001b[1;32m   1043\u001b[0m                 inputs,\n\u001b[1;32m   1044\u001b[0m                 intermediate_steps,\n\u001b[1;32m   1045\u001b[0m                 run_manager,\n\u001b[1;32m   1046\u001b[0m             )\n\u001b[1;32m   1047\u001b[0m         ]\n\u001b[1;32m   1048\u001b[0m     )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/agents/agent.py:1066\u001b[0m, in \u001b[0;36mAgentExecutor._iter_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m   1063\u001b[0m     intermediate_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_intermediate_steps(intermediate_steps)\n\u001b[1;32m   1065\u001b[0m     \u001b[38;5;66;03m# Call the LLM to see what to do.\u001b[39;00m\n\u001b[0;32m-> 1066\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplan\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1067\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1068\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1069\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1070\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OutputParserException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1072\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_parsing_errors, \u001b[38;5;28mbool\u001b[39m):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/agents/agent.py:635\u001b[0m, in \u001b[0;36mAgent.plan\u001b[0;34m(self, intermediate_steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    623\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Given input, decided what to do.\u001b[39;00m\n\u001b[1;32m    624\u001b[0m \n\u001b[1;32m    625\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;124;03m    Action specifying what tool to use.\u001b[39;00m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    634\u001b[0m full_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_full_inputs(intermediate_steps, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 635\u001b[0m full_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfull_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_parser\u001b[38;5;241m.\u001b[39mparse(full_output)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/chains/llm.py:293\u001b[0m, in \u001b[0;36mLLMChain.predict\u001b[0;34m(self, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, callbacks: Callbacks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    279\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Format prompt with kwargs and pass to LLM.\u001b[39;00m\n\u001b[1;32m    280\u001b[0m \n\u001b[1;32m    281\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;124;03m            completion = llm.predict(adjective=\"funny\")\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/chains/base.py:312\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    311\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 312\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    313\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    314\u001b[0m final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    315\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    316\u001b[0m )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/chains/base.py:306\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    299\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[1;32m    300\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    301\u001b[0m     inputs,\n\u001b[1;32m    302\u001b[0m     name\u001b[38;5;241m=\u001b[39mrun_name,\n\u001b[1;32m    303\u001b[0m )\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    305\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 306\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    309\u001b[0m     )\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    311\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/chains/llm.py:103\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call\u001b[39m(\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    100\u001b[0m     inputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[1;32m    101\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    102\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m--> 103\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_outputs(response)[\u001b[38;5;241m0\u001b[39m]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/chains/llm.py:115\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[0;34m(self, input_list, run_manager)\u001b[0m\n\u001b[1;32m    113\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m run_manager\u001b[38;5;241m.\u001b[39mget_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm, BaseLanguageModel):\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    122\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mbind(stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_kwargs)\u001b[38;5;241m.\u001b[39mbatch(\n\u001b[1;32m    123\u001b[0m         cast(List, prompts), {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks}\n\u001b[1;32m    124\u001b[0m     )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_core/language_models/llms.py:516\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    509\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    510\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    514\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    515\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_strings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_core/language_models/llms.py:666\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    650\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    651\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    652\u001b[0m         )\n\u001b[1;32m    653\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    654\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[1;32m    655\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    664\u001b[0m         )\n\u001b[1;32m    665\u001b[0m     ]\n\u001b[0;32m--> 666\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    669\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[1;32m    670\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_core/language_models/llms.py:553\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[1;32m    552\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 553\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    554\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_core/language_models/llms.py:540\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[1;32m    531\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    532\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    537\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    539\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 540\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    542\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[1;32m    544\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    547\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    548\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[1;32m    549\u001b[0m         )\n\u001b[1;32m    550\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    551\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_core/language_models/llms.py:1069\u001b[0m, in \u001b[0;36mLLM._generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1066\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1067\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[1;32m   1068\u001b[0m     text \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 1069\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1070\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m   1071\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1072\u001b[0m     )\n\u001b[1;32m   1073\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([Generation(text\u001b[38;5;241m=\u001b[39mtext)])\n\u001b[1;32m   1074\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_community/llms/ctransformers.py:104\u001b[0m, in \u001b[0;36mCTransformers._call\u001b[0;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m text \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    103\u001b[0m _run_manager \u001b[38;5;241m=\u001b[39m run_manager \u001b[38;5;129;01mor\u001b[39;00m CallbackManagerForLLMRun\u001b[38;5;241m.\u001b[39mget_noop_manager()\n\u001b[0;32m--> 104\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient(prompt, stop\u001b[38;5;241m=\u001b[39mstop, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    105\u001b[0m     text\u001b[38;5;241m.\u001b[39mappend(chunk)\n\u001b[1;32m    106\u001b[0m     _run_manager\u001b[38;5;241m.\u001b[39mon_llm_new_token(chunk, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ctransformers/llm.py:570\u001b[0m, in \u001b[0;36mLLM._stream\u001b[0;34m(self, prompt, max_new_tokens, top_k, top_p, temperature, repetition_penalty, last_n_tokens, seed, batch_size, threads, stop, reset)\u001b[0m\n\u001b[1;32m    568\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m incomplete \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 570\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m    571\u001b[0m     tokens,\n\u001b[1;32m    572\u001b[0m     top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[1;32m    573\u001b[0m     top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[1;32m    574\u001b[0m     temperature\u001b[38;5;241m=\u001b[39mtemperature,\n\u001b[1;32m    575\u001b[0m     repetition_penalty\u001b[38;5;241m=\u001b[39mrepetition_penalty,\n\u001b[1;32m    576\u001b[0m     last_n_tokens\u001b[38;5;241m=\u001b[39mlast_n_tokens,\n\u001b[1;32m    577\u001b[0m     seed\u001b[38;5;241m=\u001b[39mseed,\n\u001b[1;32m    578\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m    579\u001b[0m     threads\u001b[38;5;241m=\u001b[39mthreads,\n\u001b[1;32m    580\u001b[0m     reset\u001b[38;5;241m=\u001b[39mreset,\n\u001b[1;32m    581\u001b[0m ):\n\u001b[1;32m    582\u001b[0m     \u001b[38;5;66;03m# Handle incomplete UTF-8 multi-byte characters.\u001b[39;00m\n\u001b[1;32m    583\u001b[0m     incomplete \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetokenize([token], decode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    584\u001b[0m     complete, incomplete \u001b[38;5;241m=\u001b[39m utf8_split_incomplete(incomplete)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ctransformers/llm.py:527\u001b[0m, in \u001b[0;36mLLM.generate\u001b[0;34m(self, tokens, top_k, top_p, temperature, repetition_penalty, last_n_tokens, seed, batch_size, threads, reset)\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Generates new tokens from a list of tokens.\u001b[39;00m\n\u001b[1;32m    518\u001b[0m \n\u001b[1;32m    519\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[38;5;124;03m    The generated tokens.\u001b[39;00m\n\u001b[1;32m    525\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    526\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(tokens, reset\u001b[38;5;241m=\u001b[39mreset)\n\u001b[0;32m--> 527\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthreads\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    529\u001b[0m     token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample(\n\u001b[1;32m    530\u001b[0m         top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[1;32m    531\u001b[0m         top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    535\u001b[0m         seed\u001b[38;5;241m=\u001b[39mseed,\n\u001b[1;32m    536\u001b[0m     )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ctransformers/llm.py:403\u001b[0m, in \u001b[0;36mLLM.eval\u001b[0;34m(self, tokens, batch_size, threads)\u001b[0m\n\u001b[1;32m    399\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    400\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of tokens (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_past\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39mn_tokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) exceeded maximum context length (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    401\u001b[0m     )\n\u001b[1;32m    402\u001b[0m tokens \u001b[38;5;241m=\u001b[39m (c_int \u001b[38;5;241m*\u001b[39m n_tokens)(\u001b[38;5;241m*\u001b[39mtokens)\n\u001b[0;32m--> 403\u001b[0m status \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctransformers_llm_batch_eval\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthreads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m status:\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to evaluate tokens.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"print(conversation_sum.memory.prompt.template)","metadata":{"execution":{"iopub.status.busy":"2023-12-14T13:50:06.298767Z","iopub.execute_input":"2023-12-14T13:50:06.299633Z","iopub.status.idle":"2023-12-14T13:50:06.355691Z","shell.execute_reply.started":"2023-12-14T13:50:06.299589Z","shell.execute_reply":"2023-12-14T13:50:06.354417Z"},"trusted":true},"execution_count":20,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mconversation_sum\u001b[49m\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39mprompt\u001b[38;5;241m.\u001b[39mtemplate)\n","\u001b[0;31mNameError\u001b[0m: name 'conversation_sum' is not defined"],"ename":"NameError","evalue":"name 'conversation_sum' is not defined","output_type":"error"}]},{"cell_type":"code","source":"# logging.set_verbosity(logging.CRITICAL)\nSYSTEM_PROMPT = \"You are a helpful AI Sentient Mental Therapy Health Companion\"\n# Run text generation pipeline with our next model\nconversation_history=[]\nwhile True:\n    prompt = input(\"Human:\")\n    pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=1000,temperature=0.9)\n    result = pipe(f\"<s>[INST]<<SYS>>{SYSTEM_PROMPT}<</SYS>> {prompt} [/INST]\")\n    print(result[0]['generated_text'])\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name = \"Jaykumaran17/Samantha-Try2-13-12\" \n# dataset_name = \"jerryjalapeno/nart-100k-synthetic\"\n# new_model = \"Samantha-Try2-13-12\"\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,            # load model in 4-bit precision\n    bnb_4bit_quant_type=\"nf4\",    # pre-trained model should be quantized in 4-bit NF format\n    bnb_4bit_use_double_quant=True, # Using double quantization as mentioned in QLoRA paper\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    # During computation, pre-trained model should be loaded in BF16 format\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config = bnb_config,\n    device_map = \"auto\",\n    use_cache=True,\n    trust_remote_code=True\n)\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n\ntokenizer.pad_token = tokenizer.eos_token\n\ntokenizer.padding_side = \"left\"","metadata":{"execution":{"iopub.status.busy":"2023-12-14T14:26:28.637168Z","iopub.execute_input":"2023-12-14T14:26:28.637565Z","iopub.status.idle":"2023-12-14T14:29:36.052529Z","shell.execute_reply.started":"2023-12-14T14:26:28.637532Z","shell.execute_reply":"2023-12-14T14:29:36.050059Z"},"trusted":true},"execution_count":34,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe5e22fcbb3042469e4c40c86690a400"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.89G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c77b7d12545e48edb208a7ba3c6e730c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00002-of-00002.bin:   0%|          | 0.00/5.12G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aaa4939b11aa4cd3913cfc5662c790c1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9312cba17f8f455a8e5f42cedff59a82"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"976d3f2bf5a147cd8d49d56a689596e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/113M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e8a6e37ad8f40dd8260fc9f54c3a222"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.51k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe30536cfda0431891980cbc84e84028"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6395c3ee1cb3408bab08d98399155f12"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/435 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"081359b7895e480bba5b5a2b05792bd8"}},"metadata":{}}]},{"cell_type":"code","source":"base_model=model\nbase_tokenizer=tokenizer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_answer(query):\n SYSTEM_PROMPT = \"You are a helpful empathetic mental therapy '<Assistant>'\"\n# SYSTEM_PROMPT='You are Carl, A Therapist AI'\n\n audio_obj = None\n global conversation_history\n conversation_history.append(f\"<Human>: {query}\")\n\n  # Limit the conversation history to the last 20 queries\n if len(conversation_history) > 10:\n  conversation_history.pop(0)\n user_prompt = f\"\"\"<Human>: {query}\n \n assistant_prompt = \"<Assistant>: \"\"\"\n\n # Construct the final prompt with context cue\n final_prompt = SYSTEM_PROMPT + \"\\n\\n\" + \"\\n\\n\".join(conversation_history) + \"\\n\\n\" + \"<Human>: \" + query + \"\\n\\n<Assistant>: \"\n\n device = \"cuda:0\"\n dashline = \"-\".join(\"\" for i in range(50))\n\n print(dashline)\n base_encoding = base_tokenizer(final_prompt, return_tensors=\"pt\").to(device)\n base_outputs = base_model.generate(input_ids=base_encoding.input_ids, generation_config=GenerationConfig(max_new_tokens=4096, pad_token_id = base_tokenizer.eos_token_id, \\\n                                                           eos_token_id = base_tokenizer.eos_token_id, attention_mask = base_encoding.attention_mask, \\\n                                                           temperature=0.4, top_p=0.6, repetition_penalty=1.3, num_return_sequences=1,))\n#                                                                                                           use_flash_attention_2=True))\n  \n\n base_text_output = base_tokenizer.decode(base_outputs[0], skip_special_tokens=True)\n output_base = base_text_output.split('\\n\\n')\n#  print(output_base)\n print(output_base[-1])  \n text_prompt = f\"\"\"{output_base[-1]}\"\"\"\n \n\n#  audio_array = generate_audio(text_prompt)\n#  audio_out = Audio(audio_array, rate=SAMPLE_RATE)\n# Update the audio object with the new data\n\n \n \n\n conversation_history.append(f\"<Assistant>: {output_base[-1]}\")\n\n# Play the audio\n\n#  print(f'PEFT MODEL RESPONSE:\\n{peft_text_output}')\n print(dashline)\n#  print(display(audio_out))\n \n \n    \n#  return audio_out","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"conversation_history=[]\n# Initialize the audio object outside the loop\n# audio_obj = None\nwhile True:\n    query = input(\"<Human>:\")\n    print(f\"Length of Conversation History in Memory: {len(conversation_history)}\")\n\n    #     print(f\"CONVERSATION HISTORY: {conversation_history}\")\n\n    generate_answer(query)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}