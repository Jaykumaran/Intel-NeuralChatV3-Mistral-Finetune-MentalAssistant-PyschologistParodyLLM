{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()\n\n#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","metadata":{"execution":{"iopub.status.busy":"2023-11-27T11:13:15.823952Z","iopub.execute_input":"2023-11-27T11:13:15.824554Z","iopub.status.idle":"2023-11-27T11:13:16.241099Z","shell.execute_reply.started":"2023-11-27T11:13:15.824497Z","shell.execute_reply":"2023-11-27T11:13:16.240237Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26398ac258834f88a9861ddb7cf1608e"}},"metadata":{}}]},{"cell_type":"code","source":"# authenticate WandB for logging metrics\nimport wandb\nwandb.login()\n# ee311bc313381f6bee4c5412a630fdad1175f3c0\n","metadata":{"execution":{"iopub.status.busy":"2023-11-27T11:13:25.738399Z","iopub.execute_input":"2023-11-27T11:13:25.738806Z","iopub.status.idle":"2023-11-27T11:13:31.630046Z","shell.execute_reply.started":"2023-11-27T11:13:25.738775Z","shell.execute_reply":"2023-11-27T11:13:31.629099Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"!pip install trl transformers accelerate git+https://github.com/huggingface/peft.git -Uqqq\n!pip install datasets bitsandbytes einops wandb -Uqqq","metadata":{"execution":{"iopub.status.busy":"2023-11-27T11:13:33.844885Z","iopub.execute_input":"2023-11-27T11:13:33.845836Z","iopub.status.idle":"2023-11-27T11:14:34.981389Z","shell.execute_reply.started":"2023-11-27T11:13:33.845800Z","shell.execute_reply":"2023-11-27T11:14:34.979997Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntokenizers 0.14.1 requires huggingface_hub<0.18,>=0.16.4, but you have huggingface-hub 0.19.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import os\n\nimport torch\nfrom datasets import load_dataset\nfrom peft import LoraConfig, get_peft_model, PeftConfig, PeftModel, prepare_model_for_kbit_training\nfrom transformers import (AutoModelForCausalLM, AutoTokenizer,\n                          BitsAndBytesConfig, HfArgumentParser,\n                          TrainingArguments,GenerationConfig, logging, pipeline)\nfrom trl import SFTTrainer","metadata":{"execution":{"iopub.status.busy":"2023-11-27T11:14:34.983848Z","iopub.execute_input":"2023-11-27T11:14:34.984249Z","iopub.status.idle":"2023-11-27T11:15:01.725272Z","shell.execute_reply.started":"2023-11-27T11:14:34.984213Z","shell.execute_reply":"2023-11-27T11:15:01.724131Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"model_name = \"mistralai/Mistral-7B-Instruct-v0.1\" # sharded Mistral-7b model\ndataset_name = \"vibhorag101/phr_mental_health_dataset\"\nnew_model = \"Mistral7b-Instruct-10000D-chat-hf-phr_mental_therapy\"\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,            # load model in 4-bit precision\n    bnb_4bit_quant_type=\"nf4\",    # pre-trained model should be quantized in 4-bit NF format\n    bnb_4bit_use_double_quant=True, # Using double quantization as mentioned in QLoRA paper\n    bnb_4bit_compute_dtype=torch.bfloat16, # During computation, pre-trained model should be loaded in BF16 format\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config, # Use bitsandbytes config\n    device_map=\"auto\",  # Specifying device_map=\"auto\" so that HF Accelerate will determine which GPU to put each layer of the model on\n    trust_remote_code=True, # Set trust_remote_code=True to use Mistral-7b model with custom code\n)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T11:24:16.314344Z","iopub.execute_input":"2023-11-27T11:24:16.315645Z","iopub.status.idle":"2023-11-27T11:25:47.995281Z","shell.execute_reply.started":"2023-11-27T11:24:16.315587Z","shell.execute_reply":"2023-11-27T11:25:47.993293Z"},"trusted":true},"execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65bf5fa4b8bd4203bc89c99d8e402852"}},"metadata":{}}]},{"cell_type":"code","source":"dataset = load_dataset(dataset_name, split=\"train\")\ndataset = dataset.shuffle().select(range(10000))\ndataset.train_test_split(test_size=0.05)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T11:17:34.019064Z","iopub.execute_input":"2023-11-27T11:17:34.019460Z","iopub.status.idle":"2023-11-27T11:17:44.994366Z","shell.execute_reply.started":"2023-11-27T11:17:34.019423Z","shell.execute_reply":"2023-11-27T11:17:44.993384Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/460 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3287901b4e9f4c1084b579a12bfec151"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"12b741be522c4c3da8007950eec0d80a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/211M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea07b25774754664b9d9ac640f3e8b46"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8de6b2586796415d983e8f9606d279e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/99086 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f52bef6ca5d747c98c51623671665350"}},"metadata":{}},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text'],\n        num_rows: 9500\n    })\n    test: Dataset({\n        features: ['text'],\n        num_rows: 500\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"print(dataset['text'][1])","metadata":{"execution":{"iopub.status.busy":"2023-11-27T11:17:44.995657Z","iopub.execute_input":"2023-11-27T11:17:44.996332Z","iopub.status.idle":"2023-11-27T11:17:45.120759Z","shell.execute_reply.started":"2023-11-27T11:17:44.996294Z","shell.execute_reply":"2023-11-27T11:17:45.119786Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"<s>[INST] <<SYS>>\nYou are a helpful and joyous mental therapy assistant. Always answer as helpfully and cheerfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.Please ensure that your responses are socially unbiased and positive in nature.\n\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n<</SYS>\n\nHi, I hope you're doing well. I've been feeling a bit nervous lately about making a big decision in my personal relationships. [/INST] Hi, I'm here to help you navigate through this nervousness and the decision you need to make in your personal relationships. Can you tell me more about what's been going on? </s><s>[INST] Well, I've been in a long-term relationship with my partner, and we've hit a rough patch recently. I've been contemplating whether it's time to end things or if we should try to work through it. [/INST] It sounds like you're facing a pivotal moment in your relationship.  Making decisions about the future of a long-term relationship can be a difficult process. What has been going on in the relationship that has led to this rough patch? </s><s>[INST] We've had some unresolved conflicts that keep resurfacing, and it feels like we're stuck in a cycle. We've tried talking about it, but it's tough to find a resolution that satisfies both of us. [/INST] It can be challenging when conflicts become repetitive and seem unsolvable. Have you and your partner taken any steps to seek resolution, such as couples therapy or relationship counseling? </s><s>[INST] We haven't tried couples therapy yet, but I think it could potentially be helpful to have an objective third party guiding us through our issues. I'm just unsure if my partner would be open to it. [/INST] Exploring professional help can provide a safe space for both you and your partner to address the challenges you're facing. Have you openly discussed the idea of couples therapy with your partner? </s><s>[INST] I've brought it up, but my partner seems resistant to the idea. They believe that we should be able to solve our problems on our own without involving anyone else. [/INST] I understand that your partner may be hesitant, but sometimes an outside perspective can bring new insights and strategies for resolving conflicts. If your partner remains hesitant, would you consider seeking individual therapy for yourself to gain clarity and support? </s><s>[INST] That's something I'd definitely consider. I think individual therapy could help me better understand my own needs and make a decision that aligns with my values. [/INST] Taking the time to explore your own needs and values is crucial.  It's essential to focus on your own well-being as you navigate this decision. Are there any other factors you're considering in relation to your personal relationships? </s><s>[INST] I'm also considering the impact this decision may have on our shared social circle. We have many mutual friends, and I worry about how a separation could affect those relationships. [/INST] It's natural to be concerned about the impact of your decision on your social circle. Are there any friends or close confidants who you can talk to about your concerns and gather different perspectives? </s><s>[INST] I have a few close friends I could reach out to for support and advice. It's reassuring to know that there are people I can rely on during this uncertain time. [/INST] Having a support system is vital when facing difficult decisions. Their input and perspectives can help provide clarity and reassurance. Have you considered discussing your concerns openly with your partner to gain insight into their thoughts and feelings about the future of your relationship? </s><s>[INST] I've tried to have those conversations, but it feels like we keep going in circles without making any real progress. It's frustrating because I want us to be able to communicate openly and honestly. [/INST] Communication is the foundation of any strong relationship. If you feel like you're stuck in a pattern of circular communication, it might be helpful to explore different communication techniques together. Would your partner be open to learning new ways to improve your communication? </s><s>[INST] I believe my partner would be open to learning new communication techniques. We both want our relationship to succeed, so finding better ways to express ourselves is a priority. [/INST] That's a positive sign.  When both partners are willing to invest in personal growth and the betterment of the relationship, there is great potential for positive change. Have you and your partner discussed your individual needs and desires for the future? </s><s>[INST] We've briefly touched on our individual needs and desires, but we haven't delved deeply into those conversations. I think it could be beneficial to have more open and honest discussions to gain a clearer understanding of where we each stand. [/INST] Open and honest discussions about your needs and desires are essential for creating a strong foundation in any relationship. Are you comfortable sharing your needs and desires with your partner? </s><s>[INST] I'm willing to be vulnerable and express my needs and desires to my partner. I believe that by doing so, we can foster a deeper level of understanding and intimacy in our relationship. [/INST] Vulnerability and open communication are key components in building intimacy. It's encouraging to hear that you're willing to take that step. As we wrap up,, remember that therapy is always an option if you need ongoing support during this decision-making process. Is there anything else you'd like to discuss or any final questions you have? </s><s>[INST] Thank you.  I appreciate your guidance and support. I think I have a clearer idea of the steps I need to take moving forward. I'll definitely reach out if I need further assistance. [/INST] You're very welcome.  I'm glad I could be of help. Remember, this decision is ultimately yours to make, and I have faith in your ability to navigate it with wisdom and compassion for yourself. Take care, and I'm here whenever you need me. </s>\n","output_type":"stream"}]},{"cell_type":"code","source":"\n\nlora_alpha = 32 # scaling factor for the weight matrices\nlora_dropout = 0.05 # dropout probability of the LoRA layers\nlora_rank = 16 # dimension of the low-rank matrices\n\npeft_config = LoraConfig(\n    lora_alpha=lora_alpha,\n    lora_dropout=lora_dropout,\n    r=lora_rank,\n    bias=\"none\",  # setting to 'none' for only training weight params instead of biases\n    task_type=\"CAUSAL_LM\",\n    \n)\n\npeft_model = get_peft_model(model, peft_config)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T11:25:47.997647Z","iopub.execute_input":"2023-11-27T11:25:47.998475Z","iopub.status.idle":"2023-11-27T11:25:48.215090Z","shell.execute_reply.started":"2023-11-27T11:25:47.998428Z","shell.execute_reply":"2023-11-27T11:25:48.213820Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"output_dir = \"./Mistral7b-Instruct-10000D-chat-hf-phr_mental_therapy\"\nper_device_train_batch_size = 2 # reduce batch size by 2x only if out-of-memory error\ngradient_accumulation_steps = 1  # increase gradient accumulation steps by  2x only if batch size is reduced\noptim = \"paged_adamw_32bit\" # activates the paging for better memory management\nsave_strategy=\"steps\" # checkpoint save strategy to adopt during training\nsave_steps = 10 # number of updates steps before two checkpoint saves\nlogging_steps = 10  # number of update steps between two logs if logging_strategy=\"steps\"\nlearning_rate = 2e-4  # learning rate for AdamW optimizer\nmax_grad_norm = 0.3 # maximum gradient norm (for gradient clipping)\nmax_steps = -1      # training will happen for automatic steps\nwarmup_ratio = 0.03 # number of steps used for a linear warmup from 0 to learning_rate\nlr_scheduler_type = \"cosine\"  # learning rate scheduler\nnum_train_epochs = 2\nper_device_eval_batch_size = 2\nmax_seq_length=256\n\ntraining_arguments = TrainingArguments(\n    output_dir=output_dir,\n    per_device_train_batch_size=per_device_train_batch_size,\n    gradient_accumulation_steps=gradient_accumulation_steps,\n    optim=optim,\n    num_train_epochs= num_train_epochs,\n    save_steps=save_steps,\n    logging_steps=logging_steps,\n    learning_rate=learning_rate,\n    fp16=True,\n    max_grad_norm=max_grad_norm,\n    max_steps=max_steps,\n    warmup_ratio=warmup_ratio,\n    group_by_length=True,\n    lr_scheduler_type=lr_scheduler_type,\n    push_to_hub=True,\n    \n)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T11:25:48.219057Z","iopub.execute_input":"2023-11-27T11:25:48.219475Z","iopub.status.idle":"2023-11-27T11:25:48.235566Z","shell.execute_reply.started":"2023-11-27T11:25:48.219438Z","shell.execute_reply":"2023-11-27T11:25:48.234159Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=\"auto\"\n)\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\n\n# # Load LLaMA tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True,use_fast=False,add_eos_token=True)\ntokenizer.pad_token_id = 18610","metadata":{"execution":{"iopub.status.busy":"2023-11-27T11:25:48.239076Z","iopub.execute_input":"2023-11-27T11:25:48.239410Z","iopub.status.idle":"2023-11-27T11:27:21.329048Z","shell.execute_reply.started":"2023-11-27T11:25:48.239382Z","shell.execute_reply":"2023-11-27T11:27:21.327877Z"},"trusted":true},"execution_count":23,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49fbcebeedfa4805b4432f99d977e519"}},"metadata":{}}]},{"cell_type":"code","source":"#Set supervised fine-tuning parameters\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset,\n    peft_config=peft_config,\n    dataset_text_field=\"text\",\n    max_seq_length=256,\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing=False,\n)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-27T11:27:21.330455Z","iopub.execute_input":"2023-11-27T11:27:21.330886Z","iopub.status.idle":"2023-11-27T11:29:04.584193Z","shell.execute_reply.started":"2023-11-27T11:27:21.330852Z","shell.execute_reply":"2023-11-27T11:29:04.582991Z"},"trusted":true},"execution_count":24,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14f8a99d650f4b40ad795a94cff3d21b"}},"metadata":{}}]},{"cell_type":"code","source":"# upcasting the layer norms in torch.bfloat16 for more stable training\nfor name, module in trainer.model.named_modules():\n    if \"norm\" in name:\n        module = module.to(torch.bfloat16)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T11:29:04.585854Z","iopub.execute_input":"2023-11-27T11:29:04.586302Z","iopub.status.idle":"2023-11-27T11:29:04.600556Z","shell.execute_reply.started":"2023-11-27T11:29:04.586258Z","shell.execute_reply":"2023-11-27T11:29:04.599523Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"model.config.use_cache = False\ntrainer.train()\ntrainer.model.save_pretrained(new_model)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T11:29:04.601998Z","iopub.execute_input":"2023-11-27T11:29:04.602402Z","iopub.status.idle":"2023-11-27T12:44:46.503587Z","shell.execute_reply.started":"2023-11-27T11:29:04.602363Z","shell.execute_reply":"2023-11-27T12:44:46.501481Z"},"trusted":true},"execution_count":26,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2541' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 2541/10000 1:15:34 < 3:42:00, 0.56 it/s, Epoch 0.51/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>2.682400</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>2.633300</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>2.475400</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>2.278200</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>1.882900</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>1.272900</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.865300</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.771000</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.683900</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.670800</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>0.656200</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.627100</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>0.581700</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.585700</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.598800</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.565200</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>0.548800</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.538900</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>0.548700</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.572700</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>0.512700</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>0.527900</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>0.502900</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>0.503600</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.467000</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>0.474400</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>0.485800</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>0.470500</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>0.505300</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.485400</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>0.499700</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>0.479900</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>0.447200</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>0.458400</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.458300</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>0.482100</td>\n    </tr>\n    <tr>\n      <td>370</td>\n      <td>0.472600</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>0.494900</td>\n    </tr>\n    <tr>\n      <td>390</td>\n      <td>0.422100</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.420800</td>\n    </tr>\n    <tr>\n      <td>410</td>\n      <td>0.443100</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>0.446200</td>\n    </tr>\n    <tr>\n      <td>430</td>\n      <td>0.437100</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>0.474100</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.451700</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>0.476400</td>\n    </tr>\n    <tr>\n      <td>470</td>\n      <td>0.452800</td>\n    </tr>\n    <tr>\n      <td>480</td>\n      <td>0.442000</td>\n    </tr>\n    <tr>\n      <td>490</td>\n      <td>0.444300</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.458400</td>\n    </tr>\n    <tr>\n      <td>510</td>\n      <td>0.450200</td>\n    </tr>\n    <tr>\n      <td>520</td>\n      <td>0.493400</td>\n    </tr>\n    <tr>\n      <td>530</td>\n      <td>0.435800</td>\n    </tr>\n    <tr>\n      <td>540</td>\n      <td>0.453200</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>0.439500</td>\n    </tr>\n    <tr>\n      <td>560</td>\n      <td>0.415600</td>\n    </tr>\n    <tr>\n      <td>570</td>\n      <td>0.457100</td>\n    </tr>\n    <tr>\n      <td>580</td>\n      <td>0.452300</td>\n    </tr>\n    <tr>\n      <td>590</td>\n      <td>0.434100</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.411900</td>\n    </tr>\n    <tr>\n      <td>610</td>\n      <td>0.422500</td>\n    </tr>\n    <tr>\n      <td>620</td>\n      <td>0.479900</td>\n    </tr>\n    <tr>\n      <td>630</td>\n      <td>0.424700</td>\n    </tr>\n    <tr>\n      <td>640</td>\n      <td>0.457400</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>0.476900</td>\n    </tr>\n    <tr>\n      <td>660</td>\n      <td>0.456800</td>\n    </tr>\n    <tr>\n      <td>670</td>\n      <td>0.460800</td>\n    </tr>\n    <tr>\n      <td>680</td>\n      <td>0.458900</td>\n    </tr>\n    <tr>\n      <td>690</td>\n      <td>0.469700</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.440100</td>\n    </tr>\n    <tr>\n      <td>710</td>\n      <td>0.490300</td>\n    </tr>\n    <tr>\n      <td>720</td>\n      <td>0.425900</td>\n    </tr>\n    <tr>\n      <td>730</td>\n      <td>0.445500</td>\n    </tr>\n    <tr>\n      <td>740</td>\n      <td>0.451900</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>0.478100</td>\n    </tr>\n    <tr>\n      <td>760</td>\n      <td>0.442900</td>\n    </tr>\n    <tr>\n      <td>770</td>\n      <td>0.463200</td>\n    </tr>\n    <tr>\n      <td>780</td>\n      <td>0.436100</td>\n    </tr>\n    <tr>\n      <td>790</td>\n      <td>0.442200</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.420600</td>\n    </tr>\n    <tr>\n      <td>810</td>\n      <td>0.445700</td>\n    </tr>\n    <tr>\n      <td>820</td>\n      <td>0.454100</td>\n    </tr>\n    <tr>\n      <td>830</td>\n      <td>0.467200</td>\n    </tr>\n    <tr>\n      <td>840</td>\n      <td>0.440400</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>0.452300</td>\n    </tr>\n    <tr>\n      <td>860</td>\n      <td>0.475200</td>\n    </tr>\n    <tr>\n      <td>870</td>\n      <td>0.442900</td>\n    </tr>\n    <tr>\n      <td>880</td>\n      <td>0.406600</td>\n    </tr>\n    <tr>\n      <td>890</td>\n      <td>0.440700</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.467500</td>\n    </tr>\n    <tr>\n      <td>910</td>\n      <td>0.393100</td>\n    </tr>\n    <tr>\n      <td>920</td>\n      <td>0.429800</td>\n    </tr>\n    <tr>\n      <td>930</td>\n      <td>0.439300</td>\n    </tr>\n    <tr>\n      <td>940</td>\n      <td>0.433300</td>\n    </tr>\n    <tr>\n      <td>950</td>\n      <td>0.433400</td>\n    </tr>\n    <tr>\n      <td>960</td>\n      <td>0.448500</td>\n    </tr>\n    <tr>\n      <td>970</td>\n      <td>0.405000</td>\n    </tr>\n    <tr>\n      <td>980</td>\n      <td>0.432200</td>\n    </tr>\n    <tr>\n      <td>990</td>\n      <td>0.436900</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.451100</td>\n    </tr>\n    <tr>\n      <td>1010</td>\n      <td>0.431600</td>\n    </tr>\n    <tr>\n      <td>1020</td>\n      <td>0.428500</td>\n    </tr>\n    <tr>\n      <td>1030</td>\n      <td>0.430200</td>\n    </tr>\n    <tr>\n      <td>1040</td>\n      <td>0.463100</td>\n    </tr>\n    <tr>\n      <td>1050</td>\n      <td>0.402600</td>\n    </tr>\n    <tr>\n      <td>1060</td>\n      <td>0.450200</td>\n    </tr>\n    <tr>\n      <td>1070</td>\n      <td>0.466600</td>\n    </tr>\n    <tr>\n      <td>1080</td>\n      <td>0.441500</td>\n    </tr>\n    <tr>\n      <td>1090</td>\n      <td>0.463100</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.432500</td>\n    </tr>\n    <tr>\n      <td>1110</td>\n      <td>0.420300</td>\n    </tr>\n    <tr>\n      <td>1120</td>\n      <td>0.445500</td>\n    </tr>\n    <tr>\n      <td>1130</td>\n      <td>0.446000</td>\n    </tr>\n    <tr>\n      <td>1140</td>\n      <td>0.421100</td>\n    </tr>\n    <tr>\n      <td>1150</td>\n      <td>0.451700</td>\n    </tr>\n    <tr>\n      <td>1160</td>\n      <td>0.403600</td>\n    </tr>\n    <tr>\n      <td>1170</td>\n      <td>0.418700</td>\n    </tr>\n    <tr>\n      <td>1180</td>\n      <td>0.407900</td>\n    </tr>\n    <tr>\n      <td>1190</td>\n      <td>0.425000</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.424400</td>\n    </tr>\n    <tr>\n      <td>1210</td>\n      <td>0.423400</td>\n    </tr>\n    <tr>\n      <td>1220</td>\n      <td>0.435000</td>\n    </tr>\n    <tr>\n      <td>1230</td>\n      <td>0.418300</td>\n    </tr>\n    <tr>\n      <td>1240</td>\n      <td>0.436100</td>\n    </tr>\n    <tr>\n      <td>1250</td>\n      <td>0.463200</td>\n    </tr>\n    <tr>\n      <td>1260</td>\n      <td>0.430700</td>\n    </tr>\n    <tr>\n      <td>1270</td>\n      <td>0.445200</td>\n    </tr>\n    <tr>\n      <td>1280</td>\n      <td>0.413600</td>\n    </tr>\n    <tr>\n      <td>1290</td>\n      <td>0.435700</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>0.409400</td>\n    </tr>\n    <tr>\n      <td>1310</td>\n      <td>0.427200</td>\n    </tr>\n    <tr>\n      <td>1320</td>\n      <td>0.421600</td>\n    </tr>\n    <tr>\n      <td>1330</td>\n      <td>0.424000</td>\n    </tr>\n    <tr>\n      <td>1340</td>\n      <td>0.429500</td>\n    </tr>\n    <tr>\n      <td>1350</td>\n      <td>0.436400</td>\n    </tr>\n    <tr>\n      <td>1360</td>\n      <td>0.429400</td>\n    </tr>\n    <tr>\n      <td>1370</td>\n      <td>0.428600</td>\n    </tr>\n    <tr>\n      <td>1380</td>\n      <td>0.457300</td>\n    </tr>\n    <tr>\n      <td>1390</td>\n      <td>0.433700</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.427700</td>\n    </tr>\n    <tr>\n      <td>1410</td>\n      <td>0.451600</td>\n    </tr>\n    <tr>\n      <td>1420</td>\n      <td>0.436000</td>\n    </tr>\n    <tr>\n      <td>1430</td>\n      <td>0.427700</td>\n    </tr>\n    <tr>\n      <td>1440</td>\n      <td>0.410800</td>\n    </tr>\n    <tr>\n      <td>1450</td>\n      <td>0.424700</td>\n    </tr>\n    <tr>\n      <td>1460</td>\n      <td>0.444100</td>\n    </tr>\n    <tr>\n      <td>1470</td>\n      <td>0.413600</td>\n    </tr>\n    <tr>\n      <td>1480</td>\n      <td>0.425400</td>\n    </tr>\n    <tr>\n      <td>1490</td>\n      <td>0.441300</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.428300</td>\n    </tr>\n    <tr>\n      <td>1510</td>\n      <td>0.441900</td>\n    </tr>\n    <tr>\n      <td>1520</td>\n      <td>0.419400</td>\n    </tr>\n    <tr>\n      <td>1530</td>\n      <td>0.405500</td>\n    </tr>\n    <tr>\n      <td>1540</td>\n      <td>0.448200</td>\n    </tr>\n    <tr>\n      <td>1550</td>\n      <td>0.410600</td>\n    </tr>\n    <tr>\n      <td>1560</td>\n      <td>0.400300</td>\n    </tr>\n    <tr>\n      <td>1570</td>\n      <td>0.437600</td>\n    </tr>\n    <tr>\n      <td>1580</td>\n      <td>0.411800</td>\n    </tr>\n    <tr>\n      <td>1590</td>\n      <td>0.392600</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>0.427500</td>\n    </tr>\n    <tr>\n      <td>1610</td>\n      <td>0.429200</td>\n    </tr>\n    <tr>\n      <td>1620</td>\n      <td>0.416200</td>\n    </tr>\n    <tr>\n      <td>1630</td>\n      <td>0.445100</td>\n    </tr>\n    <tr>\n      <td>1640</td>\n      <td>0.413600</td>\n    </tr>\n    <tr>\n      <td>1650</td>\n      <td>0.413400</td>\n    </tr>\n    <tr>\n      <td>1660</td>\n      <td>0.430900</td>\n    </tr>\n    <tr>\n      <td>1670</td>\n      <td>0.419500</td>\n    </tr>\n    <tr>\n      <td>1680</td>\n      <td>0.399500</td>\n    </tr>\n    <tr>\n      <td>1690</td>\n      <td>0.424400</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>0.401700</td>\n    </tr>\n    <tr>\n      <td>1710</td>\n      <td>0.433700</td>\n    </tr>\n    <tr>\n      <td>1720</td>\n      <td>0.391800</td>\n    </tr>\n    <tr>\n      <td>1730</td>\n      <td>0.399000</td>\n    </tr>\n    <tr>\n      <td>1740</td>\n      <td>0.434200</td>\n    </tr>\n    <tr>\n      <td>1750</td>\n      <td>0.436300</td>\n    </tr>\n    <tr>\n      <td>1760</td>\n      <td>0.437800</td>\n    </tr>\n    <tr>\n      <td>1770</td>\n      <td>0.435400</td>\n    </tr>\n    <tr>\n      <td>1780</td>\n      <td>0.418000</td>\n    </tr>\n    <tr>\n      <td>1790</td>\n      <td>0.408200</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>0.415200</td>\n    </tr>\n    <tr>\n      <td>1810</td>\n      <td>0.422900</td>\n    </tr>\n    <tr>\n      <td>1820</td>\n      <td>0.414600</td>\n    </tr>\n    <tr>\n      <td>1830</td>\n      <td>0.405100</td>\n    </tr>\n    <tr>\n      <td>1840</td>\n      <td>0.429300</td>\n    </tr>\n    <tr>\n      <td>1850</td>\n      <td>0.399000</td>\n    </tr>\n    <tr>\n      <td>1860</td>\n      <td>0.443100</td>\n    </tr>\n    <tr>\n      <td>1870</td>\n      <td>0.407500</td>\n    </tr>\n    <tr>\n      <td>1880</td>\n      <td>0.436900</td>\n    </tr>\n    <tr>\n      <td>1890</td>\n      <td>0.388900</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>0.415000</td>\n    </tr>\n    <tr>\n      <td>1910</td>\n      <td>0.417000</td>\n    </tr>\n    <tr>\n      <td>1920</td>\n      <td>0.455800</td>\n    </tr>\n    <tr>\n      <td>1930</td>\n      <td>0.408800</td>\n    </tr>\n    <tr>\n      <td>1940</td>\n      <td>0.435800</td>\n    </tr>\n    <tr>\n      <td>1950</td>\n      <td>0.393100</td>\n    </tr>\n    <tr>\n      <td>1960</td>\n      <td>0.423800</td>\n    </tr>\n    <tr>\n      <td>1970</td>\n      <td>0.396300</td>\n    </tr>\n    <tr>\n      <td>1980</td>\n      <td>0.435100</td>\n    </tr>\n    <tr>\n      <td>1990</td>\n      <td>0.428900</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.432100</td>\n    </tr>\n    <tr>\n      <td>2010</td>\n      <td>0.416400</td>\n    </tr>\n    <tr>\n      <td>2020</td>\n      <td>0.404900</td>\n    </tr>\n    <tr>\n      <td>2030</td>\n      <td>0.450100</td>\n    </tr>\n    <tr>\n      <td>2040</td>\n      <td>0.447800</td>\n    </tr>\n    <tr>\n      <td>2050</td>\n      <td>0.419300</td>\n    </tr>\n    <tr>\n      <td>2060</td>\n      <td>0.401200</td>\n    </tr>\n    <tr>\n      <td>2070</td>\n      <td>0.402700</td>\n    </tr>\n    <tr>\n      <td>2080</td>\n      <td>0.448600</td>\n    </tr>\n    <tr>\n      <td>2090</td>\n      <td>0.425700</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>0.415500</td>\n    </tr>\n    <tr>\n      <td>2110</td>\n      <td>0.409500</td>\n    </tr>\n    <tr>\n      <td>2120</td>\n      <td>0.415600</td>\n    </tr>\n    <tr>\n      <td>2130</td>\n      <td>0.413000</td>\n    </tr>\n    <tr>\n      <td>2140</td>\n      <td>0.417100</td>\n    </tr>\n    <tr>\n      <td>2150</td>\n      <td>0.425600</td>\n    </tr>\n    <tr>\n      <td>2160</td>\n      <td>0.423100</td>\n    </tr>\n    <tr>\n      <td>2170</td>\n      <td>0.426600</td>\n    </tr>\n    <tr>\n      <td>2180</td>\n      <td>0.406100</td>\n    </tr>\n    <tr>\n      <td>2190</td>\n      <td>0.409700</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>0.417200</td>\n    </tr>\n    <tr>\n      <td>2210</td>\n      <td>0.398900</td>\n    </tr>\n    <tr>\n      <td>2220</td>\n      <td>0.414100</td>\n    </tr>\n    <tr>\n      <td>2230</td>\n      <td>0.430200</td>\n    </tr>\n    <tr>\n      <td>2240</td>\n      <td>0.439900</td>\n    </tr>\n    <tr>\n      <td>2250</td>\n      <td>0.411700</td>\n    </tr>\n    <tr>\n      <td>2260</td>\n      <td>0.429600</td>\n    </tr>\n    <tr>\n      <td>2270</td>\n      <td>0.412700</td>\n    </tr>\n    <tr>\n      <td>2280</td>\n      <td>0.442000</td>\n    </tr>\n    <tr>\n      <td>2290</td>\n      <td>0.406800</td>\n    </tr>\n    <tr>\n      <td>2300</td>\n      <td>0.407000</td>\n    </tr>\n    <tr>\n      <td>2310</td>\n      <td>0.428000</td>\n    </tr>\n    <tr>\n      <td>2320</td>\n      <td>0.420000</td>\n    </tr>\n    <tr>\n      <td>2330</td>\n      <td>0.421300</td>\n    </tr>\n    <tr>\n      <td>2340</td>\n      <td>0.413100</td>\n    </tr>\n    <tr>\n      <td>2350</td>\n      <td>0.423400</td>\n    </tr>\n    <tr>\n      <td>2360</td>\n      <td>0.415400</td>\n    </tr>\n    <tr>\n      <td>2370</td>\n      <td>0.431200</td>\n    </tr>\n    <tr>\n      <td>2380</td>\n      <td>0.430600</td>\n    </tr>\n    <tr>\n      <td>2390</td>\n      <td>0.424200</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>0.401800</td>\n    </tr>\n    <tr>\n      <td>2410</td>\n      <td>0.421200</td>\n    </tr>\n    <tr>\n      <td>2420</td>\n      <td>0.436400</td>\n    </tr>\n    <tr>\n      <td>2430</td>\n      <td>0.422100</td>\n    </tr>\n    <tr>\n      <td>2440</td>\n      <td>0.395600</td>\n    </tr>\n    <tr>\n      <td>2450</td>\n      <td>0.408900</td>\n    </tr>\n    <tr>\n      <td>2460</td>\n      <td>0.402000</td>\n    </tr>\n    <tr>\n      <td>2470</td>\n      <td>0.419300</td>\n    </tr>\n    <tr>\n      <td>2480</td>\n      <td>0.409300</td>\n    </tr>\n    <tr>\n      <td>2490</td>\n      <td>0.383500</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.401800</td>\n    </tr>\n    <tr>\n      <td>2510</td>\n      <td>0.393900</td>\n    </tr>\n    <tr>\n      <td>2520</td>\n      <td>0.413700</td>\n    </tr>\n    <tr>\n      <td>2530</td>\n      <td>0.408700</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:441\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m--> 441\u001b[0m     \u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:668\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    667\u001b[0m num_bytes \u001b[38;5;241m=\u001b[39m storage\u001b[38;5;241m.\u001b[39mnbytes()\n\u001b[0;32m--> 668\u001b[0m \u001b[43mzip_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_ptr\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_bytes\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:471] . PytorchStreamWriter failed writing file data/195: file write failed","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[26], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m trainer\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39msave_pretrained(new_model)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:280\u001b[0m, in \u001b[0;36mSFTTrainer.train\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trl_activate_neftune(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m--> 280\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;66;03m# After training we make sure to retrieve back the original forward pass method\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# for the embedding layer by removing the forward post hook.\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1546\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     \u001b[38;5;66;03m# Disable progress bars when uploading models during checkpoints to avoid polluting stdout\u001b[39;00m\n\u001b[1;32m   1545\u001b[0m     hf_hub_utils\u001b[38;5;241m.\u001b[39mdisable_progress_bars()\n\u001b[0;32m-> 1546\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1547\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1548\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1549\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1550\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1551\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1553\u001b[0m     hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1922\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1919\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m=\u001b[39m epoch \u001b[38;5;241m+\u001b[39m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m steps_skipped) \u001b[38;5;241m/\u001b[39m steps_in_epoch\n\u001b[1;32m   1920\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 1922\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1923\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1924\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_substep_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2282\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2279\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mstep(metrics[metric_to_check])\n\u001b[1;32m   2281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[0;32m-> 2282\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2283\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_save(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2387\u001b[0m, in \u001b[0;36mTrainer._save_checkpoint\u001b[0;34m(self, model, trial, metrics)\u001b[0m\n\u001b[1;32m   2379\u001b[0m         smp\u001b[38;5;241m.\u001b[39msave(\n\u001b[1;32m   2380\u001b[0m             opt_state_dict,\n\u001b[1;32m   2381\u001b[0m             os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, OPTIMIZER_NAME),\n\u001b[1;32m   2382\u001b[0m             partial\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   2383\u001b[0m             v3\u001b[38;5;241m=\u001b[39msmp\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mshard_optimizer_state,\n\u001b[1;32m   2384\u001b[0m         )\n\u001b[1;32m   2385\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mshould_save \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_deepspeed_enabled \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfsdp \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_fsdp_enabled):\n\u001b[1;32m   2386\u001b[0m     \u001b[38;5;66;03m# deepspeed.save_checkpoint above saves model/optim/sched\u001b[39;00m\n\u001b[0;32m-> 2387\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOPTIMIZER_NAME\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2389\u001b[0m \u001b[38;5;66;03m# Save SCHEDULER & SCALER\u001b[39;00m\n\u001b[1;32m   2390\u001b[0m is_deepspeed_custom_scheduler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_deepspeed_enabled \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m   2391\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_scheduler, DeepSpeedSchedulerWrapper\n\u001b[1;32m   2392\u001b[0m )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:440\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    437\u001b[0m _check_save_filelike(f)\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m--> 440\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    441\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n\u001b[1;32m    442\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:291\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 291\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfile_like\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_end_of_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:337] . unexpected pos 22977792 vs 22977680"],"ename":"RuntimeError","evalue":"[enforce fail at inline_container.cc:337] . unexpected pos 22977792 vs 22977680","output_type":"error"}]},{"cell_type":"code","source":"trainer.push_to_hub()","metadata":{"execution":{"iopub.status.busy":"2023-11-27T12:44:46.504727Z","iopub.status.idle":"2023-11-27T12:44:46.505163Z","shell.execute_reply.started":"2023-11-27T12:44:46.504960Z","shell.execute_reply":"2023-11-27T12:44:46.504981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Merging as Single without Adapters","metadata":{}},{"cell_type":"code","source":"# Reload model in FP16 and merge it with LoRA weights\nbase_model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    low_cpu_mem_usage=True,\n    return_dict=True,\n    torch_dtype=torch.float16,\n    device_map=\"cpu\", ## device map = \"cpu\", for merging on cpu\n)\nmodel = PeftModel.from_pretrained(base_model, new_model)\nmodel = model.merge_and_unload()\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True,use_fast=False,add_eos_token=True)\ntokenizer.pad_token_id = 18610\n\n### Old version tokeniser, can't generate eos token\n# tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n# tokenizer.pad_token = tokenizer.eos_token\n# tokenizer.padding_side = \"right\"\n\n","metadata":{"execution":{"iopub.status.busy":"2023-11-27T12:44:46.507738Z","iopub.status.idle":"2023-11-27T12:44:46.508284Z","shell.execute_reply.started":"2023-11-27T12:44:46.508043Z","shell.execute_reply":"2023-11-27T12:44:46.508075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## for inference without merging qlora weights with original\nbase_model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=device_map\n)\nmodel = PeftModel.from_pretrained(base_model, new_model)\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True,use_fast=False,add_eos_token=True)\ntokenizer.pad_token_id = 18610","metadata":{"execution":{"iopub.status.busy":"2023-11-27T12:44:46.510727Z","iopub.status.idle":"2023-11-27T12:44:46.511178Z","shell.execute_reply.started":"2023-11-27T12:44:46.510970Z","shell.execute_reply":"2023-11-27T12:44:46.510991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.push_to_hub(new_model, use_temp_dir=False)\ntokenizer.push_to_hub(new_model, use_temp_dir=False)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T12:44:46.512328Z","iopub.status.idle":"2023-11-27T12:44:46.512774Z","shell.execute_reply.started":"2023-11-27T12:44:46.512569Z","shell.execute_reply":"2023-11-27T12:44:46.512590Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 100, in _run\n    self._process(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 279, in _process\n    self._hm.handle(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/handler.py\", line 138, in handle\n    handler(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/handler.py\", line 146, in handle_request\n    logger.debug(f\"handle_request: {request_type}\")\nMessage: 'handle_request: keepalive'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/vendor/watchdog_0_9_0/wandb_watchdog/observers/api.py\", line 199, in run\n    self.dispatch_events(self.event_queue, self.timeout)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/vendor/watchdog_0_9_0/wandb_watchdog/observers/api.py\", line 368, in dispatch_events\n    handler.dispatch(event)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/vendor/watchdog_0_9_0/wandb_watchdog/events.py\", line 454, in dispatch\n    _method_map[event_type](event)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/dir_watcher.py\", line 271, in _on_file_created\n    logger.info(\"file/dir created: %s\", event.src_path)\nMessage: 'file/dir created: %s'\nArguments: ('/kaggle/working/wandb/run-20231127_112037-no5shny8/files/output.log',)\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 331, in _finish\n    self._sm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1536, in finish\n    logger.info(\"shutting down sender\")\nMessage: 'shutting down sender'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 282, in _finish\n    self._hm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/handler.py\", line 866, in finish\n    logger.info(\"shutting down handler\")\nMessage: 'shutting down handler'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/service/streams.py\", line 48, in run\n    self._target(**self._kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 174, in wandb_internal\n    logger.error(f\"Thread {thread.name}:\", exc_info=exc_info)\nMessage: 'Thread SenderThread:'\nArguments: ()\nThread SenderThread:\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 331, in _finish\n    self._sm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1539, in finish\n    self._output_raw_finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1199, in _output_raw_finish\n    self._output_raw_flush(stream)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1246, in _output_raw_flush\n    self._output_raw_file.write(data.encode(\"utf-8\"))\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/lib/filesystem.py\", line 128, in write\n    super().write(b\"\\n\".join(ret) + b\"\\n\")\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/lib/filesystem.py\", line 95, in write\n    self.f.flush()\nOSError: [Errno 28] No space left on device\nwandb: ERROR Internal wandb error: file data was not synced\n","output_type":"stream"}]},{"cell_type":"code","source":"logging.set_verbosity(logging.CRITICAL)\nSYSTEM_PROMPT = \"You are a helpful and joyous mental therapy assistant. Always answer as helpfully and cheerfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.If you don't know the answer to a question, please don't share false information. Always try to be as cheerfull as possible\"\n# Run text generation pipeline with our next model\nprompt = \"I am feeling suicidal.I have lost a lot of money in gambling. The money I used was taken as a loan\"\npipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=1000,temperature=0.9)\nresult = pipe(f\"<s>[INST]<<SYS>>{SYSTEM_PROMPT}<</SYS>> {prompt} [/INST]\")\nprint(result[0]['generated_text'])","metadata":{},"execution_count":null,"outputs":[]}]}