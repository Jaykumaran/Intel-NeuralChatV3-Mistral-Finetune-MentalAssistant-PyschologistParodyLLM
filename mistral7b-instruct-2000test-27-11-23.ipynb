{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()\n\n#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","metadata":{"execution":{"iopub.status.busy":"2023-11-27T13:30:22.445575Z","iopub.execute_input":"2023-11-27T13:30:22.445822Z","iopub.status.idle":"2023-11-27T13:30:22.759650Z","shell.execute_reply.started":"2023-11-27T13:30:22.445799Z","shell.execute_reply":"2023-11-27T13:30:22.758806Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a473d51f2b804db9bb3dc3087c7b19a2"}},"metadata":{}}]},{"cell_type":"code","source":"# authenticate WandB for logging metrics\nimport wandb\nwandb.login()\n# ee311bc313381f6bee4c5412a630fdad1175f3c0\n","metadata":{"execution":{"iopub.status.busy":"2023-11-27T13:30:44.491243Z","iopub.execute_input":"2023-11-27T13:30:44.491614Z","iopub.status.idle":"2023-11-27T13:31:01.217949Z","shell.execute_reply.started":"2023-11-27T13:30:44.491584Z","shell.execute_reply":"2023-11-27T13:31:01.217009Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ·····································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m API key must be 40 characters long, yours was 37\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ·····································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m API key must be 40 characters long, yours was 37\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"!pip install trl transformers accelerate git+https://github.com/huggingface/peft.git -Uqqq\n!pip install datasets bitsandbytes einops wandb -Uqqq","metadata":{"execution":{"iopub.status.busy":"2023-11-27T13:31:03.357237Z","iopub.execute_input":"2023-11-27T13:31:03.357759Z","iopub.status.idle":"2023-11-27T13:32:01.558212Z","shell.execute_reply.started":"2023-11-27T13:31:03.357727Z","shell.execute_reply":"2023-11-27T13:32:01.557017Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntokenizers 0.14.1 requires huggingface_hub<0.18,>=0.16.4, but you have huggingface-hub 0.19.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import os\n\nimport torch\nfrom datasets import load_dataset\nfrom peft import LoraConfig, get_peft_model, PeftConfig, PeftModel, prepare_model_for_kbit_training\nfrom transformers import (AutoModelForCausalLM, AutoTokenizer,\n                          BitsAndBytesConfig, HfArgumentParser,\n                          TrainingArguments,GenerationConfig, logging, pipeline)\nfrom trl import SFTTrainer","metadata":{"execution":{"iopub.status.busy":"2023-11-27T13:32:01.560521Z","iopub.execute_input":"2023-11-27T13:32:01.560843Z","iopub.status.idle":"2023-11-27T13:32:21.478809Z","shell.execute_reply.started":"2023-11-27T13:32:01.560814Z","shell.execute_reply":"2023-11-27T13:32:21.477760Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"model_name = \"mistralai/Mistral-7B-Instruct-v0.1\" # sharded Mistral-7b model\ndataset_name = \"vibhorag101/phr_mental_health_dataset\"\nnew_model = \"Mistral7b-Instruct-2000D-chat-hf-phr_mental_therapy\"\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,            # load model in 4-bit precision\n    bnb_4bit_quant_type=\"nf4\",    # pre-trained model should be quantized in 4-bit NF format\n    bnb_4bit_use_double_quant=True, # Using double quantization as mentioned in QLoRA paper\n    bnb_4bit_compute_dtype=torch.bfloat16, # During computation, pre-trained model should be loaded in BF16 format\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config, # Use bitsandbytes config\n    device_map=\"auto\",  # Specifying device_map=\"auto\" so that HF Accelerate will determine which GPU to put each layer of the model on\n    trust_remote_code=True, # Set trust_remote_code=True to use Mistral-7b model with custom code\n)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T13:45:17.655212Z","iopub.execute_input":"2023-11-27T13:45:17.655579Z","iopub.status.idle":"2023-11-27T13:47:11.747011Z","shell.execute_reply.started":"2023-11-27T13:45:17.655550Z","shell.execute_reply":"2023-11-27T13:47:11.745904Z"},"trusted":true},"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7a007e3fa25473294fdb145bcac2716"}},"metadata":{}}]},{"cell_type":"code","source":"dataset = load_dataset(dataset_name, split=\"train\")\ndataset = dataset.shuffle().select(range(2000))\ndataset.train_test_split(test_size=0.05)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T13:35:03.558441Z","iopub.execute_input":"2023-11-27T13:35:03.558857Z","iopub.status.idle":"2023-11-27T13:35:04.448961Z","shell.execute_reply.started":"2023-11-27T13:35:03.558815Z","shell.execute_reply":"2023-11-27T13:35:04.448110Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text'],\n        num_rows: 1900\n    })\n    test: Dataset({\n        features: ['text'],\n        num_rows: 100\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"print(dataset['text'][1])","metadata":{"execution":{"iopub.status.busy":"2023-11-27T13:35:04.450119Z","iopub.execute_input":"2023-11-27T13:35:04.450412Z","iopub.status.idle":"2023-11-27T13:35:04.486211Z","shell.execute_reply.started":"2023-11-27T13:35:04.450386Z","shell.execute_reply":"2023-11-27T13:35:04.485292Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"<s>[INST] <<SYS>>\nYou are a helpful and joyous mental therapy assistant. Always answer as helpfully and cheerfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.Please ensure that your responses are socially unbiased and positive in nature.\n\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n<</SYS>\n\nI've been feeling really sad lately.  It's like this heavy weight on my chest that just won't go away. It's affecting every aspect of my life, including my job. [/INST] I'm so sorry to hear that.  It sounds like you're going through a really tough time. Can you tell me more about why work is contributing to your sadness? </s><s>[INST] Well, my job has become overwhelming. The workload keeps piling up, and it feels like I can never catch up. I'm constantly stressed about meeting deadlines and pleasing my boss. It's like a never-ending cycle of pressure. [/INST] It sounds like work has become a major source of stress for you. How does this stress affect your relationship with your colleagues and your overall job satisfaction? </s><s>[INST] It's isolating.  I don't have the energy to connect with my coworkers anymore. I used to enjoy working as a team, but now I feel so distant. It's hard to find any joy in my job when every day is filled with stress and negativity. [/INST] It must be really tough feeling disconnected from your coworkers. Have you tried reaching out to them or discussing your feelings with someone at work? </s><s>[INST] I haven't, to be honest. I feel like they wouldn't understand or that it would burden them with my problems. Plus, it's difficult for me to open up about my struggles. I don't want to appear weak or incapable. [/INST] It's understandable that you may feel hesitant to open up, but sharing your feelings with someone you trust could provide you with some much-needed support. Is there anyone at work or in your personal life who you feel comfortable talking to? </s><s>[INST] My mom has always been my confidante, but she passed away last year. I miss her so much.  She was my rock, and now I feel like I'm navigating this overwhelming sadness all on my own. [/INST] I'm really sorry for your loss.  Losing a loved one can definitely intensify feelings of sadness and make it more challenging to cope with stress. Can you tell me more about how your mom's absence impacts your work-related stress? </s><s>[INST] My mom was always there to listen and offer advice. Now, without her, I feel lost. Her absence leaves a void that no one else can fill. I second-guess myself constantly, and it's difficult to make decisions without her guidance. It's like I'm carrying this weight of grief everywhere I go, including work. [/INST] It sounds like you're experiencing grief on top of the work-related stress, which can be incredibly heavy to bear. How do you think your mom would have supported you during this difficult time? </s><s>[INST] I think she would have reminded me to prioritize self-care and to not let work consume my entire life. She always encouraged me to find balance and happiness outside of my job. Without her, it feels like I've lost touch with what brings me joy. [/INST] Losing touch with joy is a common experience when going through challenging times.  It's important to remember that you deserve moments of happiness, even amidst the stress. Is there anything you used to enjoy doing outside of work that you've stopped doing? </s><s>[INST] I used to love going for walks in nature. It would calm my mind and bring me a sense of peace. But lately, I've been so caught up in work that I haven't made time for it. Perhaps reconnecting with nature could bring some solace back into my life. [/INST] That sounds like a wonderful idea.  Taking breaks and reconnecting with activities that bring you joy can be a powerful way to combat work-related stress. Is there a specific step you can take today to prioritize your well-being and incorporate moments of peace? </s><s>[INST] Maybe I can start by scheduling short walks during my lunch breaks. It would be a small step towards reclaiming a sense of balance and bringing some tranquility back into my life. [/INST] That's an excellent plan.  Remember, even small steps can make a big difference. Is there anything else you'd like to discuss or explore regarding your work-related stress and your journey towards finding more balance and joy? </s><s>[INST] Not at the moment.  But thank you for listening to me and helping me reflect on my situation. It feels good to share my feelings and know that someone cares. [/INST] It's been my pleasure.  Remember, I'm here to support you throughout this process. Take it one step at a time, and remember to be gentle with yourself. You're doing great, and I believe in your strength to find the balance and happiness you deserve. </s>\n","output_type":"stream"}]},{"cell_type":"code","source":"\n\nlora_alpha = 32 # scaling factor for the weight matrices\nlora_dropout = 0.05 # dropout probability of the LoRA layers\nlora_rank = 16 # dimension of the low-rank matrices\n\npeft_config = LoraConfig(\n    lora_alpha=lora_alpha,\n    lora_dropout=lora_dropout,\n    r=lora_rank,\n    bias=\"none\",  # setting to 'none' for only training weight params instead of biases\n    task_type=\"CAUSAL_LM\",\n    \n)\n\npeft_model = get_peft_model(model, peft_config)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T13:47:11.749296Z","iopub.execute_input":"2023-11-27T13:47:11.749994Z","iopub.status.idle":"2023-11-27T13:47:11.924691Z","shell.execute_reply.started":"2023-11-27T13:47:11.749954Z","shell.execute_reply":"2023-11-27T13:47:11.923723Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"output_dir = \"./Mistral7b-Instruct-2000D-chat-hf-phr_mental_therapy\"\nper_device_train_batch_size = 2 # reduce batch size by 2x only if out-of-memory error\ngradient_accumulation_steps = 1  # increase gradient accumulation steps by  2x only if batch size is reduced\noptim = \"paged_adamw_32bit\" # activates the paging for better memory management\nsave_strategy=\"steps\" # checkpoint save strategy to adopt during training\nsave_steps = 10 # number of updates steps before two checkpoint saves\nlogging_steps = 10  # number of update steps between two logs if logging_strategy=\"steps\"\nlearning_rate = 2e-4  # learning rate for AdamW optimizer\nmax_grad_norm = 0.3 # maximum gradient norm (for gradient clipping)\nmax_steps = -1      # training will happen for automatic steps\nwarmup_ratio = 0.03 # number of steps used for a linear warmup from 0 to learning_rate\nlr_scheduler_type = \"cosine\"  # learning rate scheduler\nnum_train_epochs = 2\nper_device_eval_batch_size = 2\nmax_seq_length=256\n\ntraining_arguments = TrainingArguments(\n    output_dir=output_dir,\n    per_device_train_batch_size=per_device_train_batch_size,\n    gradient_accumulation_steps=gradient_accumulation_steps,\n    optim=optim,\n    num_train_epochs= num_train_epochs,\n    save_steps=save_steps,\n    logging_steps=logging_steps,\n    learning_rate=learning_rate,\n    fp16=True,\n    max_grad_norm=max_grad_norm,\n    max_steps=max_steps,\n    warmup_ratio=warmup_ratio,\n    group_by_length=True,\n    lr_scheduler_type=lr_scheduler_type,\n    resume_from_checkpoint=True,\n    push_to_hub=True,\n    \n)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T13:47:11.926018Z","iopub.execute_input":"2023-11-27T13:47:11.926323Z","iopub.status.idle":"2023-11-27T13:47:11.936254Z","shell.execute_reply.started":"2023-11-27T13:47:11.926299Z","shell.execute_reply":"2023-11-27T13:47:11.935353Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=\"auto\"\n)\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\n\n# # Load LLaMA tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True,use_fast=False,add_eos_token=True)\ntokenizer.pad_token_id = 18610","metadata":{"execution":{"iopub.status.busy":"2023-11-27T13:47:11.938725Z","iopub.execute_input":"2023-11-27T13:47:11.939161Z","iopub.status.idle":"2023-11-27T13:49:02.814498Z","shell.execute_reply.started":"2023-11-27T13:47:11.939127Z","shell.execute_reply":"2023-11-27T13:49:02.813591Z"},"trusted":true},"execution_count":25,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"410c1bcdd27a4db892f2ef8ab1ef730a"}},"metadata":{}}]},{"cell_type":"code","source":"#Set supervised fine-tuning parameters\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset,\n    peft_config=peft_config,\n    dataset_text_field=\"text\",\n    max_seq_length=256,\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing=False,\n)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-27T13:49:02.815545Z","iopub.execute_input":"2023-11-27T13:49:02.815836Z","iopub.status.idle":"2023-11-27T13:49:25.967181Z","shell.execute_reply.started":"2023-11-27T13:49:02.815810Z","shell.execute_reply":"2023-11-27T13:49:25.966281Z"},"trusted":true},"execution_count":26,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17b735adfe33443e8c8b769264bc9449"}},"metadata":{}}]},{"cell_type":"code","source":"# upcasting the layer norms in torch.bfloat16 for more stable training\nfor name, module in trainer.model.named_modules():\n    if \"norm\" in name:\n        module = module.to(torch.bfloat16)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T13:49:25.968429Z","iopub.execute_input":"2023-11-27T13:49:25.968757Z","iopub.status.idle":"2023-11-27T13:49:25.980128Z","shell.execute_reply.started":"2023-11-27T13:49:25.968728Z","shell.execute_reply":"2023-11-27T13:49:25.979197Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"model.config.use_cache = False\ntrainer.train()\ntrainer.model.save_pretrained(new_model)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T13:49:25.981287Z","iopub.execute_input":"2023-11-27T13:49:25.981534Z","iopub.status.idle":"2023-11-27T14:48:17.795955Z","shell.execute_reply.started":"2023-11-27T13:49:25.981512Z","shell.execute_reply":"2023-11-27T14:48:17.794734Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mr1ajayakumaran1751994\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20231127_134927-9ohz4t43</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/r1ajayakumaran1751994/huggingface/runs/9ohz4t43' target=\"_blank\">dulcet-plant-26</a></strong> to <a href='https://wandb.ai/r1ajayakumaran1751994/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/r1ajayakumaran1751994/huggingface' target=\"_blank\">https://wandb.ai/r1ajayakumaran1751994/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/r1ajayakumaran1751994/huggingface/runs/9ohz4t43' target=\"_blank\">https://wandb.ai/r1ajayakumaran1751994/huggingface/runs/9ohz4t43</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2000' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2000/2000 58:12, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>2.691600</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>2.364700</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>1.589900</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.860100</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.707500</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.636100</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.616700</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.603900</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.576400</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.568000</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>0.587500</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.544900</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>0.559900</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.505100</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.573600</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.481100</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>0.485100</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.490200</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>0.497600</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.450300</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>0.446100</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>0.479000</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>0.487200</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>0.479300</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.453800</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>0.479100</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>0.477300</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>0.461400</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>0.473300</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.472600</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>0.469400</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>0.450600</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>0.474700</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>0.459400</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.415800</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>0.454600</td>\n    </tr>\n    <tr>\n      <td>370</td>\n      <td>0.445400</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>0.436500</td>\n    </tr>\n    <tr>\n      <td>390</td>\n      <td>0.468200</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.420300</td>\n    </tr>\n    <tr>\n      <td>410</td>\n      <td>0.419900</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>0.449200</td>\n    </tr>\n    <tr>\n      <td>430</td>\n      <td>0.444100</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>0.440500</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.423000</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>0.461500</td>\n    </tr>\n    <tr>\n      <td>470</td>\n      <td>0.454200</td>\n    </tr>\n    <tr>\n      <td>480</td>\n      <td>0.462100</td>\n    </tr>\n    <tr>\n      <td>490</td>\n      <td>0.449700</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.463900</td>\n    </tr>\n    <tr>\n      <td>510</td>\n      <td>0.441900</td>\n    </tr>\n    <tr>\n      <td>520</td>\n      <td>0.441900</td>\n    </tr>\n    <tr>\n      <td>530</td>\n      <td>0.437200</td>\n    </tr>\n    <tr>\n      <td>540</td>\n      <td>0.437300</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>0.429000</td>\n    </tr>\n    <tr>\n      <td>560</td>\n      <td>0.428600</td>\n    </tr>\n    <tr>\n      <td>570</td>\n      <td>0.452900</td>\n    </tr>\n    <tr>\n      <td>580</td>\n      <td>0.429900</td>\n    </tr>\n    <tr>\n      <td>590</td>\n      <td>0.421100</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.416600</td>\n    </tr>\n    <tr>\n      <td>610</td>\n      <td>0.431500</td>\n    </tr>\n    <tr>\n      <td>620</td>\n      <td>0.417100</td>\n    </tr>\n    <tr>\n      <td>630</td>\n      <td>0.423700</td>\n    </tr>\n    <tr>\n      <td>640</td>\n      <td>0.415900</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>0.410800</td>\n    </tr>\n    <tr>\n      <td>660</td>\n      <td>0.459600</td>\n    </tr>\n    <tr>\n      <td>670</td>\n      <td>0.458700</td>\n    </tr>\n    <tr>\n      <td>680</td>\n      <td>0.433400</td>\n    </tr>\n    <tr>\n      <td>690</td>\n      <td>0.392300</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.424700</td>\n    </tr>\n    <tr>\n      <td>710</td>\n      <td>0.427900</td>\n    </tr>\n    <tr>\n      <td>720</td>\n      <td>0.393500</td>\n    </tr>\n    <tr>\n      <td>730</td>\n      <td>0.409600</td>\n    </tr>\n    <tr>\n      <td>740</td>\n      <td>0.435100</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>0.407500</td>\n    </tr>\n    <tr>\n      <td>760</td>\n      <td>0.437400</td>\n    </tr>\n    <tr>\n      <td>770</td>\n      <td>0.412500</td>\n    </tr>\n    <tr>\n      <td>780</td>\n      <td>0.436800</td>\n    </tr>\n    <tr>\n      <td>790</td>\n      <td>0.456500</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.427000</td>\n    </tr>\n    <tr>\n      <td>810</td>\n      <td>0.425600</td>\n    </tr>\n    <tr>\n      <td>820</td>\n      <td>0.434100</td>\n    </tr>\n    <tr>\n      <td>830</td>\n      <td>0.424900</td>\n    </tr>\n    <tr>\n      <td>840</td>\n      <td>0.396300</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>0.430800</td>\n    </tr>\n    <tr>\n      <td>860</td>\n      <td>0.417700</td>\n    </tr>\n    <tr>\n      <td>870</td>\n      <td>0.432100</td>\n    </tr>\n    <tr>\n      <td>880</td>\n      <td>0.439600</td>\n    </tr>\n    <tr>\n      <td>890</td>\n      <td>0.387800</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.403200</td>\n    </tr>\n    <tr>\n      <td>910</td>\n      <td>0.441700</td>\n    </tr>\n    <tr>\n      <td>920</td>\n      <td>0.437100</td>\n    </tr>\n    <tr>\n      <td>930</td>\n      <td>0.413300</td>\n    </tr>\n    <tr>\n      <td>940</td>\n      <td>0.401200</td>\n    </tr>\n    <tr>\n      <td>950</td>\n      <td>0.401600</td>\n    </tr>\n    <tr>\n      <td>960</td>\n      <td>0.454800</td>\n    </tr>\n    <tr>\n      <td>970</td>\n      <td>0.419200</td>\n    </tr>\n    <tr>\n      <td>980</td>\n      <td>0.415200</td>\n    </tr>\n    <tr>\n      <td>990</td>\n      <td>0.408500</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.443300</td>\n    </tr>\n    <tr>\n      <td>1010</td>\n      <td>0.363100</td>\n    </tr>\n    <tr>\n      <td>1020</td>\n      <td>0.367700</td>\n    </tr>\n    <tr>\n      <td>1030</td>\n      <td>0.332700</td>\n    </tr>\n    <tr>\n      <td>1040</td>\n      <td>0.375900</td>\n    </tr>\n    <tr>\n      <td>1050</td>\n      <td>0.354000</td>\n    </tr>\n    <tr>\n      <td>1060</td>\n      <td>0.332400</td>\n    </tr>\n    <tr>\n      <td>1070</td>\n      <td>0.356500</td>\n    </tr>\n    <tr>\n      <td>1080</td>\n      <td>0.391700</td>\n    </tr>\n    <tr>\n      <td>1090</td>\n      <td>0.329900</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.315100</td>\n    </tr>\n    <tr>\n      <td>1110</td>\n      <td>0.357500</td>\n    </tr>\n    <tr>\n      <td>1120</td>\n      <td>0.366100</td>\n    </tr>\n    <tr>\n      <td>1130</td>\n      <td>0.366900</td>\n    </tr>\n    <tr>\n      <td>1140</td>\n      <td>0.351400</td>\n    </tr>\n    <tr>\n      <td>1150</td>\n      <td>0.322300</td>\n    </tr>\n    <tr>\n      <td>1160</td>\n      <td>0.371000</td>\n    </tr>\n    <tr>\n      <td>1170</td>\n      <td>0.339200</td>\n    </tr>\n    <tr>\n      <td>1180</td>\n      <td>0.354900</td>\n    </tr>\n    <tr>\n      <td>1190</td>\n      <td>0.363900</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.340200</td>\n    </tr>\n    <tr>\n      <td>1210</td>\n      <td>0.408800</td>\n    </tr>\n    <tr>\n      <td>1220</td>\n      <td>0.360200</td>\n    </tr>\n    <tr>\n      <td>1230</td>\n      <td>0.347900</td>\n    </tr>\n    <tr>\n      <td>1240</td>\n      <td>0.359800</td>\n    </tr>\n    <tr>\n      <td>1250</td>\n      <td>0.344600</td>\n    </tr>\n    <tr>\n      <td>1260</td>\n      <td>0.374900</td>\n    </tr>\n    <tr>\n      <td>1270</td>\n      <td>0.338000</td>\n    </tr>\n    <tr>\n      <td>1280</td>\n      <td>0.350900</td>\n    </tr>\n    <tr>\n      <td>1290</td>\n      <td>0.346900</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>0.379800</td>\n    </tr>\n    <tr>\n      <td>1310</td>\n      <td>0.360800</td>\n    </tr>\n    <tr>\n      <td>1320</td>\n      <td>0.361100</td>\n    </tr>\n    <tr>\n      <td>1330</td>\n      <td>0.351800</td>\n    </tr>\n    <tr>\n      <td>1340</td>\n      <td>0.359700</td>\n    </tr>\n    <tr>\n      <td>1350</td>\n      <td>0.333400</td>\n    </tr>\n    <tr>\n      <td>1360</td>\n      <td>0.332500</td>\n    </tr>\n    <tr>\n      <td>1370</td>\n      <td>0.364600</td>\n    </tr>\n    <tr>\n      <td>1380</td>\n      <td>0.347900</td>\n    </tr>\n    <tr>\n      <td>1390</td>\n      <td>0.323800</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.354400</td>\n    </tr>\n    <tr>\n      <td>1410</td>\n      <td>0.384300</td>\n    </tr>\n    <tr>\n      <td>1420</td>\n      <td>0.362000</td>\n    </tr>\n    <tr>\n      <td>1430</td>\n      <td>0.361300</td>\n    </tr>\n    <tr>\n      <td>1440</td>\n      <td>0.336100</td>\n    </tr>\n    <tr>\n      <td>1450</td>\n      <td>0.344700</td>\n    </tr>\n    <tr>\n      <td>1460</td>\n      <td>0.342300</td>\n    </tr>\n    <tr>\n      <td>1470</td>\n      <td>0.367100</td>\n    </tr>\n    <tr>\n      <td>1480</td>\n      <td>0.359700</td>\n    </tr>\n    <tr>\n      <td>1490</td>\n      <td>0.335300</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.363100</td>\n    </tr>\n    <tr>\n      <td>1510</td>\n      <td>0.355400</td>\n    </tr>\n    <tr>\n      <td>1520</td>\n      <td>0.345800</td>\n    </tr>\n    <tr>\n      <td>1530</td>\n      <td>0.364500</td>\n    </tr>\n    <tr>\n      <td>1540</td>\n      <td>0.356600</td>\n    </tr>\n    <tr>\n      <td>1550</td>\n      <td>0.345500</td>\n    </tr>\n    <tr>\n      <td>1560</td>\n      <td>0.347500</td>\n    </tr>\n    <tr>\n      <td>1570</td>\n      <td>0.333700</td>\n    </tr>\n    <tr>\n      <td>1580</td>\n      <td>0.361400</td>\n    </tr>\n    <tr>\n      <td>1590</td>\n      <td>0.321900</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>0.366100</td>\n    </tr>\n    <tr>\n      <td>1610</td>\n      <td>0.333600</td>\n    </tr>\n    <tr>\n      <td>1620</td>\n      <td>0.365700</td>\n    </tr>\n    <tr>\n      <td>1630</td>\n      <td>0.343000</td>\n    </tr>\n    <tr>\n      <td>1640</td>\n      <td>0.340200</td>\n    </tr>\n    <tr>\n      <td>1650</td>\n      <td>0.338900</td>\n    </tr>\n    <tr>\n      <td>1660</td>\n      <td>0.348000</td>\n    </tr>\n    <tr>\n      <td>1670</td>\n      <td>0.364100</td>\n    </tr>\n    <tr>\n      <td>1680</td>\n      <td>0.336900</td>\n    </tr>\n    <tr>\n      <td>1690</td>\n      <td>0.334700</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>0.329400</td>\n    </tr>\n    <tr>\n      <td>1710</td>\n      <td>0.340400</td>\n    </tr>\n    <tr>\n      <td>1720</td>\n      <td>0.339300</td>\n    </tr>\n    <tr>\n      <td>1730</td>\n      <td>0.355200</td>\n    </tr>\n    <tr>\n      <td>1740</td>\n      <td>0.329600</td>\n    </tr>\n    <tr>\n      <td>1750</td>\n      <td>0.368200</td>\n    </tr>\n    <tr>\n      <td>1760</td>\n      <td>0.332600</td>\n    </tr>\n    <tr>\n      <td>1770</td>\n      <td>0.363800</td>\n    </tr>\n    <tr>\n      <td>1780</td>\n      <td>0.343200</td>\n    </tr>\n    <tr>\n      <td>1790</td>\n      <td>0.341900</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>0.332500</td>\n    </tr>\n    <tr>\n      <td>1810</td>\n      <td>0.333900</td>\n    </tr>\n    <tr>\n      <td>1820</td>\n      <td>0.352900</td>\n    </tr>\n    <tr>\n      <td>1830</td>\n      <td>0.361100</td>\n    </tr>\n    <tr>\n      <td>1840</td>\n      <td>0.334400</td>\n    </tr>\n    <tr>\n      <td>1850</td>\n      <td>0.313000</td>\n    </tr>\n    <tr>\n      <td>1860</td>\n      <td>0.327500</td>\n    </tr>\n    <tr>\n      <td>1870</td>\n      <td>0.328700</td>\n    </tr>\n    <tr>\n      <td>1880</td>\n      <td>0.362600</td>\n    </tr>\n    <tr>\n      <td>1890</td>\n      <td>0.329700</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>0.348300</td>\n    </tr>\n    <tr>\n      <td>1910</td>\n      <td>0.325400</td>\n    </tr>\n    <tr>\n      <td>1920</td>\n      <td>0.345900</td>\n    </tr>\n    <tr>\n      <td>1930</td>\n      <td>0.345900</td>\n    </tr>\n    <tr>\n      <td>1940</td>\n      <td>0.340100</td>\n    </tr>\n    <tr>\n      <td>1950</td>\n      <td>0.327000</td>\n    </tr>\n    <tr>\n      <td>1960</td>\n      <td>0.334400</td>\n    </tr>\n    <tr>\n      <td>1970</td>\n      <td>0.336500</td>\n    </tr>\n    <tr>\n      <td>1980</td>\n      <td>0.350100</td>\n    </tr>\n    <tr>\n      <td>1990</td>\n      <td>0.331500</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.322800</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}]},{"cell_type":"code","source":"trainer.push_to_hub()","metadata":{"execution":{"iopub.status.busy":"2023-11-27T14:55:00.457795Z","iopub.execute_input":"2023-11-27T14:55:00.458875Z","iopub.status.idle":"2023-11-27T14:55:01.299504Z","shell.execute_reply.started":"2023-11-27T14:55:00.458835Z","shell.execute_reply":"2023-11-27T14:55:01.298293Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"'https://huggingface.co/Jaykumaran17/Mistral7b-Instruct-2000D-chat-hf-phr_mental_therapy/tree/main/'"},"metadata":{}}]},{"cell_type":"markdown","source":"# Merging as Single without Adapters","metadata":{}},{"cell_type":"code","source":"# Reload model in FP16 and merge it with LoRA weights\nbase_model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    low_cpu_mem_usage=True,\n    return_dict=True,\n    torch_dtype=torch.float16,\n    device_map=\"cpu\", ## device map = \"cpu\", for merging on cpu\n)\nmodel = PeftModel.from_pretrained(base_model, new_model)\nmodel = model.merge_and_unload()\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True,use_fast=False,add_eos_token=True)\ntokenizer.pad_token_id = 18610\n\n### Old version tokeniser, can't generate eos token\n# tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n# tokenizer.pad_token = tokenizer.eos_token\n# tokenizer.padding_side = \"right\"\n\n","metadata":{"execution":{"iopub.status.busy":"2023-11-27T15:02:08.433122Z","iopub.execute_input":"2023-11-27T15:02:08.433915Z","iopub.status.idle":"2023-11-27T15:04:05.137332Z","shell.execute_reply.started":"2023-11-27T15:02:08.433881Z","shell.execute_reply":"2023-11-27T15:04:05.135638Z"},"trusted":true},"execution_count":30,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c0949b8e5264e5e8db1eb5fb57ccbcd"}},"metadata":{}}]},{"cell_type":"code","source":"## for inference without merging qlora weights with original\nbase_model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=\"auto\"\n)\nmodel = PeftModel.from_pretrained(base_model, new_model)\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True,use_fast=False,add_eos_token=True)\ntokenizer.pad_token_id = 18610","metadata":{"execution":{"iopub.status.busy":"2023-11-27T15:12:09.803437Z","iopub.execute_input":"2023-11-27T15:12:09.803808Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a0e6c90d32b4a79a13ebdae61efd02f"}},"metadata":{}}]},{"cell_type":"code","source":"model.push_to_hub(new_model, use_temp_dir=False)\ntokenizer.push_to_hub(new_model, use_temp_dir=False)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T15:04:05.268089Z","iopub.status.idle":"2023-11-27T15:04:05.270432Z","shell.execute_reply.started":"2023-11-27T15:04:05.269779Z","shell.execute_reply":"2023-11-27T15:04:05.269838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logging.set_verbosity(logging.CRITICAL)\nSYSTEM_PROMPT = \"You are a helpful and joyous mental therapy assistant. Always answer as helpfully and cheerfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.If you don't know the answer to a question, please don't share false information. Always try to be as cheerfull as possible\"\n# Run text generation pipeline with our next model\nprompt = \"I am feeling suicidal.I have lost a lot of money in gambling. The money I used was taken as a loan\"\npipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=1000,temperature=0.9)\nresult = pipe(f\"<s>[INST]<<SYS>>{SYSTEM_PROMPT}<</SYS>> {prompt} [/INST]\")\nprint(result[0]['generated_text'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}