{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()\n\n#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","metadata":{"execution":{"iopub.status.busy":"2023-11-27T15:12:57.444657Z","iopub.execute_input":"2023-11-27T15:12:57.444917Z","iopub.status.idle":"2023-11-27T15:12:57.776870Z","shell.execute_reply.started":"2023-11-27T15:12:57.444893Z","shell.execute_reply":"2023-11-27T15:12:57.775958Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7d82ba1c5b143dfb180478075310f28"}},"metadata":{}}]},{"cell_type":"code","source":"# authenticate WandB for logging metrics\nimport wandb\nwandb.login()\n# ee311bc313381f6bee4c5412a630fdad1175f3c0\n","metadata":{"execution":{"iopub.status.busy":"2023-11-27T15:14:19.021588Z","iopub.execute_input":"2023-11-27T15:14:19.022254Z","iopub.status.idle":"2023-11-27T15:14:28.087686Z","shell.execute_reply.started":"2023-11-27T15:14:19.022220Z","shell.execute_reply":"2023-11-27T15:14:28.086744Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"!pip install trl transformers accelerate git+https://github.com/huggingface/peft.git -Uqqq\n!pip install datasets bitsandbytes einops wandb -Uqqq","metadata":{"execution":{"iopub.status.busy":"2023-11-27T15:14:29.966075Z","iopub.execute_input":"2023-11-27T15:14:29.966585Z","iopub.status.idle":"2023-11-27T15:15:28.911055Z","shell.execute_reply.started":"2023-11-27T15:14:29.966555Z","shell.execute_reply":"2023-11-27T15:15:28.910036Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntokenizers 0.14.1 requires huggingface_hub<0.18,>=0.16.4, but you have huggingface-hub 0.19.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import os\n\nimport torch\nfrom datasets import load_dataset\nfrom peft import LoraConfig, get_peft_model, PeftConfig, PeftModel, prepare_model_for_kbit_training\nfrom transformers import (AutoModelForCausalLM, AutoTokenizer,\n                          BitsAndBytesConfig, HfArgumentParser,\n                          TrainingArguments,GenerationConfig, logging, pipeline)\nfrom trl import SFTTrainer","metadata":{"execution":{"iopub.status.busy":"2023-11-27T15:15:28.912930Z","iopub.execute_input":"2023-11-27T15:15:28.913250Z","iopub.status.idle":"2023-11-27T15:15:49.333087Z","shell.execute_reply.started":"2023-11-27T15:15:28.913221Z","shell.execute_reply":"2023-11-27T15:15:49.332094Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"model_name = \"ehartford/samantha-1.2-mistral-7b\" # sharded Mistral-7b model\ndataset_name = \"vibhorag101/phr_mental_health_dataset\"\nnew_model = \"Samantha-2000D-chat-hf-phr_mental_therapy\"\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,            # load model in 4-bit precision\n    bnb_4bit_quant_type=\"nf4\",    # pre-trained model should be quantized in 4-bit NF format\n    bnb_4bit_use_double_quant=True, # Using double quantization as mentioned in QLoRA paper\n    bnb_4bit_compute_dtype=torch.bfloat16, # During computation, pre-trained model should be loaded in BF16 format\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config, # Use bitsandbytes config\n    device_map=\"auto\",  # Specifying device_map=\"auto\" so that HF Accelerate will determine which GPU to put each layer of the model on\n    trust_remote_code=True, # Set trust_remote_code=True to use Mistral-7b model with custom code\n)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T15:15:49.334493Z","iopub.execute_input":"2023-11-27T15:15:49.335283Z","iopub.status.idle":"2023-11-27T15:18:17.674821Z","shell.execute_reply.started":"2023-11-27T15:15:49.335252Z","shell.execute_reply":"2023-11-27T15:18:17.673876Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading config.json:   0%|          | 0.00/622 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d9a74c25c8ae4c52ad622d235740b2df"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)model.bin.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a76a7648e2ad4640975fac7159e8d908"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"438070ecc29c491aa8a74b7bed3c4cc6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)l-00001-of-00002.bin:   0%|          | 0.00/9.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1631cdcf69445979925b75cbe7c5b5c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)l-00002-of-00002.bin:   0%|          | 0.00/4.54G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc0c03fe9cd840b2ba31b660d327fa84"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d43ffb0190114ed8af2c3563bc90bd94"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac907a89c6db4cae9c9fd20be5a4087c"}},"metadata":{}}]},{"cell_type":"code","source":"dataset = load_dataset(dataset_name, split=\"train\")\ndataset = dataset.shuffle().select(range(2000))\ndataset.train_test_split(test_size=0.05)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T15:18:17.676964Z","iopub.execute_input":"2023-11-27T15:18:17.677292Z","iopub.status.idle":"2023-11-27T15:18:28.989803Z","shell.execute_reply.started":"2023-11-27T15:18:17.677265Z","shell.execute_reply":"2023-11-27T15:18:28.988769Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/460 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e8443d78da1a4677930847919167aefd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"506d6c356b5c41538b58b095126a486b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/211M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8e4ce72f3cc455499250a4bb8ac593a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a75622b127464d579303879e974afb21"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/99086 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15061f64bd6a49939ac03dbd8adf0134"}},"metadata":{}},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text'],\n        num_rows: 1900\n    })\n    test: Dataset({\n        features: ['text'],\n        num_rows: 100\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"print(dataset['text'][1])","metadata":{"execution":{"iopub.status.busy":"2023-11-27T15:18:28.991120Z","iopub.execute_input":"2023-11-27T15:18:28.991490Z","iopub.status.idle":"2023-11-27T15:18:29.026462Z","shell.execute_reply.started":"2023-11-27T15:18:28.991453Z","shell.execute_reply":"2023-11-27T15:18:29.025558Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"<s>[INST] <<SYS>>\nYou are a helpful and joyous mental therapy assistant. Always answer as helpfully and cheerfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.Please ensure that your responses are socially unbiased and positive in nature.\n\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n<</SYS>\n\nHey, I'm feeling really confused right now. There's a lot of family conflict going on and I feel like it's all my responsibility. It's overwhelming and I'm not sure what to do. [/INST] , thank you for reaching out. I can understand how family conflict can be confusing and make you feel a heavy sense of responsibility. It sounds like you're carrying a lot on your shoulders. Can you tell me more about the conflict and why you feel responsible? </s><s>[INST] Absolutely.  So, there have been ongoing disagreements between my parents and siblings. It's like everyone is constantly arguing about every little thing and it's creating this tense atmosphere at home. And somehow, I always find myself caught in the middle, trying to mediate and solve their issues. It feels like my responsibility to keep the peace and it's draining me mentally and emotionally. [/INST] It sounds like you've taken on the role of the peacemaker within your family.  That can be both challenging and exhausting. Have you ever had a chance to express how this situation makes you feel? </s><s>[INST] Honestly, I haven't had the chance.  Every time I try to bring up my own feelings, it gets overshadowed by the conflicts between everyone else. It's like my voice isn't being heard and I'm just expected to fix everything. I feel like I'm drowning in this chaos. [/INST] I hear the frustration and overwhelm in your words.  It's important for you to have a space where your feelings and perspective can be valued. Let's explore some ways to address this. How would you feel about having a calm and open conversation with your family members, where you can express your thoughts and boundaries? </s><s>[INST] That actually sounds like a good idea.  I've been so caught up in taking care of everyone else's needs that I've neglected my own. I think it's time for me to speak up, even if it feels uncomfortable at first. I want to find a way to communicate my boundaries without causing more conflict. [/INST] That's a great realization.  It's essential to prioritize your own needs and boundaries in order to maintain your well-being. Let's work together to find some strategies to help you express your thoughts and set healthy boundaries within your family. How do you envision these conversations going? </s><s>[INST] Well, I imagine being honest and calm.  I want to let my family know that while I care about them and want to help, it's crucial for me to also take care of myself. I think reframing the conversation around finding solutions rather than pointing fingers could be a good approach. And most importantly, I want to communicate that this responsibility should be shared among all family members, not just placed on my shoulders. [/INST] It's great that you're approaching this with an open mind and seeking collaborative solutions.  Remember, communication is a two-way street. Listening to each other's perspectives while maintaining respect is key. How can you ensure that everyone involved feels heard? </s><s>[INST] I think active listening will be important.  It's essential for everyone to feel validated and understood. I'll make a conscious effort to really listen to what my family members are saying without interrupting or getting defensive. And when it's my turn to speak, I'll express myself using \"I\" statements to avoid blaming and maintain a constructive tone. [/INST] That's an excellent plan.  Active listening and using \"I\" statements can create a safe space for open dialogue. Remember, change takes time, so patience and consistency will be key on this journey. How can you prioritize self-care during this process? </s><s>[INST] I think setting aside time for myself will be crucial.  Engaging in activities that bring me joy and help me relax will give me the energy and clarity to navigate this challenging situation. Whether it's going for a walk, practicing mindfulness, or spending time with supportive friends, I need to ensure I'm taking care of my own emotional well-being. [/INST] Absolutely.  Self-care is not selfish but necessary for mental and emotional balance. You deserve to prioritize your well-being during this process of navigating family conflict. Remember, I'm here to support you every step of the way. Is there anything else you'd like to discuss or any other concerns you have right now? </s><s>[INST] Thank you.  I feel a lot more hopeful and empowered after our conversation. Right now, I think we've covered all the major concerns I had regarding the family conflict. I will continue to reflect on our discussion and apply the strategies we discussed. If any other concerns arise, I'll definitely reach out to you again. [/INST] It's been a pleasure working with you.  Remember, I'm here to support you whenever you need. Take your time to process everything, and please don't hesitate to reach out if you have any more questions or if new concerns arise. You're on the right path towards finding resolution and fostering healthier family dynamics. </s>\n","output_type":"stream"}]},{"cell_type":"code","source":"\n\nlora_alpha = 32 # scaling factor for the weight matrices\nlora_dropout = 0.05 # dropout probability of the LoRA layers\nlora_rank = 16 # dimension of the low-rank matrices\n\npeft_config = LoraConfig(\n    lora_alpha=lora_alpha,\n    lora_dropout=lora_dropout,\n    r=lora_rank,\n    bias=\"none\",  # setting to 'none' for only training weight params instead of biases\n    task_type=\"CAUSAL_LM\",\n    \n)\n\npeft_model = get_peft_model(model, peft_config)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T15:18:29.027486Z","iopub.execute_input":"2023-11-27T15:18:29.027768Z","iopub.status.idle":"2023-11-27T15:18:29.200908Z","shell.execute_reply.started":"2023-11-27T15:18:29.027744Z","shell.execute_reply":"2023-11-27T15:18:29.200088Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"output_dir = \"./Samantha-2000D-chat-hf-phr_mental_therapy\"\nper_device_train_batch_size = 2 # reduce batch size by 2x only if out-of-memory error\ngradient_accumulation_steps = 1  # increase gradient accumulation steps by  2x only if batch size is reduced\noptim = \"paged_adamw_32bit\" # activates the paging for better memory management\nsave_strategy=\"steps\" # checkpoint save strategy to adopt during training\nsave_steps = 10 # number of updates steps before two checkpoint saves\nlogging_steps = 10  # number of update steps between two logs if logging_strategy=\"steps\"\nlearning_rate = 2e-4  # learning rate for AdamW optimizer\nmax_grad_norm = 0.3 # maximum gradient norm (for gradient clipping)\nmax_steps = -1      # training will happen for automatic steps\nwarmup_ratio = 0.03 # number of steps used for a linear warmup from 0 to learning_rate\nlr_scheduler_type = \"cosine\"  # learning rate scheduler\nnum_train_epochs = 2\nper_device_eval_batch_size = 2\nmax_seq_length=512\n\ntraining_arguments = TrainingArguments(\n    output_dir=output_dir,\n    per_device_train_batch_size=per_device_train_batch_size,\n    gradient_accumulation_steps=gradient_accumulation_steps,\n    optim=optim,\n    num_train_epochs= num_train_epochs,\n    save_steps=save_steps,\n    logging_steps=logging_steps,\n    learning_rate=learning_rate,\n    fp16=True,\n    max_grad_norm=max_grad_norm,\n    max_steps=max_steps,\n    warmup_ratio=warmup_ratio,\n    group_by_length=True,\n    lr_scheduler_type=lr_scheduler_type,\n    push_to_hub=True,\n    \n)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T15:18:29.202049Z","iopub.execute_input":"2023-11-27T15:18:29.202337Z","iopub.status.idle":"2023-11-27T15:18:29.212848Z","shell.execute_reply.started":"2023-11-27T15:18:29.202313Z","shell.execute_reply":"2023-11-27T15:18:29.211977Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=\"auto\"\n)\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\n\n# # Load LLaMA tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True,use_fast=False,add_eos_token=True)\ntokenizer.pad_token_id = 18610","metadata":{"execution":{"iopub.status.busy":"2023-11-27T15:18:29.214061Z","iopub.execute_input":"2023-11-27T15:18:29.214396Z","iopub.status.idle":"2023-11-27T15:19:43.981416Z","shell.execute_reply.started":"2023-11-27T15:18:29.214369Z","shell.execute_reply":"2023-11-27T15:19:43.980375Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a72df9f8c1041fe889366e0ffb0a431"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer_config.json:   0%|          | 0.00/1.69k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac172448ac84461685dea4b12970fe72"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"860b4dd90db64a84bc582e5dd6523218"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading added_tokens.json:   0%|          | 0.00/90.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d05436b4a9f440389e4ffa2d65478023"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/100 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ae7418194764413ab2dcf39f20dc0a8"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"code","source":"#Set supervised fine-tuning parameters\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset,\n    peft_config=peft_config,\n    dataset_text_field=\"text\",\n    max_seq_length=256,\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing=False,\n)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-27T15:19:43.982554Z","iopub.execute_input":"2023-11-27T15:19:43.982855Z","iopub.status.idle":"2023-11-27T15:20:02.568755Z","shell.execute_reply.started":"2023-11-27T15:19:43.982828Z","shell.execute_reply":"2023-11-27T15:20:02.567965Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6c89f45682c41b6a56565652b1edc29"}},"metadata":{}}]},{"cell_type":"code","source":"# upcasting the layer norms in torch.bfloat16 for more stable training\nfor name, module in trainer.model.named_modules():\n    if \"norm\" in name:\n        module = module.to(torch.bfloat16)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T15:20:02.571459Z","iopub.execute_input":"2023-11-27T15:20:02.571778Z","iopub.status.idle":"2023-11-27T15:20:02.582695Z","shell.execute_reply.started":"2023-11-27T15:20:02.571750Z","shell.execute_reply":"2023-11-27T15:20:02.581923Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"model.config.use_cache = False\ntrainer.train()\ntrainer.model.save_pretrained(new_model)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T15:20:02.583928Z","iopub.execute_input":"2023-11-27T15:20:02.584285Z","iopub.status.idle":"2023-11-27T16:16:45.921379Z","shell.execute_reply.started":"2023-11-27T15:20:02.584254Z","shell.execute_reply":"2023-11-27T16:16:45.920050Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mr1ajayakumaran1751994\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20231127_152003-dwri21yq</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/r1ajayakumaran1751994/huggingface/runs/dwri21yq' target=\"_blank\">ruby-wave-30</a></strong> to <a href='https://wandb.ai/r1ajayakumaran1751994/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/r1ajayakumaran1751994/huggingface' target=\"_blank\">https://wandb.ai/r1ajayakumaran1751994/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/r1ajayakumaran1751994/huggingface/runs/dwri21yq' target=\"_blank\">https://wandb.ai/r1ajayakumaran1751994/huggingface/runs/dwri21yq</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2000' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2000/2000 56:03, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>2.669100</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>2.317000</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>1.500800</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.880400</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.746800</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.708900</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.647200</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.636100</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.606700</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.598700</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>0.582400</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.552800</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>0.571500</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.550200</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.551300</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.532600</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>0.526100</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.510700</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>0.508300</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.490000</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>0.507700</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>0.470300</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>0.451900</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>0.489400</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.433500</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>0.459900</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>0.449300</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>0.449600</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>0.697500</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.447800</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>0.436000</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>0.477500</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>0.411000</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>0.456300</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.471800</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>0.446600</td>\n    </tr>\n    <tr>\n      <td>370</td>\n      <td>0.441000</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>0.454700</td>\n    </tr>\n    <tr>\n      <td>390</td>\n      <td>0.435400</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.445700</td>\n    </tr>\n    <tr>\n      <td>410</td>\n      <td>0.446600</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>0.441200</td>\n    </tr>\n    <tr>\n      <td>430</td>\n      <td>0.423300</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>0.450600</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.451800</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>0.456600</td>\n    </tr>\n    <tr>\n      <td>470</td>\n      <td>0.424500</td>\n    </tr>\n    <tr>\n      <td>480</td>\n      <td>0.437700</td>\n    </tr>\n    <tr>\n      <td>490</td>\n      <td>0.443600</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.428000</td>\n    </tr>\n    <tr>\n      <td>510</td>\n      <td>0.418000</td>\n    </tr>\n    <tr>\n      <td>520</td>\n      <td>0.455600</td>\n    </tr>\n    <tr>\n      <td>530</td>\n      <td>0.437000</td>\n    </tr>\n    <tr>\n      <td>540</td>\n      <td>0.408900</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>0.447200</td>\n    </tr>\n    <tr>\n      <td>560</td>\n      <td>0.441000</td>\n    </tr>\n    <tr>\n      <td>570</td>\n      <td>0.439000</td>\n    </tr>\n    <tr>\n      <td>580</td>\n      <td>0.437900</td>\n    </tr>\n    <tr>\n      <td>590</td>\n      <td>0.419600</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.408100</td>\n    </tr>\n    <tr>\n      <td>610</td>\n      <td>0.432600</td>\n    </tr>\n    <tr>\n      <td>620</td>\n      <td>0.411700</td>\n    </tr>\n    <tr>\n      <td>630</td>\n      <td>0.445800</td>\n    </tr>\n    <tr>\n      <td>640</td>\n      <td>0.446300</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>0.456900</td>\n    </tr>\n    <tr>\n      <td>660</td>\n      <td>0.411800</td>\n    </tr>\n    <tr>\n      <td>670</td>\n      <td>0.442600</td>\n    </tr>\n    <tr>\n      <td>680</td>\n      <td>0.416200</td>\n    </tr>\n    <tr>\n      <td>690</td>\n      <td>0.407400</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.424000</td>\n    </tr>\n    <tr>\n      <td>710</td>\n      <td>0.437000</td>\n    </tr>\n    <tr>\n      <td>720</td>\n      <td>0.429500</td>\n    </tr>\n    <tr>\n      <td>730</td>\n      <td>0.433700</td>\n    </tr>\n    <tr>\n      <td>740</td>\n      <td>0.432500</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>0.436600</td>\n    </tr>\n    <tr>\n      <td>760</td>\n      <td>0.415500</td>\n    </tr>\n    <tr>\n      <td>770</td>\n      <td>0.434400</td>\n    </tr>\n    <tr>\n      <td>780</td>\n      <td>0.425300</td>\n    </tr>\n    <tr>\n      <td>790</td>\n      <td>0.446700</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.418800</td>\n    </tr>\n    <tr>\n      <td>810</td>\n      <td>0.415700</td>\n    </tr>\n    <tr>\n      <td>820</td>\n      <td>0.406100</td>\n    </tr>\n    <tr>\n      <td>830</td>\n      <td>0.426500</td>\n    </tr>\n    <tr>\n      <td>840</td>\n      <td>0.438700</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>0.423200</td>\n    </tr>\n    <tr>\n      <td>860</td>\n      <td>0.410800</td>\n    </tr>\n    <tr>\n      <td>870</td>\n      <td>0.416200</td>\n    </tr>\n    <tr>\n      <td>880</td>\n      <td>0.410900</td>\n    </tr>\n    <tr>\n      <td>890</td>\n      <td>0.399500</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.436100</td>\n    </tr>\n    <tr>\n      <td>910</td>\n      <td>0.423800</td>\n    </tr>\n    <tr>\n      <td>920</td>\n      <td>0.428500</td>\n    </tr>\n    <tr>\n      <td>930</td>\n      <td>0.428600</td>\n    </tr>\n    <tr>\n      <td>940</td>\n      <td>0.413900</td>\n    </tr>\n    <tr>\n      <td>950</td>\n      <td>0.413100</td>\n    </tr>\n    <tr>\n      <td>960</td>\n      <td>0.436900</td>\n    </tr>\n    <tr>\n      <td>970</td>\n      <td>0.448400</td>\n    </tr>\n    <tr>\n      <td>980</td>\n      <td>0.411100</td>\n    </tr>\n    <tr>\n      <td>990</td>\n      <td>0.399500</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.418800</td>\n    </tr>\n    <tr>\n      <td>1010</td>\n      <td>0.341000</td>\n    </tr>\n    <tr>\n      <td>1020</td>\n      <td>0.371600</td>\n    </tr>\n    <tr>\n      <td>1030</td>\n      <td>0.358700</td>\n    </tr>\n    <tr>\n      <td>1040</td>\n      <td>0.359600</td>\n    </tr>\n    <tr>\n      <td>1050</td>\n      <td>0.360800</td>\n    </tr>\n    <tr>\n      <td>1060</td>\n      <td>0.341200</td>\n    </tr>\n    <tr>\n      <td>1070</td>\n      <td>0.350900</td>\n    </tr>\n    <tr>\n      <td>1080</td>\n      <td>0.350800</td>\n    </tr>\n    <tr>\n      <td>1090</td>\n      <td>0.333300</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.364400</td>\n    </tr>\n    <tr>\n      <td>1110</td>\n      <td>0.345800</td>\n    </tr>\n    <tr>\n      <td>1120</td>\n      <td>0.382100</td>\n    </tr>\n    <tr>\n      <td>1130</td>\n      <td>0.360600</td>\n    </tr>\n    <tr>\n      <td>1140</td>\n      <td>0.362300</td>\n    </tr>\n    <tr>\n      <td>1150</td>\n      <td>0.349500</td>\n    </tr>\n    <tr>\n      <td>1160</td>\n      <td>0.350000</td>\n    </tr>\n    <tr>\n      <td>1170</td>\n      <td>0.353100</td>\n    </tr>\n    <tr>\n      <td>1180</td>\n      <td>0.344100</td>\n    </tr>\n    <tr>\n      <td>1190</td>\n      <td>0.364600</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.349700</td>\n    </tr>\n    <tr>\n      <td>1210</td>\n      <td>0.382600</td>\n    </tr>\n    <tr>\n      <td>1220</td>\n      <td>0.370300</td>\n    </tr>\n    <tr>\n      <td>1230</td>\n      <td>0.374400</td>\n    </tr>\n    <tr>\n      <td>1240</td>\n      <td>0.347900</td>\n    </tr>\n    <tr>\n      <td>1250</td>\n      <td>0.365300</td>\n    </tr>\n    <tr>\n      <td>1260</td>\n      <td>0.354200</td>\n    </tr>\n    <tr>\n      <td>1270</td>\n      <td>0.342200</td>\n    </tr>\n    <tr>\n      <td>1280</td>\n      <td>0.339200</td>\n    </tr>\n    <tr>\n      <td>1290</td>\n      <td>0.367000</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>0.345200</td>\n    </tr>\n    <tr>\n      <td>1310</td>\n      <td>0.353700</td>\n    </tr>\n    <tr>\n      <td>1320</td>\n      <td>0.363700</td>\n    </tr>\n    <tr>\n      <td>1330</td>\n      <td>0.347500</td>\n    </tr>\n    <tr>\n      <td>1340</td>\n      <td>0.373300</td>\n    </tr>\n    <tr>\n      <td>1350</td>\n      <td>0.336800</td>\n    </tr>\n    <tr>\n      <td>1360</td>\n      <td>0.355300</td>\n    </tr>\n    <tr>\n      <td>1370</td>\n      <td>0.341300</td>\n    </tr>\n    <tr>\n      <td>1380</td>\n      <td>0.345900</td>\n    </tr>\n    <tr>\n      <td>1390</td>\n      <td>0.323100</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.360900</td>\n    </tr>\n    <tr>\n      <td>1410</td>\n      <td>0.360600</td>\n    </tr>\n    <tr>\n      <td>1420</td>\n      <td>0.338400</td>\n    </tr>\n    <tr>\n      <td>1430</td>\n      <td>0.359600</td>\n    </tr>\n    <tr>\n      <td>1440</td>\n      <td>0.359500</td>\n    </tr>\n    <tr>\n      <td>1450</td>\n      <td>0.342600</td>\n    </tr>\n    <tr>\n      <td>1460</td>\n      <td>0.356800</td>\n    </tr>\n    <tr>\n      <td>1470</td>\n      <td>0.342400</td>\n    </tr>\n    <tr>\n      <td>1480</td>\n      <td>0.348800</td>\n    </tr>\n    <tr>\n      <td>1490</td>\n      <td>0.332200</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.362300</td>\n    </tr>\n    <tr>\n      <td>1510</td>\n      <td>0.331800</td>\n    </tr>\n    <tr>\n      <td>1520</td>\n      <td>0.341400</td>\n    </tr>\n    <tr>\n      <td>1530</td>\n      <td>0.336400</td>\n    </tr>\n    <tr>\n      <td>1540</td>\n      <td>0.359100</td>\n    </tr>\n    <tr>\n      <td>1550</td>\n      <td>0.358300</td>\n    </tr>\n    <tr>\n      <td>1560</td>\n      <td>0.329100</td>\n    </tr>\n    <tr>\n      <td>1570</td>\n      <td>0.336800</td>\n    </tr>\n    <tr>\n      <td>1580</td>\n      <td>0.368100</td>\n    </tr>\n    <tr>\n      <td>1590</td>\n      <td>0.333800</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>0.338400</td>\n    </tr>\n    <tr>\n      <td>1610</td>\n      <td>0.340700</td>\n    </tr>\n    <tr>\n      <td>1620</td>\n      <td>0.362600</td>\n    </tr>\n    <tr>\n      <td>1630</td>\n      <td>0.346100</td>\n    </tr>\n    <tr>\n      <td>1640</td>\n      <td>0.348100</td>\n    </tr>\n    <tr>\n      <td>1650</td>\n      <td>0.315100</td>\n    </tr>\n    <tr>\n      <td>1660</td>\n      <td>0.345200</td>\n    </tr>\n    <tr>\n      <td>1670</td>\n      <td>0.355700</td>\n    </tr>\n    <tr>\n      <td>1680</td>\n      <td>0.332500</td>\n    </tr>\n    <tr>\n      <td>1690</td>\n      <td>0.341100</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>0.337500</td>\n    </tr>\n    <tr>\n      <td>1710</td>\n      <td>0.335700</td>\n    </tr>\n    <tr>\n      <td>1720</td>\n      <td>0.356500</td>\n    </tr>\n    <tr>\n      <td>1730</td>\n      <td>0.360900</td>\n    </tr>\n    <tr>\n      <td>1740</td>\n      <td>0.327900</td>\n    </tr>\n    <tr>\n      <td>1750</td>\n      <td>0.352400</td>\n    </tr>\n    <tr>\n      <td>1760</td>\n      <td>0.355400</td>\n    </tr>\n    <tr>\n      <td>1770</td>\n      <td>0.336200</td>\n    </tr>\n    <tr>\n      <td>1780</td>\n      <td>0.369100</td>\n    </tr>\n    <tr>\n      <td>1790</td>\n      <td>0.346000</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>0.335800</td>\n    </tr>\n    <tr>\n      <td>1810</td>\n      <td>0.351000</td>\n    </tr>\n    <tr>\n      <td>1820</td>\n      <td>0.350700</td>\n    </tr>\n    <tr>\n      <td>1830</td>\n      <td>0.322200</td>\n    </tr>\n    <tr>\n      <td>1840</td>\n      <td>0.331200</td>\n    </tr>\n    <tr>\n      <td>1850</td>\n      <td>0.309500</td>\n    </tr>\n    <tr>\n      <td>1860</td>\n      <td>0.347400</td>\n    </tr>\n    <tr>\n      <td>1870</td>\n      <td>0.348000</td>\n    </tr>\n    <tr>\n      <td>1880</td>\n      <td>0.362100</td>\n    </tr>\n    <tr>\n      <td>1890</td>\n      <td>0.332100</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>0.344700</td>\n    </tr>\n    <tr>\n      <td>1910</td>\n      <td>0.357600</td>\n    </tr>\n    <tr>\n      <td>1920</td>\n      <td>0.339800</td>\n    </tr>\n    <tr>\n      <td>1930</td>\n      <td>0.316400</td>\n    </tr>\n    <tr>\n      <td>1940</td>\n      <td>0.325200</td>\n    </tr>\n    <tr>\n      <td>1950</td>\n      <td>0.326600</td>\n    </tr>\n    <tr>\n      <td>1960</td>\n      <td>0.343500</td>\n    </tr>\n    <tr>\n      <td>1970</td>\n      <td>0.363300</td>\n    </tr>\n    <tr>\n      <td>1980</td>\n      <td>0.352700</td>\n    </tr>\n    <tr>\n      <td>1990</td>\n      <td>0.362600</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.347600</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}]},{"cell_type":"code","source":"trainer.push_to_hub()","metadata":{"execution":{"iopub.status.busy":"2023-11-27T16:25:34.859666Z","iopub.execute_input":"2023-11-27T16:25:34.860076Z","iopub.status.idle":"2023-11-27T16:25:35.621297Z","shell.execute_reply.started":"2023-11-27T16:25:34.860040Z","shell.execute_reply":"2023-11-27T16:25:35.620351Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"'https://huggingface.co/Jaykumaran17/Samantha-2000D-chat-hf-phr_mental_therapy/tree/main/'"},"metadata":{}}]},{"cell_type":"markdown","source":"# Merging as Single without Adapters","metadata":{}},{"cell_type":"code","source":"# Reload model in FP16 and merge it with LoRA weights\nbase_model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    low_cpu_mem_usage=True,\n    return_dict=True,\n    torch_dtype=torch.float16,\n    device_map=\"cpu\", ## device map = \"cpu\", for merging on cpu\n)\nmodel = PeftModel.from_pretrained(base_model, new_model)\nmodel = model.merge_and_unload()\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True,use_fast=False,add_eos_token=True)\ntokenizer.pad_token_id = 18610\n\n### Old version tokeniser, can't generate eos token\n# tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n# tokenizer.pad_token = tokenizer.eos_token\n# tokenizer.padding_side = \"right\"\n\n","metadata":{"execution":{"iopub.status.busy":"2023-11-27T16:25:49.297022Z","iopub.execute_input":"2023-11-27T16:25:49.297512Z","iopub.status.idle":"2023-11-27T16:27:20.700724Z","shell.execute_reply.started":"2023-11-27T16:25:49.297480Z","shell.execute_reply":"2023-11-27T16:27:20.699606Z"},"trusted":true},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce6582c537cc4fe7a29250f7e5611208"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"code","source":"## for inference without merging qlora weights with original\nbase_model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=\"auto\"\n)\nmodel = PeftModel.from_pretrained(base_model, new_model)\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True,use_fast=False,add_eos_token=True)\ntokenizer.pad_token_id = 18610","metadata":{"execution":{"iopub.status.busy":"2023-11-27T16:27:48.636415Z","iopub.execute_input":"2023-11-27T16:27:48.636818Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"979753e84b9a4f10aed91e92c755eec2"}},"metadata":{}}]},{"cell_type":"code","source":"model.push_to_hub(new_model, use_temp_dir=False)\ntokenizer.push_to_hub(new_model, use_temp_dir=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logging.set_verbosity(logging.CRITICAL)\nSYSTEM_PROMPT = \"You are a helpful and joyous mental therapy assistant. Always answer as helpfully and cheerfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.If you don't know the answer to a question, please don't share false information. Always try to be as cheerfull as possible\"\n# Run text generation pipeline with our next model\nprompt = \"I am feeling suicidal.I have lost a lot of money in gambling. The money I used was taken as a loan\"\npipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=1000,temperature=0.9)\nresult = pipe(f\"<s>[INST]<<SYS>>{SYSTEM_PROMPT}<</SYS>> {prompt} [/INST]\")\nprint(result[0]['generated_text'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}