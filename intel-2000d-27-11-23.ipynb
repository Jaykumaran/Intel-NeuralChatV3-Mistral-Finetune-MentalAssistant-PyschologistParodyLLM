{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()\n\n#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","metadata":{"execution":{"iopub.status.busy":"2023-11-27T13:46:38.707975Z","iopub.execute_input":"2023-11-27T13:46:38.708222Z","iopub.status.idle":"2023-11-27T13:46:39.086384Z","shell.execute_reply.started":"2023-11-27T13:46:38.708199Z","shell.execute_reply":"2023-11-27T13:46:39.085516Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e83686dc14ce4d82bb5a35a1acc1f30c"}},"metadata":{}}]},{"cell_type":"code","source":"# authenticate WandB for logging metrics\nimport wandb\nwandb.login()\n# ee311bc313381f6bee4c5412a630fdad1175f3c0\n","metadata":{"execution":{"iopub.status.busy":"2023-11-27T13:48:12.913555Z","iopub.execute_input":"2023-11-27T13:48:12.914189Z","iopub.status.idle":"2023-11-27T13:48:20.404992Z","shell.execute_reply.started":"2023-11-27T13:48:12.914152Z","shell.execute_reply":"2023-11-27T13:48:20.404023Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"!pip install trl transformers accelerate git+https://github.com/huggingface/peft.git -Uqqq\n!pip install datasets bitsandbytes einops wandb -Uqqq","metadata":{"execution":{"iopub.status.busy":"2023-11-27T13:48:25.038769Z","iopub.execute_input":"2023-11-27T13:48:25.039585Z","iopub.status.idle":"2023-11-27T13:49:26.415053Z","shell.execute_reply.started":"2023-11-27T13:48:25.039551Z","shell.execute_reply":"2023-11-27T13:49:26.413882Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntokenizers 0.14.1 requires huggingface_hub<0.18,>=0.16.4, but you have huggingface-hub 0.19.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import os\n\nimport torch\nfrom datasets import load_dataset\nfrom peft import LoraConfig, get_peft_model, PeftConfig, PeftModel, prepare_model_for_kbit_training\nfrom transformers import (AutoModelForCausalLM, AutoTokenizer,\n                          BitsAndBytesConfig, HfArgumentParser,\n                          TrainingArguments,GenerationConfig, logging, pipeline)\nfrom trl import SFTTrainer","metadata":{"execution":{"iopub.status.busy":"2023-11-27T13:49:49.524534Z","iopub.execute_input":"2023-11-27T13:49:49.525079Z","iopub.status.idle":"2023-11-27T13:49:49.531139Z","shell.execute_reply.started":"2023-11-27T13:49:49.525050Z","shell.execute_reply":"2023-11-27T13:49:49.530268Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"model_name = \"Intel/neural-chat-7b-v3-1\" # sharded Mistral-7b model\ndataset_name = \"vibhorag101/phr_mental_health_dataset\"\nnew_model = \"Intel-2000D-chat-hf-phr_mental_therapy\"\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,            # load model in 4-bit precision\n    bnb_4bit_quant_type=\"nf4\",    # pre-trained model should be quantized in 4-bit NF format\n    bnb_4bit_use_double_quant=True, # Using double quantization as mentioned in QLoRA paper\n    bnb_4bit_compute_dtype=torch.bfloat16, # During computation, pre-trained model should be loaded in BF16 format\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config, # Use bitsandbytes config\n    device_map=\"auto\",  # Specifying device_map=\"auto\" so that HF Accelerate will determine which GPU to put each layer of the model on\n    trust_remote_code=True, # Set trust_remote_code=True to use Mistral-7b model with custom code\n)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T13:51:58.901885Z","iopub.execute_input":"2023-11-27T13:51:58.902260Z","iopub.status.idle":"2023-11-27T13:52:59.110742Z","shell.execute_reply.started":"2023-11-27T13:51:58.902196Z","shell.execute_reply":"2023-11-27T13:52:59.109968Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4412f56753db4168b95674ebb3cc53be"}},"metadata":{}}]},{"cell_type":"code","source":"dataset = load_dataset(dataset_name, split=\"train\")\ndataset = dataset.shuffle().select(range(2000))\ndataset.train_test_split(test_size=0.05)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T13:52:59.112759Z","iopub.execute_input":"2023-11-27T13:52:59.113085Z","iopub.status.idle":"2023-11-27T13:53:08.710609Z","shell.execute_reply.started":"2023-11-27T13:52:59.113056Z","shell.execute_reply":"2023-11-27T13:53:08.709606Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/460 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e148df3ecd9d46969c066065018ff62a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5093016642ca49d1b6c7a97a33be06e1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/211M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61f8020d126e4c27a780fbd6465eb9fa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"834d3fdbd2504736aa71a90c101dc549"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/99086 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"296a2d140fed47a8b6a261b2749c44a2"}},"metadata":{}},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text'],\n        num_rows: 1900\n    })\n    test: Dataset({\n        features: ['text'],\n        num_rows: 100\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"print(dataset['text'][1])","metadata":{"execution":{"iopub.status.busy":"2023-11-27T13:53:08.711708Z","iopub.execute_input":"2023-11-27T13:53:08.712020Z","iopub.status.idle":"2023-11-27T13:53:08.746149Z","shell.execute_reply.started":"2023-11-27T13:53:08.711993Z","shell.execute_reply":"2023-11-27T13:53:08.745299Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"<s>[INST] <<SYS>>\nYou are a helpful and joyous mental therapy assistant. Always answer as helpfully and cheerfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.Please ensure that your responses are socially unbiased and positive in nature.\n\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n<</SYS>\n\nI feel so numb.  My addiction has completely taken over my life, and I don't know how to escape its tight grip. It's tearing me apart, especially when I think about the impact it's having on my wife. [/INST] , I'm really glad you reached out. It takes courage to acknowledge the struggles you're facing with addiction, and the effect it has on your relationship with your wife. Can you describe some of the ways addiction has affected both of you? </s><s>[INST] Thank you.  My addiction has caused me to withdraw emotionally from my wife. I'm often preoccupied with my cravings, leaving little room for genuine connection and intimacy. I know it's tearing her apart, and it hurts me deeply to see her in pain because of my actions. [/INST] It's understandable that addiction can create barriers to emotional intimacy in relationships., what would it mean to you if you were able to break free from the grip of addiction and rebuild that emotional connection with your wife? </s><s>[INST] It would mean the world to me.  I love my wife deeply, and I want nothing more than to create a loving and supportive partnership. Breaking free from addiction would allow me to be present and emotionally available for her, fostering a sense of security and trust that we both deserve. [/INST] That's a powerful motivation.  Let's explore what might be triggering your addiction. Are there any specific situations, emotions, or thoughts that tend to lead you towards substance use? </s><s>[INST] Absolutely.  Stress and anxiety play a significant role in my addiction. Whenever I feel overwhelmed or faced with difficult emotions, my instinct is to turn to substances for temporary relief. It becomes a vicious cycle that only perpetuates my struggles. [/INST] It's understandable that stress and difficult emotions can be triggers for addictive behaviors. Let's focus on healthier coping mechanisms that can help break this cycle. What are some activities or practices that bring you a sense of calm and peace? </s><s>[INST] Engaging in physical exercise, particularly running, helps clear my mind and releases some of the tension I carry. Journaling also allows me to process my emotions and gain clarity. Additionally, spending quality time with my wife, engaging in activities we both enjoy, brings a profound sense of joy and fulfillment. [/INST] Those are fantastic coping strategies, ! Incorporating these activities into your daily routine can provide a healthy outlet for stress and emotions. How can you proactively create space for these activities in your life? </s><s>[INST] I can start by scheduling dedicated time for exercise and journaling, even if it's just 30 minutes a day. By communicating with my wife and involving her in this process, we can plan activities together that nurture our bond and bring us closer. It's about being intentional and making our well-being a priority. [/INST] That's wonderful.  By involving your wife in your journey towards recovery, you're fostering shared support and understanding. As you work towards breaking free from addiction, are there any specific steps or resources you feel would be helpful? </s><s>[INST] I believe attending support group meetings, such as Alcoholics Anonymous, would provide me with a sense of community and guidance. Additionally, seeking individual therapy, like what we're doing now, can offer personalized strategies and tools to navigate the challenges of recovery. [/INST] Absolutely.  Support groups and therapy are valuable resources that can provide you with ongoing support and guidance. Remember, you don't have to face this journey alone. Is there anything else on your mind that you'd like to discuss or explore further? </s><s>[INST] Right now, I feel more motivated and hopeful.  I'm grateful for your guidance and non-judgmental approach. I think we've covered a great deal today, and I'm eager to start implementing these strategies into my life. Thank you for being there for me. [/INST] You're very welcome.  I'm glad to hear that you're feeling motivated and hopeful. Remember, I'm here to support you every step of the way. Feel free to reach out anytime you need to discuss further or share your progress. You've taken a significant step towards healing, and I believe in your strength to overcome addiction and rebuild your relationship with your wife. </s>\n","output_type":"stream"}]},{"cell_type":"code","source":"\n\nlora_alpha = 32 # scaling factor for the weight matrices\nlora_dropout = 0.05 # dropout probability of the LoRA layers\nlora_rank = 16 # dimension of the low-rank matrices\n\npeft_config = LoraConfig(\n    lora_alpha=lora_alpha,\n    lora_dropout=lora_dropout,\n    r=lora_rank,\n    bias=\"none\",  # setting to 'none' for only training weight params instead of biases\n    task_type=\"CAUSAL_LM\",\n    \n)\n\npeft_model = get_peft_model(model, peft_config)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T13:53:08.747294Z","iopub.execute_input":"2023-11-27T13:53:08.747709Z","iopub.status.idle":"2023-11-27T13:53:08.917532Z","shell.execute_reply.started":"2023-11-27T13:53:08.747675Z","shell.execute_reply":"2023-11-27T13:53:08.916706Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"output_dir = \"./Intel-2000D-chat-hf-phr_mental_therapy\"\nper_device_train_batch_size = 2 # reduce batch size by 2x only if out-of-memory error\ngradient_accumulation_steps = 1  # increase gradient accumulation steps by  2x only if batch size is reduced\noptim = \"paged_adamw_32bit\" # activates the paging for better memory management\nsave_strategy=\"steps\" # checkpoint save strategy to adopt during training\nsave_steps = 10 # number of updates steps before two checkpoint saves\nlogging_steps = 10  # number of update steps between two logs if logging_strategy=\"steps\"\nlearning_rate = 2e-4  # learning rate for AdamW optimizer\nmax_grad_norm = 0.3 # maximum gradient norm (for gradient clipping)\nmax_steps = -1      # training will happen for automatic steps\nwarmup_ratio = 0.03 # number of steps used for a linear warmup from 0 to learning_rate\nlr_scheduler_type = \"cosine\"  # learning rate scheduler\nnum_train_epochs = 2\nper_device_eval_batch_size = 2\nmax_seq_length=512\n\ntraining_arguments = TrainingArguments(\n    output_dir=output_dir,\n    per_device_train_batch_size=per_device_train_batch_size,\n    gradient_accumulation_steps=gradient_accumulation_steps,\n    optim=optim,\n    num_train_epochs= num_train_epochs,\n    save_steps=save_steps,\n    logging_steps=logging_steps,\n    learning_rate=learning_rate,\n    fp16=True,\n    max_grad_norm=max_grad_norm,\n    max_steps=max_steps,\n    warmup_ratio=warmup_ratio,\n    group_by_length=True,\n    lr_scheduler_type=lr_scheduler_type,\n    push_to_hub=True,\n    \n)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T13:53:08.918560Z","iopub.execute_input":"2023-11-27T13:53:08.918807Z","iopub.status.idle":"2023-11-27T13:53:08.930550Z","shell.execute_reply.started":"2023-11-27T13:53:08.918784Z","shell.execute_reply":"2023-11-27T13:53:08.929676Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=\"auto\"\n)\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\n\n# # Load LLaMA tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True,use_fast=False,add_eos_token=True)\ntokenizer.pad_token_id = 18610","metadata":{"execution":{"iopub.status.busy":"2023-11-27T13:53:08.931517Z","iopub.execute_input":"2023-11-27T13:53:08.931787Z","iopub.status.idle":"2023-11-27T13:54:21.190073Z","shell.execute_reply.started":"2023-11-27T13:53:08.931762Z","shell.execute_reply":"2023-11-27T13:54:21.189344Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f644451235fb4ff79b5ebe62e51169a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer_config.json:   0%|          | 0.00/953 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2bfcbbb291e04cf6b5e520fc9c04b0d6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4777f32f11c4d5e9bda3c2f98b6e1bf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/145 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38bfdbac969849dfa35c4df2792aa11f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9f1b205bb834801bc0d9a5814c09458"}},"metadata":{}}]},{"cell_type":"code","source":"#Set supervised fine-tuning parameters\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset,\n    peft_config=peft_config,\n    dataset_text_field=\"text\",\n    max_seq_length=256,\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing=False,\n)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-27T13:54:21.193048Z","iopub.execute_input":"2023-11-27T13:54:21.193805Z","iopub.status.idle":"2023-11-27T13:54:41.455711Z","shell.execute_reply.started":"2023-11-27T13:54:21.193766Z","shell.execute_reply":"2023-11-27T13:54:41.454805Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21fdd5958ca449f08a79dde41a673cc3"}},"metadata":{}}]},{"cell_type":"code","source":"# upcasting the layer norms in torch.bfloat16 for more stable training\nfor name, module in trainer.model.named_modules():\n    if \"norm\" in name:\n        module = module.to(torch.bfloat16)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T13:54:41.456830Z","iopub.execute_input":"2023-11-27T13:54:41.457097Z","iopub.status.idle":"2023-11-27T13:54:41.468894Z","shell.execute_reply.started":"2023-11-27T13:54:41.457073Z","shell.execute_reply":"2023-11-27T13:54:41.468136Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"model.config.use_cache = False\ntrainer.train()\ntrainer.model.save_pretrained(new_model)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T13:54:41.470071Z","iopub.execute_input":"2023-11-27T13:54:41.470344Z","iopub.status.idle":"2023-11-27T14:51:56.388991Z","shell.execute_reply.started":"2023-11-27T13:54:41.470321Z","shell.execute_reply":"2023-11-27T14:51:56.387938Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mr1ajayakumaran1751994\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20231127_135442-d71d3vhd</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/r1ajayakumaran1751994/huggingface/runs/d71d3vhd' target=\"_blank\">twilight-elevator-28</a></strong> to <a href='https://wandb.ai/r1ajayakumaran1751994/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/r1ajayakumaran1751994/huggingface' target=\"_blank\">https://wandb.ai/r1ajayakumaran1751994/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/r1ajayakumaran1751994/huggingface/runs/d71d3vhd' target=\"_blank\">https://wandb.ai/r1ajayakumaran1751994/huggingface/runs/d71d3vhd</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2000' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2000/2000 56:34, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>2.857000</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>2.282600</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>1.438100</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.764800</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.688400</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.599300</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.613400</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.604600</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.591600</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.560000</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>0.570700</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.546500</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>0.545100</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.556800</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.532800</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.520600</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>0.508700</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.498200</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>0.478000</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.470200</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>0.466000</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>0.478200</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>0.445100</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>0.448000</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.460200</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>0.442200</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>0.463300</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>0.456900</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>0.465100</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.459100</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>0.465600</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>0.429400</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>0.440300</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>0.477500</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.460200</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>0.456900</td>\n    </tr>\n    <tr>\n      <td>370</td>\n      <td>0.462600</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>0.452300</td>\n    </tr>\n    <tr>\n      <td>390</td>\n      <td>0.436500</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.457400</td>\n    </tr>\n    <tr>\n      <td>410</td>\n      <td>0.428000</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>0.455300</td>\n    </tr>\n    <tr>\n      <td>430</td>\n      <td>0.461700</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>0.407300</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.451800</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>0.441600</td>\n    </tr>\n    <tr>\n      <td>470</td>\n      <td>0.491600</td>\n    </tr>\n    <tr>\n      <td>480</td>\n      <td>0.442700</td>\n    </tr>\n    <tr>\n      <td>490</td>\n      <td>0.443400</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.434700</td>\n    </tr>\n    <tr>\n      <td>510</td>\n      <td>0.421600</td>\n    </tr>\n    <tr>\n      <td>520</td>\n      <td>0.427600</td>\n    </tr>\n    <tr>\n      <td>530</td>\n      <td>0.442500</td>\n    </tr>\n    <tr>\n      <td>540</td>\n      <td>0.404500</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>0.435700</td>\n    </tr>\n    <tr>\n      <td>560</td>\n      <td>0.423900</td>\n    </tr>\n    <tr>\n      <td>570</td>\n      <td>0.468200</td>\n    </tr>\n    <tr>\n      <td>580</td>\n      <td>0.442700</td>\n    </tr>\n    <tr>\n      <td>590</td>\n      <td>0.462100</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.424300</td>\n    </tr>\n    <tr>\n      <td>610</td>\n      <td>0.404200</td>\n    </tr>\n    <tr>\n      <td>620</td>\n      <td>0.454600</td>\n    </tr>\n    <tr>\n      <td>630</td>\n      <td>0.441900</td>\n    </tr>\n    <tr>\n      <td>640</td>\n      <td>0.399000</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>0.415800</td>\n    </tr>\n    <tr>\n      <td>660</td>\n      <td>0.429000</td>\n    </tr>\n    <tr>\n      <td>670</td>\n      <td>0.432500</td>\n    </tr>\n    <tr>\n      <td>680</td>\n      <td>0.438900</td>\n    </tr>\n    <tr>\n      <td>690</td>\n      <td>0.434800</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.432900</td>\n    </tr>\n    <tr>\n      <td>710</td>\n      <td>0.426700</td>\n    </tr>\n    <tr>\n      <td>720</td>\n      <td>0.431900</td>\n    </tr>\n    <tr>\n      <td>730</td>\n      <td>0.429700</td>\n    </tr>\n    <tr>\n      <td>740</td>\n      <td>0.420700</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>0.435600</td>\n    </tr>\n    <tr>\n      <td>760</td>\n      <td>0.414400</td>\n    </tr>\n    <tr>\n      <td>770</td>\n      <td>0.388200</td>\n    </tr>\n    <tr>\n      <td>780</td>\n      <td>0.431000</td>\n    </tr>\n    <tr>\n      <td>790</td>\n      <td>0.407700</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.426000</td>\n    </tr>\n    <tr>\n      <td>810</td>\n      <td>0.415600</td>\n    </tr>\n    <tr>\n      <td>820</td>\n      <td>0.415200</td>\n    </tr>\n    <tr>\n      <td>830</td>\n      <td>0.400300</td>\n    </tr>\n    <tr>\n      <td>840</td>\n      <td>0.430100</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>0.424200</td>\n    </tr>\n    <tr>\n      <td>860</td>\n      <td>0.429900</td>\n    </tr>\n    <tr>\n      <td>870</td>\n      <td>0.426000</td>\n    </tr>\n    <tr>\n      <td>880</td>\n      <td>0.399400</td>\n    </tr>\n    <tr>\n      <td>890</td>\n      <td>0.408800</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.435900</td>\n    </tr>\n    <tr>\n      <td>910</td>\n      <td>0.440200</td>\n    </tr>\n    <tr>\n      <td>920</td>\n      <td>0.390100</td>\n    </tr>\n    <tr>\n      <td>930</td>\n      <td>0.387300</td>\n    </tr>\n    <tr>\n      <td>940</td>\n      <td>0.418200</td>\n    </tr>\n    <tr>\n      <td>950</td>\n      <td>0.397900</td>\n    </tr>\n    <tr>\n      <td>960</td>\n      <td>0.424700</td>\n    </tr>\n    <tr>\n      <td>970</td>\n      <td>0.431800</td>\n    </tr>\n    <tr>\n      <td>980</td>\n      <td>0.397200</td>\n    </tr>\n    <tr>\n      <td>990</td>\n      <td>0.429100</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.389500</td>\n    </tr>\n    <tr>\n      <td>1010</td>\n      <td>0.351800</td>\n    </tr>\n    <tr>\n      <td>1020</td>\n      <td>0.371900</td>\n    </tr>\n    <tr>\n      <td>1030</td>\n      <td>0.366200</td>\n    </tr>\n    <tr>\n      <td>1040</td>\n      <td>0.354200</td>\n    </tr>\n    <tr>\n      <td>1050</td>\n      <td>0.346300</td>\n    </tr>\n    <tr>\n      <td>1060</td>\n      <td>0.332600</td>\n    </tr>\n    <tr>\n      <td>1070</td>\n      <td>0.377800</td>\n    </tr>\n    <tr>\n      <td>1080</td>\n      <td>0.337500</td>\n    </tr>\n    <tr>\n      <td>1090</td>\n      <td>0.355900</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.336400</td>\n    </tr>\n    <tr>\n      <td>1110</td>\n      <td>0.362600</td>\n    </tr>\n    <tr>\n      <td>1120</td>\n      <td>0.351600</td>\n    </tr>\n    <tr>\n      <td>1130</td>\n      <td>0.368600</td>\n    </tr>\n    <tr>\n      <td>1140</td>\n      <td>0.335000</td>\n    </tr>\n    <tr>\n      <td>1150</td>\n      <td>0.357100</td>\n    </tr>\n    <tr>\n      <td>1160</td>\n      <td>0.340100</td>\n    </tr>\n    <tr>\n      <td>1170</td>\n      <td>0.342700</td>\n    </tr>\n    <tr>\n      <td>1180</td>\n      <td>0.354200</td>\n    </tr>\n    <tr>\n      <td>1190</td>\n      <td>0.355100</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.369400</td>\n    </tr>\n    <tr>\n      <td>1210</td>\n      <td>0.356700</td>\n    </tr>\n    <tr>\n      <td>1220</td>\n      <td>0.350400</td>\n    </tr>\n    <tr>\n      <td>1230</td>\n      <td>0.355200</td>\n    </tr>\n    <tr>\n      <td>1240</td>\n      <td>0.346400</td>\n    </tr>\n    <tr>\n      <td>1250</td>\n      <td>0.355000</td>\n    </tr>\n    <tr>\n      <td>1260</td>\n      <td>0.355000</td>\n    </tr>\n    <tr>\n      <td>1270</td>\n      <td>0.344600</td>\n    </tr>\n    <tr>\n      <td>1280</td>\n      <td>0.357800</td>\n    </tr>\n    <tr>\n      <td>1290</td>\n      <td>0.342700</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>0.335800</td>\n    </tr>\n    <tr>\n      <td>1310</td>\n      <td>0.353100</td>\n    </tr>\n    <tr>\n      <td>1320</td>\n      <td>0.356200</td>\n    </tr>\n    <tr>\n      <td>1330</td>\n      <td>0.340900</td>\n    </tr>\n    <tr>\n      <td>1340</td>\n      <td>0.333800</td>\n    </tr>\n    <tr>\n      <td>1350</td>\n      <td>0.359400</td>\n    </tr>\n    <tr>\n      <td>1360</td>\n      <td>0.344200</td>\n    </tr>\n    <tr>\n      <td>1370</td>\n      <td>0.348300</td>\n    </tr>\n    <tr>\n      <td>1380</td>\n      <td>0.347200</td>\n    </tr>\n    <tr>\n      <td>1390</td>\n      <td>0.346600</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.354300</td>\n    </tr>\n    <tr>\n      <td>1410</td>\n      <td>0.350100</td>\n    </tr>\n    <tr>\n      <td>1420</td>\n      <td>0.336200</td>\n    </tr>\n    <tr>\n      <td>1430</td>\n      <td>0.343000</td>\n    </tr>\n    <tr>\n      <td>1440</td>\n      <td>0.366900</td>\n    </tr>\n    <tr>\n      <td>1450</td>\n      <td>0.359100</td>\n    </tr>\n    <tr>\n      <td>1460</td>\n      <td>0.357500</td>\n    </tr>\n    <tr>\n      <td>1470</td>\n      <td>0.320200</td>\n    </tr>\n    <tr>\n      <td>1480</td>\n      <td>0.333900</td>\n    </tr>\n    <tr>\n      <td>1490</td>\n      <td>0.351200</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.340400</td>\n    </tr>\n    <tr>\n      <td>1510</td>\n      <td>0.355900</td>\n    </tr>\n    <tr>\n      <td>1520</td>\n      <td>0.332100</td>\n    </tr>\n    <tr>\n      <td>1530</td>\n      <td>0.336000</td>\n    </tr>\n    <tr>\n      <td>1540</td>\n      <td>0.366200</td>\n    </tr>\n    <tr>\n      <td>1550</td>\n      <td>0.379500</td>\n    </tr>\n    <tr>\n      <td>1560</td>\n      <td>0.350400</td>\n    </tr>\n    <tr>\n      <td>1570</td>\n      <td>0.329900</td>\n    </tr>\n    <tr>\n      <td>1580</td>\n      <td>0.364000</td>\n    </tr>\n    <tr>\n      <td>1590</td>\n      <td>0.351400</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>0.353300</td>\n    </tr>\n    <tr>\n      <td>1610</td>\n      <td>0.353700</td>\n    </tr>\n    <tr>\n      <td>1620</td>\n      <td>0.343200</td>\n    </tr>\n    <tr>\n      <td>1630</td>\n      <td>0.342900</td>\n    </tr>\n    <tr>\n      <td>1640</td>\n      <td>0.345900</td>\n    </tr>\n    <tr>\n      <td>1650</td>\n      <td>0.346100</td>\n    </tr>\n    <tr>\n      <td>1660</td>\n      <td>0.356100</td>\n    </tr>\n    <tr>\n      <td>1670</td>\n      <td>0.364600</td>\n    </tr>\n    <tr>\n      <td>1680</td>\n      <td>0.351600</td>\n    </tr>\n    <tr>\n      <td>1690</td>\n      <td>0.341600</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>0.340300</td>\n    </tr>\n    <tr>\n      <td>1710</td>\n      <td>0.350500</td>\n    </tr>\n    <tr>\n      <td>1720</td>\n      <td>0.330700</td>\n    </tr>\n    <tr>\n      <td>1730</td>\n      <td>0.352900</td>\n    </tr>\n    <tr>\n      <td>1740</td>\n      <td>0.348400</td>\n    </tr>\n    <tr>\n      <td>1750</td>\n      <td>0.344800</td>\n    </tr>\n    <tr>\n      <td>1760</td>\n      <td>0.359300</td>\n    </tr>\n    <tr>\n      <td>1770</td>\n      <td>0.350300</td>\n    </tr>\n    <tr>\n      <td>1780</td>\n      <td>0.328300</td>\n    </tr>\n    <tr>\n      <td>1790</td>\n      <td>0.346900</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>0.350100</td>\n    </tr>\n    <tr>\n      <td>1810</td>\n      <td>0.348200</td>\n    </tr>\n    <tr>\n      <td>1820</td>\n      <td>0.337700</td>\n    </tr>\n    <tr>\n      <td>1830</td>\n      <td>0.365600</td>\n    </tr>\n    <tr>\n      <td>1840</td>\n      <td>0.349200</td>\n    </tr>\n    <tr>\n      <td>1850</td>\n      <td>0.350800</td>\n    </tr>\n    <tr>\n      <td>1860</td>\n      <td>0.339800</td>\n    </tr>\n    <tr>\n      <td>1870</td>\n      <td>0.332700</td>\n    </tr>\n    <tr>\n      <td>1880</td>\n      <td>0.333200</td>\n    </tr>\n    <tr>\n      <td>1890</td>\n      <td>0.324700</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>0.325600</td>\n    </tr>\n    <tr>\n      <td>1910</td>\n      <td>0.331700</td>\n    </tr>\n    <tr>\n      <td>1920</td>\n      <td>0.342000</td>\n    </tr>\n    <tr>\n      <td>1930</td>\n      <td>0.320000</td>\n    </tr>\n    <tr>\n      <td>1940</td>\n      <td>0.372000</td>\n    </tr>\n    <tr>\n      <td>1950</td>\n      <td>0.351900</td>\n    </tr>\n    <tr>\n      <td>1960</td>\n      <td>0.306200</td>\n    </tr>\n    <tr>\n      <td>1970</td>\n      <td>0.332600</td>\n    </tr>\n    <tr>\n      <td>1980</td>\n      <td>0.344800</td>\n    </tr>\n    <tr>\n      <td>1990</td>\n      <td>0.357500</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.315000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}]},{"cell_type":"code","source":"trainer.push_to_hub()","metadata":{"execution":{"iopub.status.busy":"2023-11-27T15:09:58.280053Z","iopub.execute_input":"2023-11-27T15:09:58.280944Z","iopub.status.idle":"2023-11-27T15:10:00.282623Z","shell.execute_reply.started":"2023-11-27T15:09:58.280908Z","shell.execute_reply":"2023-11-27T15:10:00.281652Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"'https://huggingface.co/Jaykumaran17/Intel-2000D-chat-hf-phr_mental_therapy/tree/main/'"},"metadata":{}}]},{"cell_type":"markdown","source":"# Merging as Single without Adapters","metadata":{}},{"cell_type":"code","source":"# Reload model in FP16 and merge it with LoRA weights\nbase_model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    low_cpu_mem_usage=True,\n    return_dict=True,\n    torch_dtype=torch.float16,\n    device_map=\"cpu\", ## device map = \"cpu\", for merging on cpu\n)\nmodel = PeftModel.from_pretrained(base_model, new_model)\nmodel = model.merge_and_unload()\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True,use_fast=False,add_eos_token=True)\ntokenizer.pad_token_id = 18610\n\n### Old version tokeniser, can't generate eos token\n# tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n# tokenizer.pad_token = tokenizer.eos_token\n# tokenizer.padding_side = \"right\"\n\n","metadata":{"execution":{"iopub.status.busy":"2023-11-27T14:55:47.242152Z","iopub.execute_input":"2023-11-27T14:55:47.243168Z","iopub.status.idle":"2023-11-27T14:57:09.845173Z","shell.execute_reply.started":"2023-11-27T14:55:47.243119Z","shell.execute_reply":"2023-11-27T14:57:09.844114Z"},"trusted":true},"execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"666ad9206efb402f83bbc4bea858cb87"}},"metadata":{}}]},{"cell_type":"code","source":"## for inference without merging qlora weights with original\nbase_model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=\"auto\"\n)\nmodel = PeftModel.from_pretrained(base_model, new_model)\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True,use_fast=False,add_eos_token=True)\ntokenizer.pad_token_id = 18610","metadata":{"execution":{"iopub.status.busy":"2023-11-27T14:57:47.635539Z","iopub.execute_input":"2023-11-27T14:57:47.636577Z","iopub.status.idle":"2023-11-27T15:00:17.454766Z","shell.execute_reply.started":"2023-11-27T14:57:47.636542Z","shell.execute_reply":"2023-11-27T15:00:17.452300Z"},"trusted":true},"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5813e7e90ef84c6e8eaf00a769636ac5"}},"metadata":{}}]},{"cell_type":"code","source":"model.push_to_hub(new_model, use_temp_dir=False)\ntokenizer.push_to_hub(new_model, use_temp_dir=False)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T15:00:17.462249Z","iopub.execute_input":"2023-11-27T15:00:17.462829Z","iopub.status.idle":"2023-11-27T15:00:20.195486Z","shell.execute_reply.started":"2023-11-27T15:00:17.462767Z","shell.execute_reply":"2023-11-27T15:00:20.194462Z"},"trusted":true},"execution_count":23,"outputs":[{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/27.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3acf9db652e4682af3f15e3b39d41d8"}},"metadata":{}},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/Jaykumaran17/Intel-10000D-chat-hf-phr_mental_therapy/commit/b3e89db48be9539f29683d9032a22c3838c1637f', commit_message='Upload tokenizer', commit_description='', oid='b3e89db48be9539f29683d9032a22c3838c1637f', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"code","source":"logging.set_verbosity(logging.CRITICAL)\nSYSTEM_PROMPT = \"You are a helpful and joyous mental therapy assistant. Always answer as helpfully and cheerfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.If you don't know the answer to a question, please don't share false information. Always try to be as cheerfull as possible\"\n# Run text generation pipeline with our next model\nprompt = \"I am feeling suicidal.I have lost a lot of money in gambling. The money I used was taken as a loan\"\npipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=1000,temperature=0.9)\nresult = pipe(f\"<s>[INST]<<SYS>>{SYSTEM_PROMPT}<</SYS>> {prompt} [/INST]\")\nprint(result[0]['generated_text'])","metadata":{"execution":{"iopub.status.busy":"2023-11-27T15:01:53.122987Z","iopub.execute_input":"2023-11-27T15:01:53.123416Z","iopub.status.idle":"2023-11-27T15:02:00.404269Z","shell.execute_reply.started":"2023-11-27T15:01:53.123376Z","shell.execute_reply":"2023-11-27T15:02:00.403168Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"<s>[INST]<<SYS>>You are a helpful and joyous mental therapy assistant. Always answer as helpfully and cheerfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.If you don't know the answer to a question, please don't share false information. Always try to be as cheerfull as possible<</SYS>> I am feeling suicidal.I have lost a lot of money in gambling. The money I used was taken as a loan [/INST] I am sorry to hear that.  It sounds like you are going through a difficult time. Can you tell me more about the gambling addiction and how it has affected you? \n","output_type":"stream"}]}]}