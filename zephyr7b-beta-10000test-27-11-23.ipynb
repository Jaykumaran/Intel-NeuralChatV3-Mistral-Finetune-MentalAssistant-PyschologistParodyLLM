{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()\n\n#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","metadata":{"execution":{"iopub.status.busy":"2023-11-27T11:18:56.493799Z","iopub.execute_input":"2023-11-27T11:18:56.494064Z","iopub.status.idle":"2023-11-27T11:18:56.823194Z","shell.execute_reply.started":"2023-11-27T11:18:56.494040Z","shell.execute_reply":"2023-11-27T11:18:56.822304Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d46d3219010841359dbe7c8800f95d41"}},"metadata":{}}]},{"cell_type":"code","source":"# authenticate WandB for logging metrics\nimport wandb\nwandb.login()\n# ee311bc313381f6bee4c5412a630fdad1175f3c0\n","metadata":{"execution":{"iopub.status.busy":"2023-11-27T11:19:03.776009Z","iopub.execute_input":"2023-11-27T11:19:03.776352Z","iopub.status.idle":"2023-11-27T11:19:09.555742Z","shell.execute_reply.started":"2023-11-27T11:19:03.776323Z","shell.execute_reply":"2023-11-27T11:19:09.554580Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"!pip install trl transformers accelerate git+https://github.com/huggingface/peft.git -Uqqq\n!pip install datasets bitsandbytes einops wandb -Uqqq","metadata":{"execution":{"iopub.status.busy":"2023-11-27T11:19:11.600620Z","iopub.execute_input":"2023-11-27T11:19:11.601605Z","iopub.status.idle":"2023-11-27T11:20:08.782898Z","shell.execute_reply.started":"2023-11-27T11:19:11.601572Z","shell.execute_reply":"2023-11-27T11:20:08.781647Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntokenizers 0.14.1 requires huggingface_hub<0.18,>=0.16.4, but you have huggingface-hub 0.19.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import os\n\nimport torch\nfrom datasets import load_dataset\nfrom peft import LoraConfig, get_peft_model, PeftConfig, PeftModel, prepare_model_for_kbit_training\nfrom transformers import (AutoModelForCausalLM, AutoTokenizer,\n                          BitsAndBytesConfig, HfArgumentParser,\n                          TrainingArguments,GenerationConfig, logging, pipeline)\nfrom trl import SFTTrainer","metadata":{"execution":{"iopub.status.busy":"2023-11-27T11:20:08.785166Z","iopub.execute_input":"2023-11-27T11:20:08.785478Z","iopub.status.idle":"2023-11-27T11:20:28.148083Z","shell.execute_reply.started":"2023-11-27T11:20:08.785452Z","shell.execute_reply":"2023-11-27T11:20:28.147140Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"model_name = \"HuggingFaceH4/zephyr-7b-beta\" # sharded Mistral-7b model\ndataset_name = \"vibhorag101/phr_mental_health_dataset\"\nnew_model = \"Zephyr7b-Beta-10000D-chat-hf-phr_mental_therapy\"\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,            # load model in 4-bit precision\n    bnb_4bit_quant_type=\"nf4\",    # pre-trained model should be quantized in 4-bit NF format\n    bnb_4bit_use_double_quant=True, # Using double quantization as mentioned in QLoRA paper\n    bnb_4bit_compute_dtype=torch.bfloat16, # During computation, pre-trained model should be loaded in BF16 format\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config, # Use bitsandbytes config\n    device_map=\"auto\",  # Specifying device_map=\"auto\" so that HF Accelerate will determine which GPU to put each layer of the model on\n    trust_remote_code=True, # Set trust_remote_code=True to use Mistral-7b model with custom code\n)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T11:20:37.345457Z","iopub.execute_input":"2023-11-27T11:20:37.346284Z","iopub.status.idle":"2023-11-27T11:22:51.951515Z","shell.execute_reply.started":"2023-11-27T11:20:37.346233Z","shell.execute_reply":"2023-11-27T11:22:51.950702Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading config.json:   0%|          | 0.00/638 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e626f61a57d44c2c9b3450576765491e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)fetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6e44cd74334440698f841de124a8ee1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0a6ae5a63e64cec8ed94f9de3606810"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)of-00008.safetensors:   0%|          | 0.00/1.89G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"723418e70e6c4ac9bb8b338742d71399"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)of-00008.safetensors:   0%|          | 0.00/1.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a935c9d7a804c30b1b4e5b75a1121d6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)of-00008.safetensors:   0%|          | 0.00/1.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f225f9401c74af49c0f11622f409b30"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)of-00008.safetensors:   0%|          | 0.00/1.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a92f6150d6c40068099dae19e3fd47b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)of-00008.safetensors:   0%|          | 0.00/1.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe67229650ab41179c0b7e635caef64b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)of-00008.safetensors:   0%|          | 0.00/1.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30a5f98d4758428b9f0a5ce63cd9fb97"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)of-00008.safetensors:   0%|          | 0.00/1.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58ea905d134946f18cd1ff732e984275"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)of-00008.safetensors:   0%|          | 0.00/816M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc67b9789d054f0c8b57f046cbc0fe36"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b8f9f2ebd7e4581b0d1e0fed1c79e96"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cdfafa9a3da34a6f8a010e9d6f976939"}},"metadata":{}}]},{"cell_type":"code","source":"dataset = load_dataset(dataset_name, split=\"train\")\ndataset = dataset.shuffle().select(range(10000))\ndataset.train_test_split(test_size=0.05)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T11:22:51.953343Z","iopub.execute_input":"2023-11-27T11:22:51.953629Z","iopub.status.idle":"2023-11-27T11:23:00.794585Z","shell.execute_reply.started":"2023-11-27T11:22:51.953602Z","shell.execute_reply":"2023-11-27T11:23:00.793695Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/460 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de5bdd879a3b4c0fa9f32da88bd5369d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5759739b3e9b45f2a68abcc3ceb1f82d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/211M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45052c5e869544479e4934c941291014"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b05c1128de8c43849e02ae260241d8a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/99086 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5627c42474dd44f08cbb7b9c90c24eec"}},"metadata":{}},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text'],\n        num_rows: 9500\n    })\n    test: Dataset({\n        features: ['text'],\n        num_rows: 500\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"print(dataset['text'][1])","metadata":{"execution":{"iopub.status.busy":"2023-11-27T11:23:00.795590Z","iopub.execute_input":"2023-11-27T11:23:00.795896Z","iopub.status.idle":"2023-11-27T11:23:00.926921Z","shell.execute_reply.started":"2023-11-27T11:23:00.795852Z","shell.execute_reply":"2023-11-27T11:23:00.925982Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"<s>[INST] <<SYS>>\nYou are a helpful and joyous mental therapy assistant. Always answer as helpfully and cheerfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.Please ensure that your responses are socially unbiased and positive in nature.\n\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n<</SYS>\n\nI am feeling really disgusted with myself right now. It's been a few months since the end of my last relationship, and I just can't seem to move on. The pain and grief I feel are overwhelming. [/INST] I hear you.  It sounds like you're in a difficult place, managing the emotions tied to the end of your relationship. Grief can be incredibly challenging, and it's important to acknowledge and process these feelings. Can you tell me more about what specifically is making you feel disgusted? </s><s>[INST] Thank you for listening.  I think I feel disgusted because I invested so much time, effort, and love into that relationship, and it ended up failing. It makes me question my worth and judgement. I feel like I failed not only myself but also my partner. [/INST] It's understandable that you're experiencing these thoughts and emotions.  Endings can be tough, especially when we've given so much of ourselves. But it's crucial to remember that your worth isn't defined by this one relationship or its outcome. Relationships are a two-way street, and it's not solely your responsibility to make them work. How have you been coping with these feelings of self-disgust? </s><s>[INST] I've been trying to be kind to myself, but it's been a struggle. I've been journaling to process my emotions and seeking support from friends and family. However, there are times when the weight of grief feels too heavy to bear, and I find myself slipping back into self-criticism. [/INST] It's great to hear that you're taking steps towards self-care.  Journaling can be an excellent way to explore and understand your emotions. Remember, healing takes time, and it's normal to have moments where it feels like you're taking a step back. It's essential to approach yourself with compassion during these moments. How do you envision your healing journey moving forward? </s><s>[INST] I am hopeful.  Moving forward, I aim to focus on my personal growth and rediscover my passions. I want to channel my energy into activities that bring me joy and fulfillment. Additionally, I'll continue seeking guidance and support from therapy to help me navigate this grief and regain my sense of self-worth. [/INST] That sounds like a great plan.  Focusing on your personal growth and finding joy in activities can be transformative during times of grief. Therapy can offer invaluable guidance and be a safe space for you to explore and heal. As we continue this journey together, I'm here to support you. How do you think you can begin incorporating these steps into your life starting today? </s><s>[INST] Thank you.  Today, I will make a list of activities that once brought me joy and start integrating them back into my daily routine. I will also schedule regular therapy sessions to ensure I receive the support and guidance I need to navigate this grief. Taking small steps towards healing is a start, and I am willing to put in the effort. [/INST] That's wonderful.  Incremental steps can make a significant difference in the healing process. Remember to be patient and kind to yourself as you embark on this journey of self-discovery. Celebrate each small victory along the way and share your progress during our therapy sessions. Is there anything else you'd like to discuss or any additional support you need right now? </s><s>[INST] Thank you.  I feel relieved after sharing my thoughts and plans with you. Right now, I think I have what I need to start moving forward. I will continue to reach out for support when necessary and embrace the hope within me. Thank you for being there for me. [/INST] It's been a privilege to listen and assist you.  I'm glad to hear that you're feeling more hopeful and ready to take steps towards healing. Remember, I am just a message away, always here to support and guide you through this challenging but transformative process of grief. Take care of yourself and have faith in your ability to heal. </s>\n","output_type":"stream"}]},{"cell_type":"code","source":"\n\nlora_alpha = 32 # scaling factor for the weight matrices\nlora_dropout = 0.05 # dropout probability of the LoRA layers\nlora_rank = 16 # dimension of the low-rank matrices\n\npeft_config = LoraConfig(\n    lora_alpha=lora_alpha,\n    lora_dropout=lora_dropout,\n    r=lora_rank,\n    bias=\"none\",  # setting to 'none' for only training weight params instead of biases\n    task_type=\"CAUSAL_LM\",\n    \n)\n\npeft_model = get_peft_model(model, peft_config)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T11:23:00.929008Z","iopub.execute_input":"2023-11-27T11:23:00.929290Z","iopub.status.idle":"2023-11-27T11:23:01.092396Z","shell.execute_reply.started":"2023-11-27T11:23:00.929265Z","shell.execute_reply":"2023-11-27T11:23:01.091448Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"output_dir = \"./Zephyr7b-Beta-10000D-chat-hf-phr_mental_therapy\"\nper_device_train_batch_size = 2 # reduce batch size by 2x only if out-of-memory error\ngradient_accumulation_steps = 1  # increase gradient accumulation steps by  2x only if batch size is reduced\noptim = \"paged_adamw_32bit\" # activates the paging for better memory management\nsave_strategy=\"steps\" # checkpoint save strategy to adopt during training\nsave_steps = 10 # number of updates steps before two checkpoint saves\nlogging_steps = 10  # number of update steps between two logs if logging_strategy=\"steps\"\nlearning_rate = 2e-4  # learning rate for AdamW optimizer\nmax_grad_norm = 0.3 # maximum gradient norm (for gradient clipping)\nmax_steps = -1      # training will happen for automatic steps\nwarmup_ratio = 0.03 # number of steps used for a linear warmup from 0 to learning_rate\nlr_scheduler_type = \"cosine\"  # learning rate scheduler\nnum_train_epochs = 2\nper_device_eval_batch_size = 2\nmax_seq_length=256\n\ntraining_arguments = TrainingArguments(\n    output_dir=output_dir,\n    per_device_train_batch_size=per_device_train_batch_size,\n    gradient_accumulation_steps=gradient_accumulation_steps,\n    optim=optim,\n    num_train_epochs= num_train_epochs,\n    save_steps=save_steps,\n    logging_steps=logging_steps,\n    learning_rate=learning_rate,\n    fp16=True,\n    max_grad_norm=max_grad_norm,\n    max_steps=max_steps,\n    warmup_ratio=warmup_ratio,\n    group_by_length=True,\n    lr_scheduler_type=lr_scheduler_type,\n    push_to_hub=True,\n    \n)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T11:23:01.093632Z","iopub.execute_input":"2023-11-27T11:23:01.093993Z","iopub.status.idle":"2023-11-27T11:23:01.105454Z","shell.execute_reply.started":"2023-11-27T11:23:01.093959Z","shell.execute_reply":"2023-11-27T11:23:01.104520Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=\"auto\"\n)\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\n\n# # Load LLaMA tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True,use_fast=False,add_eos_token=True)\ntokenizer.pad_token_id = 18610","metadata":{"execution":{"iopub.status.busy":"2023-11-27T11:23:01.106491Z","iopub.execute_input":"2023-11-27T11:23:01.106767Z","iopub.status.idle":"2023-11-27T11:24:04.530904Z","shell.execute_reply.started":"2023-11-27T11:23:01.106744Z","shell.execute_reply":"2023-11-27T11:24:04.530091Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dfa5bde066bd41d5b14fc1aa1b8e8a77"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer_config.json:   0%|          | 0.00/1.43k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8093e2de93bc4031a3a6b6e888feae9f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74ccc66608ac4dd991a47b78704febfb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading added_tokens.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f881edd4c1840bbadff79f9699c324b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/168 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f9f6cbda0504822ab962c0b72f89ca0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f1eb27769f44451ad4ae537a90d13b8"}},"metadata":{}}]},{"cell_type":"code","source":"#Set supervised fine-tuning parameters\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset,\n    peft_config=peft_config,\n    dataset_text_field=\"text\",\n    max_seq_length=256,\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing=False,\n)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-27T11:24:04.532076Z","iopub.execute_input":"2023-11-27T11:24:04.532349Z","iopub.status.idle":"2023-11-27T11:25:34.361258Z","shell.execute_reply.started":"2023-11-27T11:24:04.532325Z","shell.execute_reply":"2023-11-27T11:25:34.360395Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bef0fc11c6324ce785047081576d22a8"}},"metadata":{}}]},{"cell_type":"code","source":"# upcasting the layer norms in torch.bfloat16 for more stable training\nfor name, module in trainer.model.named_modules():\n    if \"norm\" in name:\n        module = module.to(torch.bfloat16)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T11:25:34.362430Z","iopub.execute_input":"2023-11-27T11:25:34.362754Z","iopub.status.idle":"2023-11-27T11:25:34.375753Z","shell.execute_reply.started":"2023-11-27T11:25:34.362724Z","shell.execute_reply":"2023-11-27T11:25:34.374667Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"model.config.use_cache = False\ntrainer.train()\ntrainer.model.save_pretrained(new_model)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T11:25:34.376992Z","iopub.execute_input":"2023-11-27T11:25:34.377281Z","iopub.status.idle":"2023-11-27T12:36:34.188547Z","shell.execute_reply.started":"2023-11-27T11:25:34.377255Z","shell.execute_reply":"2023-11-27T12:36:34.179600Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mr1ajayakumaran1751994\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20231127_112538-1euju8i5</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/r1ajayakumaran1751994/huggingface/runs/1euju8i5' target=\"_blank\">pious-fog-24</a></strong> to <a href='https://wandb.ai/r1ajayakumaran1751994/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/r1ajayakumaran1751994/huggingface' target=\"_blank\">https://wandb.ai/r1ajayakumaran1751994/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/r1ajayakumaran1751994/huggingface/runs/1euju8i5' target=\"_blank\">https://wandb.ai/r1ajayakumaran1751994/huggingface/runs/1euju8i5</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2541' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 2541/10000 1:10:16 < 3:26:27, 0.60 it/s, Epoch 0.51/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>1.997800</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>2.036400</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>1.845200</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>1.746300</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>1.587200</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>1.459300</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>1.234900</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>1.253400</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>1.140500</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.165800</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>1.173400</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>1.193000</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>1.155400</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>1.138100</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>1.168000</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>1.126400</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>1.119800</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>1.089100</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>1.110000</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.171700</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>1.101400</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>1.108700</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>1.044300</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>1.054500</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>1.145500</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>1.067400</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>1.061200</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>1.044900</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>1.101000</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>1.103800</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>1.018500</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>1.067100</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>1.068800</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>1.096900</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>1.076100</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>1.060500</td>\n    </tr>\n    <tr>\n      <td>370</td>\n      <td>1.059500</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>1.031400</td>\n    </tr>\n    <tr>\n      <td>390</td>\n      <td>1.085900</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>1.064200</td>\n    </tr>\n    <tr>\n      <td>410</td>\n      <td>1.075400</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>1.045100</td>\n    </tr>\n    <tr>\n      <td>430</td>\n      <td>1.090000</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>1.088500</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>1.018100</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>1.027500</td>\n    </tr>\n    <tr>\n      <td>470</td>\n      <td>1.042900</td>\n    </tr>\n    <tr>\n      <td>480</td>\n      <td>1.057700</td>\n    </tr>\n    <tr>\n      <td>490</td>\n      <td>1.094900</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>1.145200</td>\n    </tr>\n    <tr>\n      <td>510</td>\n      <td>1.048100</td>\n    </tr>\n    <tr>\n      <td>520</td>\n      <td>1.040900</td>\n    </tr>\n    <tr>\n      <td>530</td>\n      <td>1.032800</td>\n    </tr>\n    <tr>\n      <td>540</td>\n      <td>1.078200</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>1.007400</td>\n    </tr>\n    <tr>\n      <td>560</td>\n      <td>1.085400</td>\n    </tr>\n    <tr>\n      <td>570</td>\n      <td>1.019400</td>\n    </tr>\n    <tr>\n      <td>580</td>\n      <td>1.024900</td>\n    </tr>\n    <tr>\n      <td>590</td>\n      <td>1.093000</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.981900</td>\n    </tr>\n    <tr>\n      <td>610</td>\n      <td>1.027800</td>\n    </tr>\n    <tr>\n      <td>620</td>\n      <td>1.046700</td>\n    </tr>\n    <tr>\n      <td>630</td>\n      <td>1.022400</td>\n    </tr>\n    <tr>\n      <td>640</td>\n      <td>0.988800</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>1.164300</td>\n    </tr>\n    <tr>\n      <td>660</td>\n      <td>1.078200</td>\n    </tr>\n    <tr>\n      <td>670</td>\n      <td>1.021800</td>\n    </tr>\n    <tr>\n      <td>680</td>\n      <td>1.041300</td>\n    </tr>\n    <tr>\n      <td>690</td>\n      <td>1.044900</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>1.027700</td>\n    </tr>\n    <tr>\n      <td>710</td>\n      <td>1.027100</td>\n    </tr>\n    <tr>\n      <td>720</td>\n      <td>1.031200</td>\n    </tr>\n    <tr>\n      <td>730</td>\n      <td>0.993600</td>\n    </tr>\n    <tr>\n      <td>740</td>\n      <td>1.013200</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>0.988100</td>\n    </tr>\n    <tr>\n      <td>760</td>\n      <td>1.011900</td>\n    </tr>\n    <tr>\n      <td>770</td>\n      <td>1.048200</td>\n    </tr>\n    <tr>\n      <td>780</td>\n      <td>1.023900</td>\n    </tr>\n    <tr>\n      <td>790</td>\n      <td>0.978400</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>1.072400</td>\n    </tr>\n    <tr>\n      <td>810</td>\n      <td>1.042300</td>\n    </tr>\n    <tr>\n      <td>820</td>\n      <td>1.049700</td>\n    </tr>\n    <tr>\n      <td>830</td>\n      <td>1.002800</td>\n    </tr>\n    <tr>\n      <td>840</td>\n      <td>1.006200</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>1.127900</td>\n    </tr>\n    <tr>\n      <td>860</td>\n      <td>1.036600</td>\n    </tr>\n    <tr>\n      <td>870</td>\n      <td>1.067200</td>\n    </tr>\n    <tr>\n      <td>880</td>\n      <td>1.084500</td>\n    </tr>\n    <tr>\n      <td>890</td>\n      <td>1.068700</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>1.057000</td>\n    </tr>\n    <tr>\n      <td>910</td>\n      <td>0.985300</td>\n    </tr>\n    <tr>\n      <td>920</td>\n      <td>0.970800</td>\n    </tr>\n    <tr>\n      <td>930</td>\n      <td>0.996900</td>\n    </tr>\n    <tr>\n      <td>940</td>\n      <td>1.004700</td>\n    </tr>\n    <tr>\n      <td>950</td>\n      <td>1.037600</td>\n    </tr>\n    <tr>\n      <td>960</td>\n      <td>0.974000</td>\n    </tr>\n    <tr>\n      <td>970</td>\n      <td>1.049800</td>\n    </tr>\n    <tr>\n      <td>980</td>\n      <td>1.034900</td>\n    </tr>\n    <tr>\n      <td>990</td>\n      <td>1.003700</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>1.002100</td>\n    </tr>\n    <tr>\n      <td>1010</td>\n      <td>1.019500</td>\n    </tr>\n    <tr>\n      <td>1020</td>\n      <td>1.049500</td>\n    </tr>\n    <tr>\n      <td>1030</td>\n      <td>0.981800</td>\n    </tr>\n    <tr>\n      <td>1040</td>\n      <td>1.029800</td>\n    </tr>\n    <tr>\n      <td>1050</td>\n      <td>0.996600</td>\n    </tr>\n    <tr>\n      <td>1060</td>\n      <td>1.028300</td>\n    </tr>\n    <tr>\n      <td>1070</td>\n      <td>0.991500</td>\n    </tr>\n    <tr>\n      <td>1080</td>\n      <td>1.044800</td>\n    </tr>\n    <tr>\n      <td>1090</td>\n      <td>1.031500</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>1.082100</td>\n    </tr>\n    <tr>\n      <td>1110</td>\n      <td>0.974700</td>\n    </tr>\n    <tr>\n      <td>1120</td>\n      <td>0.981700</td>\n    </tr>\n    <tr>\n      <td>1130</td>\n      <td>1.061300</td>\n    </tr>\n    <tr>\n      <td>1140</td>\n      <td>0.968300</td>\n    </tr>\n    <tr>\n      <td>1150</td>\n      <td>1.023200</td>\n    </tr>\n    <tr>\n      <td>1160</td>\n      <td>1.016700</td>\n    </tr>\n    <tr>\n      <td>1170</td>\n      <td>1.012800</td>\n    </tr>\n    <tr>\n      <td>1180</td>\n      <td>0.957700</td>\n    </tr>\n    <tr>\n      <td>1190</td>\n      <td>0.993100</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>1.013900</td>\n    </tr>\n    <tr>\n      <td>1210</td>\n      <td>0.950700</td>\n    </tr>\n    <tr>\n      <td>1220</td>\n      <td>0.982800</td>\n    </tr>\n    <tr>\n      <td>1230</td>\n      <td>0.985400</td>\n    </tr>\n    <tr>\n      <td>1240</td>\n      <td>0.985100</td>\n    </tr>\n    <tr>\n      <td>1250</td>\n      <td>1.058400</td>\n    </tr>\n    <tr>\n      <td>1260</td>\n      <td>0.982200</td>\n    </tr>\n    <tr>\n      <td>1270</td>\n      <td>0.985400</td>\n    </tr>\n    <tr>\n      <td>1280</td>\n      <td>0.975200</td>\n    </tr>\n    <tr>\n      <td>1290</td>\n      <td>1.018100</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>1.026500</td>\n    </tr>\n    <tr>\n      <td>1310</td>\n      <td>1.046700</td>\n    </tr>\n    <tr>\n      <td>1320</td>\n      <td>1.027700</td>\n    </tr>\n    <tr>\n      <td>1330</td>\n      <td>1.006400</td>\n    </tr>\n    <tr>\n      <td>1340</td>\n      <td>0.967200</td>\n    </tr>\n    <tr>\n      <td>1350</td>\n      <td>0.973900</td>\n    </tr>\n    <tr>\n      <td>1360</td>\n      <td>0.981400</td>\n    </tr>\n    <tr>\n      <td>1370</td>\n      <td>0.969400</td>\n    </tr>\n    <tr>\n      <td>1380</td>\n      <td>1.014000</td>\n    </tr>\n    <tr>\n      <td>1390</td>\n      <td>1.004800</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.933100</td>\n    </tr>\n    <tr>\n      <td>1410</td>\n      <td>1.026600</td>\n    </tr>\n    <tr>\n      <td>1420</td>\n      <td>0.991600</td>\n    </tr>\n    <tr>\n      <td>1430</td>\n      <td>1.006700</td>\n    </tr>\n    <tr>\n      <td>1440</td>\n      <td>0.965100</td>\n    </tr>\n    <tr>\n      <td>1450</td>\n      <td>1.020000</td>\n    </tr>\n    <tr>\n      <td>1460</td>\n      <td>1.020400</td>\n    </tr>\n    <tr>\n      <td>1470</td>\n      <td>1.021800</td>\n    </tr>\n    <tr>\n      <td>1480</td>\n      <td>1.001200</td>\n    </tr>\n    <tr>\n      <td>1490</td>\n      <td>0.964100</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.957800</td>\n    </tr>\n    <tr>\n      <td>1510</td>\n      <td>0.976800</td>\n    </tr>\n    <tr>\n      <td>1520</td>\n      <td>0.997000</td>\n    </tr>\n    <tr>\n      <td>1530</td>\n      <td>1.006700</td>\n    </tr>\n    <tr>\n      <td>1540</td>\n      <td>0.944200</td>\n    </tr>\n    <tr>\n      <td>1550</td>\n      <td>0.989400</td>\n    </tr>\n    <tr>\n      <td>1560</td>\n      <td>1.045200</td>\n    </tr>\n    <tr>\n      <td>1570</td>\n      <td>0.988200</td>\n    </tr>\n    <tr>\n      <td>1580</td>\n      <td>0.984100</td>\n    </tr>\n    <tr>\n      <td>1590</td>\n      <td>1.028300</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>0.989400</td>\n    </tr>\n    <tr>\n      <td>1610</td>\n      <td>0.987900</td>\n    </tr>\n    <tr>\n      <td>1620</td>\n      <td>1.010900</td>\n    </tr>\n    <tr>\n      <td>1630</td>\n      <td>0.984700</td>\n    </tr>\n    <tr>\n      <td>1640</td>\n      <td>1.006700</td>\n    </tr>\n    <tr>\n      <td>1650</td>\n      <td>0.991700</td>\n    </tr>\n    <tr>\n      <td>1660</td>\n      <td>0.980700</td>\n    </tr>\n    <tr>\n      <td>1670</td>\n      <td>1.016400</td>\n    </tr>\n    <tr>\n      <td>1680</td>\n      <td>1.026400</td>\n    </tr>\n    <tr>\n      <td>1690</td>\n      <td>0.952300</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>0.978600</td>\n    </tr>\n    <tr>\n      <td>1710</td>\n      <td>0.976100</td>\n    </tr>\n    <tr>\n      <td>1720</td>\n      <td>0.964100</td>\n    </tr>\n    <tr>\n      <td>1730</td>\n      <td>1.011900</td>\n    </tr>\n    <tr>\n      <td>1740</td>\n      <td>1.030000</td>\n    </tr>\n    <tr>\n      <td>1750</td>\n      <td>1.043600</td>\n    </tr>\n    <tr>\n      <td>1760</td>\n      <td>0.955300</td>\n    </tr>\n    <tr>\n      <td>1770</td>\n      <td>0.984500</td>\n    </tr>\n    <tr>\n      <td>1780</td>\n      <td>0.961900</td>\n    </tr>\n    <tr>\n      <td>1790</td>\n      <td>0.945000</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>1.014300</td>\n    </tr>\n    <tr>\n      <td>1810</td>\n      <td>0.967600</td>\n    </tr>\n    <tr>\n      <td>1820</td>\n      <td>1.055700</td>\n    </tr>\n    <tr>\n      <td>1830</td>\n      <td>0.988900</td>\n    </tr>\n    <tr>\n      <td>1840</td>\n      <td>0.958100</td>\n    </tr>\n    <tr>\n      <td>1850</td>\n      <td>0.949000</td>\n    </tr>\n    <tr>\n      <td>1860</td>\n      <td>0.988000</td>\n    </tr>\n    <tr>\n      <td>1870</td>\n      <td>0.972100</td>\n    </tr>\n    <tr>\n      <td>1880</td>\n      <td>0.930400</td>\n    </tr>\n    <tr>\n      <td>1890</td>\n      <td>0.983400</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>1.021500</td>\n    </tr>\n    <tr>\n      <td>1910</td>\n      <td>0.942800</td>\n    </tr>\n    <tr>\n      <td>1920</td>\n      <td>1.012400</td>\n    </tr>\n    <tr>\n      <td>1930</td>\n      <td>0.956500</td>\n    </tr>\n    <tr>\n      <td>1940</td>\n      <td>0.992400</td>\n    </tr>\n    <tr>\n      <td>1950</td>\n      <td>0.966000</td>\n    </tr>\n    <tr>\n      <td>1960</td>\n      <td>1.005300</td>\n    </tr>\n    <tr>\n      <td>1970</td>\n      <td>0.966200</td>\n    </tr>\n    <tr>\n      <td>1980</td>\n      <td>1.006500</td>\n    </tr>\n    <tr>\n      <td>1990</td>\n      <td>0.938300</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>1.031600</td>\n    </tr>\n    <tr>\n      <td>2010</td>\n      <td>1.059000</td>\n    </tr>\n    <tr>\n      <td>2020</td>\n      <td>0.931400</td>\n    </tr>\n    <tr>\n      <td>2030</td>\n      <td>0.976100</td>\n    </tr>\n    <tr>\n      <td>2040</td>\n      <td>0.936700</td>\n    </tr>\n    <tr>\n      <td>2050</td>\n      <td>0.936700</td>\n    </tr>\n    <tr>\n      <td>2060</td>\n      <td>0.977300</td>\n    </tr>\n    <tr>\n      <td>2070</td>\n      <td>0.978500</td>\n    </tr>\n    <tr>\n      <td>2080</td>\n      <td>0.930000</td>\n    </tr>\n    <tr>\n      <td>2090</td>\n      <td>0.965300</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>0.959700</td>\n    </tr>\n    <tr>\n      <td>2110</td>\n      <td>0.976600</td>\n    </tr>\n    <tr>\n      <td>2120</td>\n      <td>1.030100</td>\n    </tr>\n    <tr>\n      <td>2130</td>\n      <td>0.978900</td>\n    </tr>\n    <tr>\n      <td>2140</td>\n      <td>0.994500</td>\n    </tr>\n    <tr>\n      <td>2150</td>\n      <td>0.962100</td>\n    </tr>\n    <tr>\n      <td>2160</td>\n      <td>0.985300</td>\n    </tr>\n    <tr>\n      <td>2170</td>\n      <td>1.007900</td>\n    </tr>\n    <tr>\n      <td>2180</td>\n      <td>0.921200</td>\n    </tr>\n    <tr>\n      <td>2190</td>\n      <td>0.984800</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>0.988400</td>\n    </tr>\n    <tr>\n      <td>2210</td>\n      <td>0.972500</td>\n    </tr>\n    <tr>\n      <td>2220</td>\n      <td>1.012700</td>\n    </tr>\n    <tr>\n      <td>2230</td>\n      <td>0.980200</td>\n    </tr>\n    <tr>\n      <td>2240</td>\n      <td>0.980800</td>\n    </tr>\n    <tr>\n      <td>2250</td>\n      <td>0.966100</td>\n    </tr>\n    <tr>\n      <td>2260</td>\n      <td>0.998000</td>\n    </tr>\n    <tr>\n      <td>2270</td>\n      <td>0.951600</td>\n    </tr>\n    <tr>\n      <td>2280</td>\n      <td>0.996800</td>\n    </tr>\n    <tr>\n      <td>2290</td>\n      <td>1.015100</td>\n    </tr>\n    <tr>\n      <td>2300</td>\n      <td>0.987900</td>\n    </tr>\n    <tr>\n      <td>2310</td>\n      <td>0.968600</td>\n    </tr>\n    <tr>\n      <td>2320</td>\n      <td>0.932000</td>\n    </tr>\n    <tr>\n      <td>2330</td>\n      <td>0.975300</td>\n    </tr>\n    <tr>\n      <td>2340</td>\n      <td>0.982600</td>\n    </tr>\n    <tr>\n      <td>2350</td>\n      <td>0.995200</td>\n    </tr>\n    <tr>\n      <td>2360</td>\n      <td>0.964700</td>\n    </tr>\n    <tr>\n      <td>2370</td>\n      <td>0.941400</td>\n    </tr>\n    <tr>\n      <td>2380</td>\n      <td>0.997900</td>\n    </tr>\n    <tr>\n      <td>2390</td>\n      <td>1.029600</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>0.992700</td>\n    </tr>\n    <tr>\n      <td>2410</td>\n      <td>0.965800</td>\n    </tr>\n    <tr>\n      <td>2420</td>\n      <td>0.987100</td>\n    </tr>\n    <tr>\n      <td>2430</td>\n      <td>0.954600</td>\n    </tr>\n    <tr>\n      <td>2440</td>\n      <td>0.995700</td>\n    </tr>\n    <tr>\n      <td>2450</td>\n      <td>1.022500</td>\n    </tr>\n    <tr>\n      <td>2460</td>\n      <td>0.996400</td>\n    </tr>\n    <tr>\n      <td>2470</td>\n      <td>1.009800</td>\n    </tr>\n    <tr>\n      <td>2480</td>\n      <td>0.953500</td>\n    </tr>\n    <tr>\n      <td>2490</td>\n      <td>1.000800</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.995300</td>\n    </tr>\n    <tr>\n      <td>2510</td>\n      <td>0.964100</td>\n    </tr>\n    <tr>\n      <td>2520</td>\n      <td>1.008100</td>\n    </tr>\n    <tr>\n      <td>2530</td>\n      <td>1.022300</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:441\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m--> 441\u001b[0m     \u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:668\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    667\u001b[0m num_bytes \u001b[38;5;241m=\u001b[39m storage\u001b[38;5;241m.\u001b[39mnbytes()\n\u001b[0;32m--> 668\u001b[0m \u001b[43mzip_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_ptr\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_bytes\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:471] . PytorchStreamWriter failed writing file data/195: file write failed","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m trainer\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39msave_pretrained(new_model)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:280\u001b[0m, in \u001b[0;36mSFTTrainer.train\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trl_activate_neftune(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m--> 280\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;66;03m# After training we make sure to retrieve back the original forward pass method\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# for the embedding layer by removing the forward post hook.\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1546\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     \u001b[38;5;66;03m# Disable progress bars when uploading models during checkpoints to avoid polluting stdout\u001b[39;00m\n\u001b[1;32m   1545\u001b[0m     hf_hub_utils\u001b[38;5;241m.\u001b[39mdisable_progress_bars()\n\u001b[0;32m-> 1546\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1547\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1548\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1549\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1550\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1551\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1553\u001b[0m     hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1922\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1919\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m=\u001b[39m epoch \u001b[38;5;241m+\u001b[39m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m steps_skipped) \u001b[38;5;241m/\u001b[39m steps_in_epoch\n\u001b[1;32m   1920\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 1922\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1923\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1924\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_substep_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2282\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2279\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mstep(metrics[metric_to_check])\n\u001b[1;32m   2281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[0;32m-> 2282\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2283\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_save(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2387\u001b[0m, in \u001b[0;36mTrainer._save_checkpoint\u001b[0;34m(self, model, trial, metrics)\u001b[0m\n\u001b[1;32m   2379\u001b[0m         smp\u001b[38;5;241m.\u001b[39msave(\n\u001b[1;32m   2380\u001b[0m             opt_state_dict,\n\u001b[1;32m   2381\u001b[0m             os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, OPTIMIZER_NAME),\n\u001b[1;32m   2382\u001b[0m             partial\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   2383\u001b[0m             v3\u001b[38;5;241m=\u001b[39msmp\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mshard_optimizer_state,\n\u001b[1;32m   2384\u001b[0m         )\n\u001b[1;32m   2385\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mshould_save \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_deepspeed_enabled \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfsdp \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_fsdp_enabled):\n\u001b[1;32m   2386\u001b[0m     \u001b[38;5;66;03m# deepspeed.save_checkpoint above saves model/optim/sched\u001b[39;00m\n\u001b[0;32m-> 2387\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOPTIMIZER_NAME\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2389\u001b[0m \u001b[38;5;66;03m# Save SCHEDULER & SCALER\u001b[39;00m\n\u001b[1;32m   2390\u001b[0m is_deepspeed_custom_scheduler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_deepspeed_enabled \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m   2391\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_scheduler, DeepSpeedSchedulerWrapper\n\u001b[1;32m   2392\u001b[0m )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:440\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    437\u001b[0m _check_save_filelike(f)\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m--> 440\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    441\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n\u001b[1;32m    442\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:291\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 291\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfile_like\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_end_of_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:337] . unexpected pos 22977792 vs 22977680"],"ename":"RuntimeError","evalue":"[enforce fail at inline_container.cc:337] . unexpected pos 22977792 vs 22977680","output_type":"error"},{"name":"stderr","text":"--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer.push_to_hub()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Merging as Single without Adapters","metadata":{}},{"cell_type":"code","source":"# Reload model in FP16 and merge it with LoRA weights\nbase_model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    low_cpu_mem_usage=True,\n    return_dict=True,\n    torch_dtype=torch.float16,\n    device_map=\"cpu\", ## device map = \"cpu\", for merging on cpu\n)\nmodel = PeftModel.from_pretrained(base_model, new_model)\nmodel = model.merge_and_unload()\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True,use_fast=False,add_eos_token=True)\ntokenizer.pad_token_id = 18610\n\n### Old version tokeniser, can't generate eos token\n# tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n# tokenizer.pad_token = tokenizer.eos_token\n# tokenizer.padding_side = \"right\"\n\n","metadata":{"execution":{"iopub.status.busy":"2023-11-27T12:36:34.190134Z","iopub.status.idle":"2023-11-27T12:36:34.190714Z","shell.execute_reply.started":"2023-11-27T12:36:34.190408Z","shell.execute_reply":"2023-11-27T12:36:34.190436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## for inference without merging qlora weights with original\nbase_model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=device_map\n)\nmodel = PeftModel.from_pretrained(base_model, new_model)\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True,use_fast=False,add_eos_token=True)\ntokenizer.pad_token_id = 18610","metadata":{"execution":{"iopub.status.busy":"2023-11-27T12:36:34.192157Z","iopub.status.idle":"2023-11-27T12:36:34.192641Z","shell.execute_reply.started":"2023-11-27T12:36:34.192394Z","shell.execute_reply":"2023-11-27T12:36:34.192421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.push_to_hub(new_model, use_temp_dir=False)\ntokenizer.push_to_hub(new_model, use_temp_dir=False)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T12:36:34.194823Z","iopub.status.idle":"2023-11-27T12:36:34.195332Z","shell.execute_reply.started":"2023-11-27T12:36:34.195075Z","shell.execute_reply":"2023-11-27T12:36:34.195100Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logging.set_verbosity(logging.CRITICAL)\nSYSTEM_PROMPT = \"You are a helpful and joyous mental therapy assistant. Always answer as helpfully and cheerfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.If you don't know the answer to a question, please don't share false information. Always try to be as cheerfull as possible\"\n# Run text generation pipeline with our next model\nprompt = \"I am feeling suicidal.I have lost a lot of money in gambling. The money I used was taken as a loan\"\npipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=1000,temperature=0.9)\nresult = pipe(f\"<s>[INST]<<SYS>>{SYSTEM_PROMPT}<</SYS>> {prompt} [/INST]\")\nprint(result[0]['generated_text'])","metadata":{},"execution_count":null,"outputs":[]}]}